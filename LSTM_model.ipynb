{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DMMgFMAbpYy"
   },
   "source": [
    "## Downloading and cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nRXrfsOKprtd",
    "outputId": "3a6d20e8-a1cc-4c2f-a71c-d0fb19c9e1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.38-py2.py3-none-any.whl (72 kB)\n",
      "     ---------------------------------------- 73.0/73.0 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting requests>=2.31\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.6/62.6 kB ? eta 0:00:00\n",
      "Collecting appdirs>=1.4.4\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting peewee>=3.16.2\n",
      "  Downloading peewee-3.17.3.tar.gz (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 11.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from yfinance) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from yfinance) (1.21.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from yfinance) (4.11.1)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from yfinance) (4.9.1)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from yfinance) (2022.7)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting html5lib>=1.1\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "     -------------------------------------- 112.2/112.2 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting frozendict>=2.3.4\n",
      "  Downloading frozendict-2.4.2-cp37-cp37m-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from requests>=2.31->yfinance) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from requests>=2.31->yfinance) (1.26.14)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml): started\n",
      "  Building wheel for peewee (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.17.3-py3-none-any.whl size=138452 sha256=5dc1a1a8c203251027569ec1dd7d552cf490b09c6f045a32452cba4ff1276a07\n",
      "  Stored in directory: C:\\Users\\rober\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ewj3f_pw\\wheels\\0b\\ae\\ec\\bc17d5247792d19d7f3abd30f787ab392b29c6e7075e6cdaf3\n",
      "Successfully built peewee\n",
      "Installing collected packages: peewee, multitasking, appdirs, requests, html5lib, frozendict, yfinance\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "Successfully installed appdirs-1.4.4 frozendict-2.4.2 html5lib-1.1 multitasking-0.0.11 peewee-3.17.3 requests-2.31.0 yfinance-0.2.38\n",
      "Collecting ta\n",
      "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from ta) (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from ta) (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from pandas->ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from pandas->ta) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->ta) (1.16.0)\n",
      "Building wheels for collected packages: ta\n",
      "  Building wheel for ta (setup.py): started\n",
      "  Building wheel for ta (setup.py): finished with status 'done'\n",
      "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=9c18b188f3cd1c3f1bc46920b03e765bb743c73af151583145d23f657eeaa837\n",
      "  Stored in directory: c:\\users\\rober\\appdata\\local\\pip\\cache\\wheels\\45\\01\\fe\\162332a456910060068e10c109e0d550acd45c0eb97dfc0a1c\n",
      "Successfully built ta\n",
      "Installing collected packages: ta\n",
      "Successfully installed ta-0.11.0\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n",
      "     -------------------------------------- 512.7/512.7 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (4.11.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp37-cp37m-win_amd64.whl (21.5 MB)\n",
      "     --------------------------------------- 21.5/21.5 MB 29.7 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py37-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 116.3/116.3 kB ? eta 0:00:00\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp37-cp37m-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (4.66.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.6-cp37-cp37m-win_amd64.whl (326 kB)\n",
      "     ------------------------------------- 326.9/326.9 kB 19.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (2023.1.0)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ---------------------------------------- 110.5/110.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from aiohttp->datasets) (4.4.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp37-cp37m-win_amd64.whl (77 kB)\n",
      "     ---------------------------------------- 77.2/77.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp37-cp37m-win_amd64.whl (34 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp37-cp37m-win_amd64.whl (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from importlib-metadata->datasets) (3.11.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.7/115.7 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, frozenlist, dill, asynctest, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 asynctest-0.13.0 datasets-2.13.2 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.5 multiprocess-0.70.14 pyarrow-12.0.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# Download the python packages\n",
    "!pip install yfinance --upgrade --no-cache-dir\n",
    "!pip install ta\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fOyZxnRpu19"
   },
   "outputs": [],
   "source": [
    "# yfinance is the API and ta is a technical analysis tool\n",
    "import yfinance as yf\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For data manupilation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV3L8nvnpxrH"
   },
   "outputs": [],
   "source": [
    "def pull_stock_data(stocks, start_date, end_date, interval):\n",
    "    stock_data = dict()\n",
    "\n",
    "    for stock in stocks:\n",
    "        stock_data[stock] = yf.download(stock, start=start_date, end=end_date, interval=interval)\n",
    "        if len(stock_data[stock])==0:\n",
    "          del stock_data[stock]\n",
    "\n",
    "    return stock_data\n",
    "\n",
    "def pull_stock_indicators(stock_data):\n",
    "\n",
    "    for stock in stock_data.keys():\n",
    "        # Adding simple moving average 20 and 50\n",
    "        stock_data[stock]['SMA_20'] = ta.trend.sma_indicator(close=stock_data[stock]['Close'], window=20)\n",
    "        stock_data[stock]['SMA_50'] = ta.trend.sma_indicator(close=stock_data[stock]['Close'], window=50)\n",
    "\n",
    "        # Adding stochastic oscillators\n",
    "        stock_data[stock]['%K'] = ta.momentum.stoch(high=stock_data[stock]['High'], low=stock_data[stock]['Low'], close=stock_data[stock]['Close'], window=14)\n",
    "        stock_data[stock]['%D'] = ta.momentum.stoch_signal(high=stock_data[stock]['High'], low=stock_data[stock]['Low'], close=stock_data[stock]['Close'], window=14)\n",
    "\n",
    "        # Adding RSI indicator\n",
    "        stock_data[stock]['RSI'] = ta.momentum.RSIIndicator(close=stock_data[stock]['Close'], window=14).rsi()\n",
    "\n",
    "        # Calculate Bollinger Bands\n",
    "        bollinger_bands = ta.volatility.BollingerBands(close=stock_data[stock]['Close'], window=20, window_dev=2)\n",
    "\n",
    "        # Add Bollinger Bands\n",
    "        stock_data[stock]['BB_Middle'] = bollinger_bands.bollinger_mavg()\n",
    "        stock_data[stock]['BB_Upper'] = bollinger_bands.bollinger_hband()\n",
    "        stock_data[stock]['BB_Lower'] = bollinger_bands.bollinger_lband()\n",
    "\n",
    "        # Calculate MACD\n",
    "        macd = ta.trend.MACD(close=stock_data[stock]['Close'], window_fast=12, window_slow=26, window_sign=9)\n",
    "\n",
    "        # Add MACD and signal line\n",
    "        stock_data[stock]['MACD'] = macd.macd()\n",
    "        stock_data[stock]['MACD_Signal'] = macd.macd_signal()\n",
    "\n",
    "        # Calculate ATR\n",
    "        atr = ta.volatility.AverageTrueRange(high=stock_data[stock]['High'], low=stock_data[stock]['Low'], close=stock_data[stock]['Close'], window=14)\n",
    "\n",
    "        # Add ATR\n",
    "        stock_data[stock]['ATR'] = atr.average_true_range()\n",
    "\n",
    "    return stock_data\n",
    "\n",
    "def cleaning_data(stock_data):\n",
    "    temp_dict = dict()\n",
    "\n",
    "    for stock in stock_data.keys():\n",
    "      try:\n",
    "          temp_dict[stock_data[stock].shape][stock] = stock_data[stock]\n",
    "      except:\n",
    "          temp_dict[stock_data[stock].shape] = dict()\n",
    "          temp_dict[stock_data[stock].shape][stock] = stock_data[stock]\n",
    "\n",
    "    max_l = max([len(temp_dict[shape]) for shape in temp_dict.keys()])\n",
    "\n",
    "    for shape in temp_dict.keys():\n",
    "      if len(temp_dict[shape])==max_l:\n",
    "        stock_data = temp_dict[shape]\n",
    "        break\n",
    "\n",
    "    # Remove all rows where there is a nan/non number entry\n",
    "    for stock in stock_data.keys():\n",
    "      stock_data[stock] = stock_data[stock].dropna()\n",
    "\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tqzRHqVSD5s"
   },
   "outputs": [],
   "source": [
    "# Obtaining the SNP 500 companies\n",
    "stock_dfs = {}\n",
    "\n",
    "all_tickers = pd.read_html(\n",
    "    'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "all_stocks = all_tickers.Symbol.to_list()\n",
    "# print(\"SNP500 stocks: \"+str(stocks))\n",
    "\n",
    "tech = ['GOOG', 'T', 'EA', 'META', 'NFLX', 'AMZN', 'ABNB', 'ADBE', 'ORCL', 'CRM',]\n",
    "comm = ['T', 'VZ', 'CMCSA', 'TMUS', 'CHTR']\n",
    "defense = ['BA', 'GE', 'GD', 'LHX', 'LMT', 'NOC', 'RTX']\n",
    "finance = ['BK', 'BX', 'IVZ', 'STT', 'TROW', 'BAC', 'C', 'FITB', 'JPM', 'WFC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "78sGmzrkvtV-",
    "outputId": "38729c41-e3e1-4a7f-89c5-6f0cc8c4c0eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BRK.B']: Exception('%ticker%: No timezone found, symbol may be delisted')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['BF.B']: Exception('%ticker%: No price data found, symbol may be delisted (1h 2023-01-01 -> 2024-01-01)')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['CPAY']: Exception('%ticker%: No price data found, symbol may be delisted (1h 2023-01-01 -> 2024-01-01)')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['DAY']: Exception('%ticker%: No price data found, symbol may be delisted (1h 2023-01-01 -> 2024-01-01)')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['GEV']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1704085200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['SOLV']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1704085200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "full_stock_data = pull_stock_data(all_stocks, '2023-01-01', '2024-01-01', '1h')\n",
    "full_stock_data = pull_stock_indicators(full_stock_data)\n",
    "full_stock_data = cleaning_data(full_stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "teAj77HGpzSA",
    "outputId": "d31c93e4-bd82-4250-8e05-0e0dda2f82f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Setting up data for AAPL\n",
    "tech_stock_data = pull_stock_data(tech, '2023-01-01', '2024-01-01', '1h')\n",
    "tech_stock_data = pull_stock_indicators(tech_stock_data)\n",
    "tech_stock_data = cleaning_data(tech_stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ubm3SjrW5VcC"
   },
   "outputs": [],
   "source": [
    "# code for importing stocks by sector\n",
    "\n",
    "industry = None\n",
    "\n",
    "if industry == 'tech':\n",
    "  stock_data = tech_stock_data\n",
    "else:\n",
    "  stock_data = full_stock_data\n",
    "\n",
    "\n",
    "path = os.path.join(r'/Users/rob/Desktop/CS4701/Data')\n",
    "\n",
    "for key in stock_data.keys():\n",
    "  csv = stock_data[key].to_csv(key + '.csv')\n",
    "  files.download(key + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKr7IGNtbl_E"
   },
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knwU22R6tv_4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rober\\.conda\\envs\\orie_3310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as sklp\n",
    "import sklearn.model_selection as sklm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ILGNkZGMuBby",
    "outputId": "f4d0e44c-0fcd-49ae-f0eb-e69d3d200816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tickers = list(stock_data.keys())\n",
    "stock_dfs = []\n",
    "\n",
    "for i in range(0, len(tickers)):\n",
    "  stock_dfs.append(stock_data[tickers[i]])\n",
    "\n",
    "print(len(stock_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6quQgu7xCTw"
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, n_lags, forecast_horizon=1, feature_cols=None, label_col='Close', normalize=True):\n",
    "        self.n_lags = n_lags\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.label_col = label_col\n",
    "\n",
    "        #allow selection of features\n",
    "        if feature_cols:\n",
    "          data = data[feature_cols]\n",
    "        else:\n",
    "          data = data[data.columns]\n",
    "\n",
    "        #normalize features\n",
    "        if normalize:\n",
    "          self.scaler = sklp.MinMaxScaler()\n",
    "          data_scaled = self.scaler.fit_transform(data)\n",
    "\n",
    "        #create usable data from scaled df\n",
    "        self.X, self.y = self.create_sequences(data_scaled)\n",
    "\n",
    "    def create_sequences(self, data):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.n_lags - self.forecast_horizon + 1):\n",
    "          X.append(data[i:i + self.n_lags])\n",
    "          y.append(data[i + self.n_lags + self.forecast_horizon - 1, 3])\n",
    "\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float), torch.tensor(self.y[idx], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6h4zl2Gr3L5L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n"
     ]
    }
   ],
   "source": [
    "#params for dataloader\n",
    "b = 6\n",
    "f = 1\n",
    "\n",
    "num_stocks = 100 # number of stocks to use\n",
    "\n",
    "n_lags = 8 * 21 * b #look back n MONTHS: hours/market-day * days * quantity\n",
    "forecast_horizon = 8 * 5 * f #look ahead f WEEKS (5 trading days/week):\n",
    "\n",
    "ts_datasets = []\n",
    "\n",
    "for i in range(0, num_stocks):\n",
    "  ts_datasets.append(TimeSeriesDataset(stock_dfs[i], n_lags, forecast_horizon))\n",
    "\n",
    "print(len(ts_datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weP9KGeEwgkH"
   },
   "source": [
    "# LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkNm0qdRwgRG"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_layers, input_size, hidden_size, seq_length, num_classes=1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        num_layers: Number of recurrent layers\n",
    "        input_size: Number of features for input\n",
    "        hidden_size: Number of features in hidden state\n",
    "        **these could be wrong**\n",
    "        seq_length: Length of sequences in a batch\n",
    "        num_classes: Number of categories for labels\n",
    "\n",
    "        Outputs: none\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(num_layers * hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      '''\n",
    "      Inputs:\n",
    "      x: input data\n",
    "\n",
    "      Outputs:\n",
    "      out: output of forward pass\n",
    "      '''\n",
    "\n",
    "      out, (hn, cn) = self.lstm(x)\n",
    "\n",
    "      hn = hn.transpose(0, 1).reshape(x.size(0), -1)\n",
    "\n",
    "      x = self.dense(hn)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnUv3scIyIH6"
   },
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiWWd5gexfGo"
   },
   "outputs": [],
   "source": [
    "tvt_datasets = []\n",
    "\n",
    "for i in range(len(ts_datasets)):\n",
    "  train_size = int(0.8 * len(ts_datasets[i]))\n",
    "  val_size = int(0.1 * len(ts_datasets[i]))\n",
    "  test_size = len(ts_datasets[i]) - train_size - val_size\n",
    "  train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(ts_datasets[i], [train_size, val_size, test_size])\n",
    "  tvt_datasets.append((train_dataset, val_dataset, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpuntkUGvd4i"
   },
   "outputs": [],
   "source": [
    "tvt_dataloaders = []\n",
    "\n",
    "for i in range(len(tvt_datasets)):\n",
    "  train_dataset, val_dataset, test_dataset = tvt_datasets[i]\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "  tvt_dataloaders.append((train_dataloader, val_dataloader, test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYTZSJJLyPAT"
   },
   "source": [
    "### Train + Validation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3NO027rxfzr"
   },
   "outputs": [],
   "source": [
    "def val(model, val_loader, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    model (torch.nn.Module): The deep learning model to be trained.\n",
    "    val_data_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "    criterion (torch.nn.Module): Loss function to compute the training loss.\n",
    "\n",
    "    Outputs:\n",
    "    Validation Loss\n",
    "    \"\"\"\n",
    "    val_running_loss = 0\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(val_loader, 0):\n",
    "            labels = labels.view(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "\n",
    "    return val_running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7VOP2b5x8mh"
   },
   "outputs": [],
   "source": [
    "def train(model, tvt_loaders, criterion, epochs, optimizer):\n",
    "  \"\"\"\n",
    "    Inputs:\n",
    "    model (torch.nn.Module): The deep learning model to be trained.\n",
    "    train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n",
    "    val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n",
    "    criterion (torch.nn.Module): Loss function to compute the training loss.\n",
    "    epochs: Number of epochs to train for.\n",
    "    optimizer: The optimizer to use during training.\n",
    "\n",
    "    Outputs:\n",
    "    Tuple of (train_loss_arr, val_loss_arr, val_acc_arr)\n",
    "  \"\"\"\n",
    "  train_loss_arr = []\n",
    "  val_loss_arr = []\n",
    "  model.train()\n",
    "    \n",
    "  count = 0\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      count = 0\n",
    "      for train_loader, val_loader, _ in tvt_loaders:\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        count += 1\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "          labels = labels.view(-1, 1)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          preds = model(inputs)\n",
    "          loss = criterion(preds, labels)\n",
    "\n",
    "          running_loss += loss.item()\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "\n",
    "        val_loss = val(model, val_loader, criterion)\n",
    "        train_loss_arr.append(running_loss)\n",
    "        val_loss_arr.append(val_loss)\n",
    "\n",
    "        print(\"epoch:\", epoch+1, \"trial\", count, \"training loss:\", running_loss)\n",
    "\n",
    "  print(running_loss)\n",
    "  print('Training finished.')\n",
    "\n",
    "  return train_loss_arr, val_loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rva1EfHErlI"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loaders):\n",
    "  model.eval()\n",
    "  print(len(train_dataset))\n",
    "\n",
    "  for i in range(len(test_loaders)):\n",
    "    with torch.no_grad():\n",
    "      for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "        print(i)\n",
    "        print(f\"Input shape: {inputs.shape}\")  # Check input shape\n",
    "        print(f\"Sample input: {inputs[0]}\")     # Inspect the first input of the batch\n",
    "\n",
    "        output = model(inputs)\n",
    "        print(f\"Output: {output}\")  # Print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O62mJnR7x_My"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 trial 1 training loss: 0.17415676638484\n",
      "epoch: 1 trial 2 training loss: 0.20290744584053755\n",
      "epoch: 1 trial 3 training loss: 0.11934223212301731\n",
      "epoch: 1 trial 4 training loss: 0.12200629711151123\n",
      "epoch: 1 trial 5 training loss: 0.6809547692537308\n",
      "epoch: 1 trial 6 training loss: 0.23699750285595655\n",
      "epoch: 1 trial 7 training loss: 0.3530936688184738\n",
      "epoch: 1 trial 8 training loss: 0.5426320433616638\n",
      "epoch: 1 trial 9 training loss: 0.37529049068689346\n",
      "epoch: 1 trial 10 training loss: 0.05788796208798885\n",
      "epoch: 1 trial 11 training loss: 0.09171219170093536\n",
      "epoch: 1 trial 12 training loss: 0.23325257003307343\n",
      "epoch: 1 trial 13 training loss: 0.4740568548440933\n",
      "epoch: 1 trial 14 training loss: 0.5988186746835709\n",
      "epoch: 1 trial 15 training loss: 0.14093885198235512\n",
      "epoch: 1 trial 16 training loss: 0.09825443476438522\n",
      "epoch: 1 trial 17 training loss: 0.22428283467888832\n",
      "epoch: 1 trial 18 training loss: 0.07008695974946022\n",
      "epoch: 1 trial 19 training loss: 0.18265777453780174\n",
      "epoch: 1 trial 20 training loss: 0.04324306920170784\n",
      "epoch: 1 trial 21 training loss: 0.10316772386431694\n",
      "epoch: 1 trial 22 training loss: 0.31103260070085526\n",
      "epoch: 1 trial 23 training loss: 0.2644093185663223\n",
      "epoch: 1 trial 24 training loss: 0.021374518517404795\n",
      "epoch: 1 trial 25 training loss: 0.15091707929968834\n",
      "epoch: 1 trial 26 training loss: 0.033664146438241005\n",
      "epoch: 1 trial 27 training loss: 0.03870195522904396\n",
      "epoch: 1 trial 28 training loss: 0.4092639982700348\n",
      "epoch: 1 trial 29 training loss: 0.12653022399172187\n",
      "epoch: 1 trial 30 training loss: 0.11415400356054306\n",
      "epoch: 1 trial 31 training loss: 0.21458065882325172\n",
      "epoch: 1 trial 32 training loss: 0.025832635816186666\n",
      "epoch: 1 trial 33 training loss: 0.08164242096245289\n",
      "epoch: 1 trial 34 training loss: 0.01873401366174221\n",
      "epoch: 1 trial 35 training loss: 0.024150054436177015\n",
      "epoch: 1 trial 36 training loss: 0.04963390715420246\n",
      "epoch: 1 trial 37 training loss: 0.190634123980999\n",
      "epoch: 1 trial 38 training loss: 0.34530341252684593\n",
      "epoch: 1 trial 39 training loss: 0.023422556463629007\n",
      "epoch: 1 trial 40 training loss: 0.6898747980594635\n",
      "epoch: 1 trial 41 training loss: 0.3977731317281723\n",
      "epoch: 1 trial 42 training loss: 0.09429879672825336\n",
      "epoch: 1 trial 43 training loss: 0.18655481934547424\n",
      "epoch: 1 trial 44 training loss: 0.13565838150680065\n",
      "epoch: 1 trial 45 training loss: 0.17383120954036713\n",
      "epoch: 1 trial 46 training loss: 0.06271358765661716\n",
      "epoch: 1 trial 47 training loss: 0.010642882203683257\n",
      "epoch: 1 trial 48 training loss: 0.06143535114824772\n",
      "epoch: 1 trial 49 training loss: 0.05100331734865904\n",
      "epoch: 1 trial 50 training loss: 0.0806417278945446\n",
      "epoch: 1 trial 51 training loss: 0.0430916603654623\n",
      "epoch: 1 trial 52 training loss: 0.09846203494817019\n",
      "epoch: 1 trial 53 training loss: 0.17286885157227516\n",
      "epoch: 1 trial 54 training loss: 0.24456575512886047\n",
      "epoch: 1 trial 55 training loss: 0.05434722173959017\n",
      "epoch: 1 trial 56 training loss: 0.1868051066994667\n",
      "epoch: 1 trial 57 training loss: 0.019727856386452913\n",
      "epoch: 1 trial 58 training loss: 0.05592218041419983\n",
      "epoch: 1 trial 59 training loss: 0.13091185688972473\n",
      "epoch: 1 trial 60 training loss: 0.1492828205227852\n",
      "epoch: 1 trial 61 training loss: 0.10065378062427044\n",
      "epoch: 1 trial 62 training loss: 0.20469128713011742\n",
      "epoch: 1 trial 63 training loss: 0.021488984115421772\n",
      "epoch: 1 trial 64 training loss: 0.004621146072167903\n",
      "epoch: 1 trial 65 training loss: 0.2306680679321289\n",
      "epoch: 1 trial 66 training loss: 0.4512312561273575\n",
      "epoch: 1 trial 67 training loss: 0.14379999041557312\n",
      "epoch: 1 trial 68 training loss: 0.020624756580218673\n",
      "epoch: 1 trial 69 training loss: 0.20082440227270126\n",
      "epoch: 1 trial 70 training loss: 0.26893143355846405\n",
      "epoch: 1 trial 71 training loss: 0.055094070732593536\n",
      "epoch: 1 trial 72 training loss: 0.05377658363431692\n",
      "epoch: 1 trial 73 training loss: 0.09179005399346352\n",
      "epoch: 1 trial 74 training loss: 0.13936614990234375\n",
      "epoch: 1 trial 75 training loss: 0.04677565861493349\n",
      "epoch: 1 trial 76 training loss: 0.03325647581368685\n",
      "epoch: 1 trial 77 training loss: 0.0394715229049325\n",
      "epoch: 1 trial 78 training loss: 0.15956501103937626\n",
      "epoch: 1 trial 79 training loss: 0.08910192735493183\n",
      "epoch: 1 trial 80 training loss: 0.27036264911293983\n",
      "epoch: 1 trial 81 training loss: 0.03352993819862604\n",
      "epoch: 1 trial 82 training loss: 0.051148186437785625\n",
      "epoch: 1 trial 83 training loss: 0.064250772818923\n",
      "epoch: 1 trial 84 training loss: 0.022017556242644787\n",
      "epoch: 1 trial 85 training loss: 0.09890168718993664\n",
      "epoch: 1 trial 86 training loss: 0.020077495370060205\n",
      "epoch: 1 trial 87 training loss: 0.02675251243636012\n",
      "epoch: 1 trial 88 training loss: 0.0725465677678585\n",
      "epoch: 1 trial 89 training loss: 0.07650940492749214\n",
      "epoch: 1 trial 90 training loss: 0.014089512405917048\n",
      "epoch: 1 trial 91 training loss: 0.0289047472178936\n",
      "epoch: 1 trial 92 training loss: 0.06058345548808575\n",
      "epoch: 1 trial 93 training loss: 0.01433757459744811\n",
      "epoch: 1 trial 94 training loss: 0.03791238693520427\n",
      "epoch: 1 trial 95 training loss: 0.0428877342492342\n",
      "epoch: 1 trial 96 training loss: 0.027051165234297514\n",
      "epoch: 1 trial 97 training loss: 0.019456636626273394\n",
      "epoch: 1 trial 98 training loss: 0.11822068691253662\n",
      "epoch: 1 trial 99 training loss: 0.0347261056303978\n",
      "epoch: 1 trial 100 training loss: 0.04431535769253969\n",
      "epoch: 1 trial 101 training loss: 0.07600411772727966\n",
      "epoch: 1 trial 102 training loss: 0.14104386419057846\n",
      "epoch: 1 trial 103 training loss: 0.02955930307507515\n",
      "epoch: 1 trial 104 training loss: 0.12027961388230324\n",
      "epoch: 1 trial 105 training loss: 0.28303563594818115\n",
      "epoch: 1 trial 106 training loss: 0.053118811920285225\n",
      "epoch: 1 trial 107 training loss: 0.24777860194444656\n",
      "epoch: 1 trial 108 training loss: 0.07648347690701485\n",
      "epoch: 1 trial 109 training loss: 0.21263786032795906\n",
      "epoch: 1 trial 110 training loss: 0.014502294827252626\n",
      "epoch: 1 trial 111 training loss: 0.0211156839504838\n",
      "epoch: 1 trial 112 training loss: 0.05694758240133524\n",
      "epoch: 1 trial 113 training loss: 0.05592060089111328\n",
      "epoch: 1 trial 114 training loss: 0.04256751947104931\n",
      "epoch: 1 trial 115 training loss: 0.06544170156121254\n",
      "epoch: 1 trial 116 training loss: 0.012133221374824643\n",
      "epoch: 1 trial 117 training loss: 0.023607433773577213\n",
      "epoch: 1 trial 118 training loss: 0.026117888744920492\n",
      "epoch: 1 trial 119 training loss: 0.028308644890785217\n",
      "epoch: 1 trial 120 training loss: 0.020495129749178886\n",
      "epoch: 1 trial 121 training loss: 0.007552007446065545\n",
      "epoch: 1 trial 122 training loss: 0.08286783844232559\n",
      "epoch: 1 trial 123 training loss: 0.08586927503347397\n",
      "epoch: 1 trial 124 training loss: 0.027375779580324888\n",
      "epoch: 1 trial 125 training loss: 0.015211706515401602\n",
      "epoch: 1 trial 126 training loss: 0.05285574868321419\n",
      "epoch: 1 trial 127 training loss: 0.036523519083857536\n",
      "epoch: 1 trial 128 training loss: 0.03293764125555754\n",
      "epoch: 1 trial 129 training loss: 0.112452682107687\n",
      "epoch: 1 trial 130 training loss: 0.04835938010364771\n",
      "epoch: 1 trial 131 training loss: 0.22170433402061462\n",
      "epoch: 1 trial 132 training loss: 0.039166188798844814\n",
      "epoch: 1 trial 133 training loss: 0.03046862967312336\n",
      "epoch: 1 trial 134 training loss: 0.06716318614780903\n",
      "epoch: 1 trial 135 training loss: 0.043012949638068676\n",
      "epoch: 1 trial 136 training loss: 0.024253579787909985\n",
      "epoch: 1 trial 137 training loss: 0.04448463395237923\n",
      "epoch: 1 trial 138 training loss: 0.12805823236703873\n",
      "epoch: 1 trial 139 training loss: 0.020071410574018955\n",
      "epoch: 1 trial 140 training loss: 0.07036658003926277\n",
      "epoch: 1 trial 141 training loss: 0.03919802885502577\n",
      "epoch: 1 trial 142 training loss: 0.02209822926670313\n",
      "epoch: 1 trial 143 training loss: 0.0443549333140254\n",
      "epoch: 1 trial 144 training loss: 0.03923607338219881\n",
      "epoch: 1 trial 145 training loss: 0.06404516845941544\n",
      "epoch: 1 trial 146 training loss: 0.03523875307291746\n",
      "epoch: 1 trial 147 training loss: 0.061481635086238384\n",
      "epoch: 1 trial 148 training loss: 0.020447320304811\n",
      "epoch: 1 trial 149 training loss: 0.15114052221179008\n",
      "epoch: 1 trial 150 training loss: 0.1395753249526024\n",
      "epoch: 1 trial 151 training loss: 0.03899221774190664\n",
      "epoch: 1 trial 152 training loss: 0.06024963594973087\n",
      "epoch: 1 trial 153 training loss: 0.0311962952837348\n",
      "epoch: 1 trial 154 training loss: 0.08903385512530804\n",
      "epoch: 1 trial 155 training loss: 0.012437013443559408\n",
      "epoch: 1 trial 156 training loss: 0.015040075406432152\n",
      "epoch: 1 trial 157 training loss: 0.017649520188570023\n",
      "epoch: 1 trial 158 training loss: 0.009445341187529266\n",
      "epoch: 1 trial 159 training loss: 0.009011298650875688\n",
      "epoch: 1 trial 160 training loss: 0.023145401384681463\n",
      "epoch: 1 trial 161 training loss: 0.027738338336348534\n",
      "epoch: 1 trial 162 training loss: 0.06121835671365261\n",
      "epoch: 1 trial 163 training loss: 0.018647102173417807\n",
      "epoch: 1 trial 164 training loss: 0.07599116675555706\n",
      "epoch: 1 trial 165 training loss: 0.022269290406256914\n",
      "epoch: 1 trial 166 training loss: 0.02562065701931715\n",
      "epoch: 1 trial 167 training loss: 0.03270912962034345\n",
      "epoch: 1 trial 168 training loss: 0.011040115030482411\n",
      "epoch: 1 trial 169 training loss: 0.02583258179947734\n",
      "epoch: 1 trial 170 training loss: 0.18847351148724556\n",
      "epoch: 1 trial 171 training loss: 0.04257972864434123\n",
      "epoch: 1 trial 172 training loss: 0.019165356177836657\n",
      "epoch: 1 trial 173 training loss: 0.05917029082775116\n",
      "epoch: 1 trial 174 training loss: 0.047699688002467155\n",
      "epoch: 1 trial 175 training loss: 0.04432763438671827\n",
      "epoch: 1 trial 176 training loss: 0.01714659365825355\n",
      "epoch: 1 trial 177 training loss: 0.0026476214407011867\n",
      "epoch: 1 trial 178 training loss: 0.054063791409134865\n",
      "epoch: 1 trial 179 training loss: 0.045620148070156574\n",
      "epoch: 1 trial 180 training loss: 0.0441885506734252\n",
      "epoch: 1 trial 181 training loss: 0.044500465504825115\n",
      "epoch: 1 trial 182 training loss: 0.009752194862812757\n",
      "epoch: 1 trial 183 training loss: 0.07068116404116154\n",
      "epoch: 1 trial 184 training loss: 0.04749137070029974\n",
      "epoch: 1 trial 185 training loss: 0.025883257389068604\n",
      "epoch: 1 trial 186 training loss: 0.05648996960371733\n",
      "epoch: 1 trial 187 training loss: 0.08610677346587181\n",
      "epoch: 1 trial 188 training loss: 0.027905580587685108\n",
      "epoch: 1 trial 189 training loss: 0.06256907805800438\n",
      "epoch: 1 trial 190 training loss: 0.04958903230726719\n",
      "epoch: 1 trial 191 training loss: 0.04643669445067644\n",
      "epoch: 1 trial 192 training loss: 0.06927753612399101\n",
      "epoch: 1 trial 193 training loss: 0.03978502191603184\n",
      "epoch: 1 trial 194 training loss: 0.016958967316895723\n",
      "epoch: 1 trial 195 training loss: 0.032733017578721046\n",
      "epoch: 1 trial 196 training loss: 0.036457743495702744\n",
      "epoch: 1 trial 197 training loss: 0.028710953891277313\n",
      "epoch: 1 trial 198 training loss: 0.057760149240493774\n",
      "epoch: 1 trial 199 training loss: 0.08521893061697483\n",
      "epoch: 1 trial 200 training loss: 0.04279594914987683\n",
      "epoch: 1 trial 201 training loss: 0.01766233635134995\n",
      "epoch: 1 trial 202 training loss: 0.03379247058182955\n",
      "epoch: 1 trial 203 training loss: 0.00484699453227222\n",
      "epoch: 1 trial 204 training loss: 0.020676969550549984\n",
      "epoch: 1 trial 205 training loss: 0.04523376654833555\n",
      "epoch: 1 trial 206 training loss: 0.05305827967822552\n",
      "epoch: 1 trial 207 training loss: 0.0362962931394577\n",
      "epoch: 1 trial 208 training loss: 0.025311633944511414\n",
      "epoch: 1 trial 209 training loss: 0.11126157641410828\n",
      "epoch: 1 trial 210 training loss: 0.04985773377120495\n",
      "epoch: 1 trial 211 training loss: 0.0689101004973054\n",
      "epoch: 1 trial 212 training loss: 0.04841240495443344\n",
      "epoch: 1 trial 213 training loss: 0.2915474697947502\n",
      "epoch: 1 trial 214 training loss: 0.042382299434393644\n",
      "epoch: 1 trial 215 training loss: 0.022640543524175882\n",
      "epoch: 1 trial 216 training loss: 0.06441390700638294\n",
      "epoch: 1 trial 217 training loss: 0.04521371331065893\n",
      "epoch: 1 trial 218 training loss: 0.03080684971064329\n",
      "epoch: 1 trial 219 training loss: 0.06961908005177975\n",
      "epoch: 1 trial 220 training loss: 0.05617947317659855\n",
      "epoch: 1 trial 221 training loss: 0.02757444977760315\n",
      "epoch: 1 trial 222 training loss: 0.07800666429102421\n",
      "epoch: 1 trial 223 training loss: 0.08487993478775024\n",
      "epoch: 1 trial 224 training loss: 0.04087536968290806\n",
      "epoch: 1 trial 225 training loss: 0.01904349960386753\n",
      "epoch: 1 trial 226 training loss: 0.039313788525760174\n",
      "epoch: 1 trial 227 training loss: 0.021591962315142155\n",
      "epoch: 1 trial 228 training loss: 0.008036698680371046\n",
      "epoch: 1 trial 229 training loss: 0.025927245151251554\n",
      "epoch: 1 trial 230 training loss: 0.016535349190235138\n",
      "epoch: 1 trial 231 training loss: 0.14147736877202988\n",
      "epoch: 1 trial 232 training loss: 0.07517522387206554\n",
      "epoch: 1 trial 233 training loss: 0.16292114928364754\n",
      "epoch: 1 trial 234 training loss: 0.02376246394123882\n",
      "epoch: 1 trial 235 training loss: 0.013854865450412035\n",
      "epoch: 1 trial 236 training loss: 0.07641996257007122\n",
      "epoch: 1 trial 237 training loss: 0.04691136162728071\n",
      "epoch: 1 trial 238 training loss: 0.03269198723137379\n",
      "epoch: 1 trial 239 training loss: 0.03367707785218954\n",
      "epoch: 1 trial 240 training loss: 0.0646918136626482\n",
      "epoch: 1 trial 241 training loss: 0.003369353071320802\n",
      "epoch: 1 trial 242 training loss: 0.02989569678902626\n",
      "epoch: 1 trial 243 training loss: 0.03459893353283405\n",
      "epoch: 1 trial 244 training loss: 0.016003160271793604\n",
      "epoch: 1 trial 245 training loss: 0.09781762957572937\n",
      "epoch: 1 trial 246 training loss: 0.008785276440903544\n",
      "epoch: 1 trial 247 training loss: 0.042844513431191444\n",
      "epoch: 1 trial 248 training loss: 0.030882274731993675\n",
      "epoch: 1 trial 249 training loss: 0.04306778497993946\n",
      "epoch: 1 trial 250 training loss: 0.0991663970053196\n",
      "epoch: 1 trial 251 training loss: 0.01467110589146614\n",
      "epoch: 1 trial 252 training loss: 0.011322881793603301\n",
      "epoch: 1 trial 253 training loss: 0.03806342929601669\n",
      "epoch: 1 trial 254 training loss: 0.08537397533655167\n",
      "epoch: 1 trial 255 training loss: 0.017279347870498896\n",
      "epoch: 1 trial 256 training loss: 0.052055712789297104\n",
      "epoch: 1 trial 257 training loss: 0.02290386101230979\n",
      "epoch: 1 trial 258 training loss: 0.019071492832154036\n",
      "epoch: 1 trial 259 training loss: 0.2996869310736656\n",
      "epoch: 1 trial 260 training loss: 0.013808749616146088\n",
      "epoch: 1 trial 261 training loss: 0.028591372072696686\n",
      "epoch: 1 trial 262 training loss: 0.011892263777554035\n",
      "epoch: 1 trial 263 training loss: 0.008479116251692176\n",
      "epoch: 1 trial 264 training loss: 0.015140999108552933\n",
      "epoch: 1 trial 265 training loss: 0.04763831291347742\n",
      "epoch: 1 trial 266 training loss: 0.0543229766190052\n",
      "epoch: 1 trial 267 training loss: 0.03912716917693615\n",
      "epoch: 1 trial 268 training loss: 0.030655354261398315\n",
      "epoch: 1 trial 269 training loss: 0.03889050427824259\n",
      "epoch: 1 trial 270 training loss: 0.08171265013515949\n",
      "epoch: 1 trial 271 training loss: 0.013063199818134308\n",
      "epoch: 1 trial 272 training loss: 0.008184681413695216\n",
      "epoch: 1 trial 273 training loss: 0.021711405832320452\n",
      "epoch: 1 trial 274 training loss: 0.005362940370105207\n",
      "epoch: 1 trial 275 training loss: 0.040143768303096294\n",
      "epoch: 1 trial 276 training loss: 0.006379625527188182\n",
      "epoch: 1 trial 277 training loss: 0.0234001986682415\n",
      "epoch: 1 trial 278 training loss: 0.02029939368367195\n",
      "epoch: 1 trial 279 training loss: 0.053423977456986904\n",
      "epoch: 1 trial 280 training loss: 0.019734765402972698\n",
      "epoch: 1 trial 281 training loss: 0.003156113438308239\n",
      "epoch: 1 trial 282 training loss: 0.10146159864962101\n",
      "epoch: 1 trial 283 training loss: 0.08145507425069809\n",
      "epoch: 1 trial 284 training loss: 0.04918501619249582\n",
      "epoch: 1 trial 285 training loss: 0.1593693420290947\n",
      "epoch: 1 trial 286 training loss: 0.024844753555953503\n",
      "epoch: 1 trial 287 training loss: 0.02974804351106286\n",
      "epoch: 1 trial 288 training loss: 0.179103322327137\n",
      "epoch: 1 trial 289 training loss: 0.02300347900018096\n",
      "epoch: 1 trial 290 training loss: 0.06939111463725567\n",
      "epoch: 1 trial 291 training loss: 0.0585069265216589\n",
      "epoch: 1 trial 292 training loss: 0.02377154491841793\n",
      "epoch: 1 trial 293 training loss: 0.015940548852086067\n",
      "epoch: 1 trial 294 training loss: 0.01178898336365819\n",
      "epoch: 1 trial 295 training loss: 0.03281665127724409\n",
      "epoch: 1 trial 296 training loss: 0.035189904272556305\n",
      "epoch: 1 trial 297 training loss: 0.015809490578249097\n",
      "epoch: 1 trial 298 training loss: 0.01020401855930686\n",
      "epoch: 1 trial 299 training loss: 0.04253075737506151\n",
      "epoch: 1 trial 300 training loss: 0.03071080520749092\n",
      "epoch: 1 trial 301 training loss: 0.012592957587912679\n",
      "epoch: 1 trial 302 training loss: 0.006434128968976438\n",
      "epoch: 1 trial 303 training loss: 0.017244125250726938\n",
      "epoch: 1 trial 304 training loss: 0.03191936481744051\n",
      "epoch: 1 trial 305 training loss: 0.06185505725443363\n",
      "epoch: 1 trial 306 training loss: 0.04933199193328619\n",
      "epoch: 1 trial 307 training loss: 0.004168357118032873\n",
      "epoch: 1 trial 308 training loss: 0.025194581132382154\n",
      "epoch: 1 trial 309 training loss: 0.042458673007786274\n",
      "epoch: 1 trial 310 training loss: 0.01876183459535241\n",
      "epoch: 1 trial 311 training loss: 0.011990586295723915\n",
      "epoch: 1 trial 312 training loss: 0.018663977272808552\n",
      "epoch: 1 trial 313 training loss: 0.062132807448506355\n",
      "epoch: 1 trial 314 training loss: 0.02118057431653142\n",
      "epoch: 1 trial 315 training loss: 0.037200688384473324\n",
      "epoch: 1 trial 316 training loss: 0.05794052593410015\n",
      "epoch: 1 trial 317 training loss: 0.039115553721785545\n",
      "epoch: 1 trial 318 training loss: 0.0897995475679636\n",
      "epoch: 1 trial 319 training loss: 0.06900985259562731\n",
      "epoch: 1 trial 320 training loss: 0.02002635132521391\n",
      "epoch: 1 trial 321 training loss: 0.16790048405528069\n",
      "epoch: 1 trial 322 training loss: 0.03133315034210682\n",
      "epoch: 1 trial 323 training loss: 0.18232411891222\n",
      "epoch: 1 trial 324 training loss: 0.03929059859365225\n",
      "epoch: 1 trial 325 training loss: 0.03995053190737963\n",
      "epoch: 1 trial 326 training loss: 0.005493143456988037\n",
      "epoch: 1 trial 327 training loss: 0.0754147469997406\n",
      "epoch: 1 trial 328 training loss: 0.019893386866897345\n",
      "epoch: 1 trial 329 training loss: 0.08725934475660324\n",
      "epoch: 1 trial 330 training loss: 0.0199530478566885\n",
      "epoch: 1 trial 331 training loss: 0.043875426054000854\n",
      "epoch: 1 trial 332 training loss: 0.08913012221455574\n",
      "epoch: 1 trial 333 training loss: 0.09224065020680428\n",
      "epoch: 1 trial 334 training loss: 0.05799086578190327\n",
      "epoch: 1 trial 335 training loss: 0.03735572658479214\n",
      "epoch: 1 trial 336 training loss: 0.008227992337197065\n",
      "epoch: 1 trial 337 training loss: 0.05492977239191532\n",
      "epoch: 1 trial 338 training loss: 0.027594097889959812\n",
      "epoch: 1 trial 339 training loss: 0.019406826235353947\n",
      "epoch: 1 trial 340 training loss: 0.21098046749830246\n",
      "epoch: 1 trial 341 training loss: 0.07365275174379349\n",
      "epoch: 1 trial 342 training loss: 0.024172545410692692\n",
      "epoch: 1 trial 343 training loss: 0.029802738688886166\n",
      "epoch: 1 trial 344 training loss: 0.1848246157169342\n",
      "epoch: 1 trial 345 training loss: 0.06191139295697212\n",
      "epoch: 1 trial 346 training loss: 0.020985383074730635\n",
      "epoch: 1 trial 347 training loss: 0.008508807281032205\n",
      "epoch: 1 trial 348 training loss: 0.05254686530679464\n",
      "epoch: 1 trial 349 training loss: 0.014853737317025661\n",
      "epoch: 1 trial 350 training loss: 0.065500533208251\n",
      "epoch: 1 trial 351 training loss: 0.04788228124380112\n",
      "epoch: 1 trial 352 training loss: 0.06436592526733875\n",
      "epoch: 1 trial 353 training loss: 0.04766461718827486\n",
      "epoch: 1 trial 354 training loss: 0.029542440082877874\n",
      "epoch: 1 trial 355 training loss: 0.018080977257341146\n",
      "epoch: 1 trial 356 training loss: 0.006132121430709958\n",
      "epoch: 1 trial 357 training loss: 0.04039756953716278\n",
      "epoch: 1 trial 358 training loss: 0.07899703457951546\n",
      "epoch: 1 trial 359 training loss: 0.04508876334875822\n",
      "epoch: 1 trial 360 training loss: 0.021195750683546066\n",
      "epoch: 1 trial 361 training loss: 0.08556663431227207\n",
      "epoch: 1 trial 362 training loss: 0.09004986844956875\n",
      "epoch: 1 trial 363 training loss: 0.11455336585640907\n",
      "epoch: 1 trial 364 training loss: 0.08462957106530666\n",
      "epoch: 1 trial 365 training loss: 0.016433726530522108\n",
      "epoch: 1 trial 366 training loss: 0.11213752813637257\n",
      "epoch: 1 trial 367 training loss: 0.036245775409042835\n",
      "epoch: 1 trial 368 training loss: 0.05147985089570284\n",
      "epoch: 1 trial 369 training loss: 0.27699221670627594\n",
      "epoch: 1 trial 370 training loss: 0.17956766858696938\n",
      "epoch: 1 trial 371 training loss: 0.08121196366846561\n",
      "epoch: 1 trial 372 training loss: 0.032913606613874435\n",
      "epoch: 1 trial 373 training loss: 0.021852050442248583\n",
      "epoch: 1 trial 374 training loss: 0.024695278611034155\n",
      "epoch: 1 trial 375 training loss: 0.05274498090147972\n",
      "epoch: 1 trial 376 training loss: 0.03518377244472504\n",
      "epoch: 1 trial 377 training loss: 0.03327231481671333\n",
      "epoch: 1 trial 378 training loss: 0.06803212594240904\n",
      "epoch: 1 trial 379 training loss: 0.06604023650288582\n",
      "epoch: 1 trial 380 training loss: 0.016989007126539946\n",
      "epoch: 1 trial 381 training loss: 0.04121440462768078\n",
      "epoch: 1 trial 382 training loss: 0.004562549409456551\n",
      "epoch: 1 trial 383 training loss: 0.03466619085520506\n",
      "epoch: 1 trial 384 training loss: 0.038923694752156734\n",
      "epoch: 1 trial 385 training loss: 0.022050839848816395\n",
      "epoch: 1 trial 386 training loss: 0.0038966917200013995\n",
      "epoch: 1 trial 387 training loss: 0.018830185290426016\n",
      "epoch: 1 trial 388 training loss: 0.01897594053298235\n",
      "epoch: 1 trial 389 training loss: 0.060241738334298134\n",
      "epoch: 1 trial 390 training loss: 0.006712127244099975\n",
      "epoch: 1 trial 391 training loss: 0.0028498551109805703\n",
      "epoch: 1 trial 392 training loss: 0.0035003328230232\n",
      "epoch: 1 trial 393 training loss: 0.008164777886122465\n",
      "epoch: 1 trial 394 training loss: 0.011547291651368141\n",
      "epoch: 1 trial 395 training loss: 0.04286835063248873\n",
      "epoch: 1 trial 396 training loss: 0.11103908345103264\n",
      "epoch: 1 trial 397 training loss: 0.076773252338171\n",
      "epoch: 1 trial 398 training loss: 0.02778037264943123\n",
      "epoch: 1 trial 399 training loss: 0.03405733359977603\n",
      "epoch: 1 trial 400 training loss: 0.01231368281878531\n",
      "epoch: 1 trial 401 training loss: 0.021493814885616302\n",
      "epoch: 1 trial 402 training loss: 0.0399490213021636\n",
      "epoch: 1 trial 403 training loss: 0.06974541395902634\n",
      "epoch: 1 trial 404 training loss: 0.0969870900735259\n",
      "epoch: 1 trial 405 training loss: 0.04972404334694147\n",
      "epoch: 1 trial 406 training loss: 0.030558321624994278\n",
      "epoch: 1 trial 407 training loss: 0.17017585784196854\n",
      "epoch: 1 trial 408 training loss: 0.04247341398149729\n",
      "epoch: 1 trial 409 training loss: 0.02015651250258088\n",
      "epoch: 1 trial 410 training loss: 0.04871935024857521\n",
      "epoch: 1 trial 411 training loss: 0.07675499096512794\n",
      "epoch: 1 trial 412 training loss: 0.03565716743469238\n",
      "epoch: 1 trial 413 training loss: 0.022587915416806936\n",
      "epoch: 1 trial 414 training loss: 0.0715306643396616\n",
      "epoch: 1 trial 415 training loss: 0.026433496735990047\n",
      "epoch: 1 trial 416 training loss: 0.07047133520245552\n",
      "epoch: 1 trial 417 training loss: 0.030495384242385626\n",
      "epoch: 1 trial 418 training loss: 0.048971038311719894\n",
      "epoch: 1 trial 419 training loss: 0.09379696100950241\n",
      "epoch: 1 trial 420 training loss: 0.005067632533609867\n",
      "epoch: 1 trial 421 training loss: 0.017931703943759203\n",
      "epoch: 1 trial 422 training loss: 0.05884491465985775\n",
      "epoch: 1 trial 423 training loss: 0.040789864026010036\n",
      "epoch: 1 trial 424 training loss: 0.03581623360514641\n",
      "epoch: 1 trial 425 training loss: 0.010640262393280864\n",
      "epoch: 1 trial 426 training loss: 0.05211340542882681\n",
      "epoch: 1 trial 427 training loss: 0.04038693197071552\n",
      "epoch: 1 trial 428 training loss: 0.01073493529111147\n",
      "epoch: 1 trial 429 training loss: 0.02295447140932083\n",
      "epoch: 1 trial 430 training loss: 0.010321524227038026\n",
      "epoch: 1 trial 431 training loss: 0.018122512847185135\n",
      "epoch: 1 trial 432 training loss: 0.04030230734497309\n",
      "epoch: 1 trial 433 training loss: 0.032552361488342285\n",
      "epoch: 1 trial 434 training loss: 0.014393492136150599\n",
      "epoch: 1 trial 435 training loss: 0.018811844289302826\n",
      "epoch: 1 trial 436 training loss: 0.018208160530775785\n",
      "epoch: 1 trial 437 training loss: 0.01107290806248784\n",
      "epoch: 1 trial 438 training loss: 0.05559984128922224\n",
      "epoch: 1 trial 439 training loss: 0.020320522598922253\n",
      "epoch: 1 trial 440 training loss: 0.01829575514420867\n",
      "epoch: 1 trial 441 training loss: 0.07669151946902275\n",
      "epoch: 1 trial 442 training loss: 0.020021239761263132\n",
      "epoch: 1 trial 443 training loss: 0.09697675332427025\n",
      "epoch: 1 trial 444 training loss: 0.014992430806159973\n",
      "epoch: 1 trial 445 training loss: 0.08371879532933235\n",
      "epoch: 1 trial 446 training loss: 0.021443146746605635\n",
      "epoch: 1 trial 447 training loss: 0.06964940764009953\n",
      "epoch: 1 trial 448 training loss: 0.037714567966759205\n",
      "epoch: 1 trial 449 training loss: 0.04361744225025177\n",
      "epoch: 1 trial 450 training loss: 0.032602133229374886\n",
      "epoch: 1 trial 451 training loss: 0.012144106905907393\n",
      "epoch: 1 trial 452 training loss: 0.13025855273008347\n",
      "epoch: 1 trial 453 training loss: 0.007472602883353829\n",
      "epoch: 1 trial 454 training loss: 0.01946956431493163\n",
      "epoch: 1 trial 455 training loss: 0.1418776959180832\n",
      "epoch: 1 trial 456 training loss: 0.03644950781017542\n",
      "epoch: 1 trial 457 training loss: 0.021670933812856674\n",
      "epoch: 1 trial 458 training loss: 0.003702152462210506\n",
      "epoch: 1 trial 459 training loss: 0.022922107949852943\n",
      "epoch: 1 trial 460 training loss: 0.02341307047754526\n",
      "epoch: 1 trial 461 training loss: 0.015976558905094862\n",
      "epoch: 1 trial 462 training loss: 0.045202696695923805\n",
      "epoch: 1 trial 463 training loss: 0.10433156788349152\n",
      "epoch: 1 trial 464 training loss: 0.048703959211707115\n",
      "epoch: 1 trial 465 training loss: 0.017505628988146782\n",
      "epoch: 1 trial 466 training loss: 0.2933803051710129\n",
      "epoch: 1 trial 467 training loss: 0.126368448138237\n",
      "epoch: 1 trial 468 training loss: 0.03537316434085369\n",
      "epoch: 1 trial 469 training loss: 0.034546177834272385\n",
      "epoch: 1 trial 470 training loss: 0.028545931912958622\n",
      "epoch: 1 trial 471 training loss: 0.10314776375889778\n",
      "epoch: 1 trial 472 training loss: 0.030323610175400972\n",
      "epoch: 1 trial 473 training loss: 0.07077185809612274\n",
      "epoch: 1 trial 474 training loss: 0.11300791800022125\n",
      "epoch: 1 trial 475 training loss: 0.033024963922798634\n",
      "epoch: 1 trial 476 training loss: 0.016009454615414143\n",
      "epoch: 1 trial 477 training loss: 0.021447245497256517\n",
      "epoch: 1 trial 478 training loss: 0.11234357953071594\n",
      "epoch: 1 trial 479 training loss: 0.026822383981198072\n",
      "epoch: 1 trial 480 training loss: 0.025478525087237358\n",
      "epoch: 1 trial 481 training loss: 0.045161846093833447\n",
      "epoch: 1 trial 482 training loss: 0.11398391053080559\n",
      "epoch: 1 trial 483 training loss: 0.010009335586801171\n",
      "epoch: 1 trial 484 training loss: 0.03889234736561775\n",
      "epoch: 2 trial 485 training loss: 0.029766950756311417\n",
      "epoch: 2 trial 486 training loss: 0.013195984065532684\n",
      "epoch: 2 trial 487 training loss: 0.013860276667401195\n",
      "epoch: 2 trial 488 training loss: 0.06471411511301994\n",
      "epoch: 2 trial 489 training loss: 0.009268155321478844\n",
      "epoch: 2 trial 490 training loss: 0.006697740405797958\n",
      "epoch: 2 trial 491 training loss: 0.030280373990535736\n",
      "epoch: 2 trial 492 training loss: 0.02136304136365652\n",
      "epoch: 2 trial 493 training loss: 0.00745460030157119\n",
      "epoch: 2 trial 494 training loss: 0.015448186546564102\n",
      "epoch: 2 trial 495 training loss: 0.048548655584454536\n",
      "epoch: 2 trial 496 training loss: 0.02820818079635501\n",
      "epoch: 2 trial 497 training loss: 0.013290823204442859\n",
      "epoch: 2 trial 498 training loss: 0.11984238773584366\n",
      "epoch: 2 trial 499 training loss: 0.01991195185109973\n",
      "epoch: 2 trial 500 training loss: 0.03652101382613182\n",
      "epoch: 2 trial 501 training loss: 0.10937688685953617\n",
      "epoch: 2 trial 502 training loss: 0.08695471845567226\n",
      "epoch: 2 trial 503 training loss: 0.0420004902407527\n",
      "epoch: 2 trial 504 training loss: 0.027268579229712486\n",
      "epoch: 2 trial 505 training loss: 0.04369561094790697\n",
      "epoch: 2 trial 506 training loss: 0.04158230125904083\n",
      "epoch: 2 trial 507 training loss: 0.010490394313819706\n",
      "epoch: 2 trial 508 training loss: 0.002450508181937039\n",
      "epoch: 2 trial 509 training loss: 0.15920765697956085\n",
      "epoch: 2 trial 510 training loss: 0.02860463224351406\n",
      "epoch: 2 trial 511 training loss: 0.021820402238518\n",
      "epoch: 2 trial 512 training loss: 0.13286694139242172\n",
      "epoch: 2 trial 513 training loss: 0.008693258336279541\n",
      "epoch: 2 trial 514 training loss: 0.043955287896096706\n",
      "epoch: 2 trial 515 training loss: 0.018817166332155466\n",
      "epoch: 2 trial 516 training loss: 0.034383644349873066\n",
      "epoch: 2 trial 517 training loss: 0.0369468629360199\n",
      "epoch: 2 trial 518 training loss: 0.046882325783371925\n",
      "epoch: 2 trial 519 training loss: 0.018718269653618336\n",
      "epoch: 2 trial 520 training loss: 0.06034233048558235\n",
      "epoch: 2 trial 521 training loss: 0.16397669911384583\n",
      "epoch: 2 trial 522 training loss: 0.33123279362916946\n",
      "epoch: 2 trial 523 training loss: 0.020255293464288116\n",
      "epoch: 2 trial 524 training loss: 0.3517291843891144\n",
      "epoch: 2 trial 525 training loss: 0.18995307758450508\n",
      "epoch: 2 trial 526 training loss: 0.03203852288424969\n",
      "epoch: 2 trial 527 training loss: 0.34649737924337387\n",
      "epoch: 2 trial 528 training loss: 0.056397140957415104\n",
      "epoch: 2 trial 529 training loss: 0.10110917314887047\n",
      "epoch: 2 trial 530 training loss: 0.06990017183125019\n",
      "epoch: 2 trial 531 training loss: 0.03282753378152847\n",
      "epoch: 2 trial 532 training loss: 0.01688280887901783\n",
      "epoch: 2 trial 533 training loss: 0.03409813717007637\n",
      "epoch: 2 trial 534 training loss: 0.06867515854537487\n",
      "epoch: 2 trial 535 training loss: 0.04042379278689623\n",
      "epoch: 2 trial 536 training loss: 0.07951955124735832\n",
      "epoch: 2 trial 537 training loss: 0.16415272653102875\n",
      "epoch: 2 trial 538 training loss: 0.22442148998379707\n",
      "epoch: 2 trial 539 training loss: 0.06232075113803148\n",
      "epoch: 2 trial 540 training loss: 0.18330322578549385\n",
      "epoch: 2 trial 541 training loss: 0.009241843479685485\n",
      "epoch: 2 trial 542 training loss: 0.023949648719280958\n",
      "epoch: 2 trial 543 training loss: 0.08550082705914974\n",
      "epoch: 2 trial 544 training loss: 0.07536758109927177\n",
      "epoch: 2 trial 545 training loss: 0.04759881179779768\n",
      "epoch: 2 trial 546 training loss: 0.1536259651184082\n",
      "epoch: 2 trial 547 training loss: 0.0174043127335608\n",
      "epoch: 2 trial 548 training loss: 0.027561213821172714\n",
      "epoch: 2 trial 549 training loss: 0.09728065319359303\n",
      "epoch: 2 trial 550 training loss: 0.16230003908276558\n",
      "epoch: 2 trial 551 training loss: 0.04719631001353264\n",
      "epoch: 2 trial 552 training loss: 0.013780086999759078\n",
      "epoch: 2 trial 553 training loss: 0.10807848535478115\n",
      "epoch: 2 trial 554 training loss: 0.08581838384270668\n",
      "epoch: 2 trial 555 training loss: 0.044110940769314766\n",
      "epoch: 2 trial 556 training loss: 0.014370712218806148\n",
      "epoch: 2 trial 557 training loss: 0.09497562237083912\n",
      "epoch: 2 trial 558 training loss: 0.08325904607772827\n",
      "epoch: 2 trial 559 training loss: 0.027086672838777304\n",
      "epoch: 2 trial 560 training loss: 0.026585310697555542\n",
      "epoch: 2 trial 561 training loss: 0.04044359549880028\n",
      "epoch: 2 trial 562 training loss: 0.16415958851575851\n",
      "epoch: 2 trial 563 training loss: 0.10717121697962284\n",
      "epoch: 2 trial 564 training loss: 0.07991474866867065\n",
      "epoch: 2 trial 565 training loss: 0.03277622442692518\n",
      "epoch: 2 trial 566 training loss: 0.029921564739197493\n",
      "epoch: 2 trial 567 training loss: 0.07421720400452614\n",
      "epoch: 2 trial 568 training loss: 0.020287469029426575\n",
      "epoch: 2 trial 569 training loss: 0.059922090731561184\n",
      "epoch: 2 trial 570 training loss: 0.017577124759554863\n",
      "epoch: 2 trial 571 training loss: 0.049438477493822575\n",
      "epoch: 2 trial 572 training loss: 0.02659010700881481\n",
      "epoch: 2 trial 573 training loss: 0.0957526434212923\n",
      "epoch: 2 trial 574 training loss: 0.024758954998105764\n",
      "epoch: 2 trial 575 training loss: 0.030107676051557064\n",
      "epoch: 2 trial 576 training loss: 0.060642363503575325\n",
      "epoch: 2 trial 577 training loss: 0.01644975459203124\n",
      "epoch: 2 trial 578 training loss: 0.05640210583806038\n",
      "epoch: 2 trial 579 training loss: 0.045052241533994675\n",
      "epoch: 2 trial 580 training loss: 0.028786039911210537\n",
      "epoch: 2 trial 581 training loss: 0.017759154550731182\n",
      "epoch: 2 trial 582 training loss: 0.12138617038726807\n",
      "epoch: 2 trial 583 training loss: 0.04137361794710159\n",
      "epoch: 2 trial 584 training loss: 0.030144674703478813\n",
      "epoch: 2 trial 585 training loss: 0.07298231311142445\n",
      "epoch: 2 trial 586 training loss: 0.20390764251351357\n",
      "epoch: 2 trial 587 training loss: 0.046400400809943676\n",
      "epoch: 2 trial 588 training loss: 0.1033723633736372\n",
      "epoch: 2 trial 589 training loss: 0.33867400139570236\n",
      "epoch: 2 trial 590 training loss: 0.12108979932963848\n",
      "epoch: 2 trial 591 training loss: 0.107754435390234\n",
      "epoch: 2 trial 592 training loss: 0.10107845067977905\n",
      "epoch: 2 trial 593 training loss: 0.21653104573488235\n",
      "epoch: 2 trial 594 training loss: 0.027771828696131706\n",
      "epoch: 2 trial 595 training loss: 0.011058511678129435\n",
      "epoch: 2 trial 596 training loss: 0.12445100210607052\n",
      "epoch: 2 trial 597 training loss: 0.038083137944340706\n",
      "epoch: 2 trial 598 training loss: 0.04236331582069397\n",
      "epoch: 2 trial 599 training loss: 0.05591271258890629\n",
      "epoch: 2 trial 600 training loss: 0.013771753758192062\n",
      "epoch: 2 trial 601 training loss: 0.028468619100749493\n",
      "epoch: 2 trial 602 training loss: 0.04846580605953932\n",
      "epoch: 2 trial 603 training loss: 0.02340404177084565\n",
      "epoch: 2 trial 604 training loss: 0.018441086169332266\n",
      "epoch: 2 trial 605 training loss: 0.007525094086304307\n",
      "epoch: 2 trial 606 training loss: 0.11824100464582443\n",
      "epoch: 2 trial 607 training loss: 0.08985067717730999\n",
      "epoch: 2 trial 608 training loss: 0.02152406075038016\n",
      "epoch: 2 trial 609 training loss: 0.019434230867773294\n",
      "epoch: 2 trial 610 training loss: 0.06452154368162155\n",
      "epoch: 2 trial 611 training loss: 0.09266197122633457\n",
      "epoch: 2 trial 612 training loss: 0.03900632169097662\n",
      "epoch: 2 trial 613 training loss: 0.12039496935904026\n",
      "epoch: 2 trial 614 training loss: 0.07128899358212948\n",
      "epoch: 2 trial 615 training loss: 0.2953275218605995\n",
      "epoch: 2 trial 616 training loss: 0.05872285459190607\n",
      "epoch: 2 trial 617 training loss: 0.03397204354405403\n",
      "epoch: 2 trial 618 training loss: 0.03681075759232044\n",
      "epoch: 2 trial 619 training loss: 0.017147719860076904\n",
      "epoch: 2 trial 620 training loss: 0.01131473551504314\n",
      "epoch: 2 trial 621 training loss: 0.017641411162912846\n",
      "epoch: 2 trial 622 training loss: 0.20515082031488419\n",
      "epoch: 2 trial 623 training loss: 0.03723632171750069\n",
      "epoch: 2 trial 624 training loss: 0.1265963688492775\n",
      "epoch: 2 trial 625 training loss: 0.05965251289308071\n",
      "epoch: 2 trial 626 training loss: 0.03154637012630701\n",
      "epoch: 2 trial 627 training loss: 0.12925322726368904\n",
      "epoch: 2 trial 628 training loss: 0.04963787738233805\n",
      "epoch: 2 trial 629 training loss: 0.02973894402384758\n",
      "epoch: 2 trial 630 training loss: 0.04392482992261648\n",
      "epoch: 2 trial 631 training loss: 0.11782249622046947\n",
      "epoch: 2 trial 632 training loss: 0.046855905558913946\n",
      "epoch: 2 trial 633 training loss: 0.1371229961514473\n",
      "epoch: 2 trial 634 training loss: 0.13537509739398956\n",
      "epoch: 2 trial 635 training loss: 0.08887783624231815\n",
      "epoch: 2 trial 636 training loss: 0.06170268729329109\n",
      "epoch: 2 trial 637 training loss: 0.029871167615056038\n",
      "epoch: 2 trial 638 training loss: 0.10275612398982048\n",
      "epoch: 2 trial 639 training loss: 0.013350818771868944\n",
      "epoch: 2 trial 640 training loss: 0.007231819210574031\n",
      "epoch: 2 trial 641 training loss: 0.04074121732264757\n",
      "epoch: 2 trial 642 training loss: 0.022424854338169098\n",
      "epoch: 2 trial 643 training loss: 0.010652376571670175\n",
      "epoch: 2 trial 644 training loss: 0.030758682638406754\n",
      "epoch: 2 trial 645 training loss: 0.006770874373614788\n",
      "epoch: 2 trial 646 training loss: 0.06682423502206802\n",
      "epoch: 2 trial 647 training loss: 0.002832052647136152\n",
      "epoch: 2 trial 648 training loss: 0.052529752254486084\n",
      "epoch: 2 trial 649 training loss: 0.05477866157889366\n",
      "epoch: 2 trial 650 training loss: 0.020145385060459375\n",
      "epoch: 2 trial 651 training loss: 0.020402178168296814\n",
      "epoch: 2 trial 652 training loss: 0.012820110190659761\n",
      "epoch: 2 trial 653 training loss: 0.02626950293779373\n",
      "epoch: 2 trial 654 training loss: 0.1660093441605568\n",
      "epoch: 2 trial 655 training loss: 0.052279379684478045\n",
      "epoch: 2 trial 656 training loss: 0.014168939553201199\n",
      "epoch: 2 trial 657 training loss: 0.047612604685127735\n",
      "epoch: 2 trial 658 training loss: 0.07061941549181938\n",
      "epoch: 2 trial 659 training loss: 0.06850945390760899\n",
      "epoch: 2 trial 660 training loss: 0.03509214799851179\n",
      "epoch: 2 trial 661 training loss: 0.00969140324741602\n",
      "epoch: 2 trial 662 training loss: 0.06092696264386177\n",
      "epoch: 2 trial 663 training loss: 0.0472777308896184\n",
      "epoch: 2 trial 664 training loss: 0.036261982284486294\n",
      "epoch: 2 trial 665 training loss: 0.031571876257658005\n",
      "epoch: 2 trial 666 training loss: 0.02716958662495017\n",
      "epoch: 2 trial 667 training loss: 0.040828388184309006\n",
      "epoch: 2 trial 668 training loss: 0.01628119172528386\n",
      "epoch: 2 trial 669 training loss: 0.02817436493933201\n",
      "epoch: 2 trial 670 training loss: 0.060093361884355545\n",
      "epoch: 2 trial 671 training loss: 0.05771017074584961\n",
      "epoch: 2 trial 672 training loss: 0.03264332376420498\n",
      "epoch: 2 trial 673 training loss: 0.0594062265008688\n",
      "epoch: 2 trial 674 training loss: 0.043756766244769096\n",
      "epoch: 2 trial 675 training loss: 0.06685296632349491\n",
      "epoch: 2 trial 676 training loss: 0.06911261938512325\n",
      "epoch: 2 trial 677 training loss: 0.04932752810418606\n",
      "epoch: 2 trial 678 training loss: 0.019886610563844442\n",
      "epoch: 2 trial 679 training loss: 0.03950013034045696\n",
      "epoch: 2 trial 680 training loss: 0.03535330668091774\n",
      "epoch: 2 trial 681 training loss: 0.029955925419926643\n",
      "epoch: 2 trial 682 training loss: 0.06134488619863987\n",
      "epoch: 2 trial 683 training loss: 0.07944636978209019\n",
      "epoch: 2 trial 684 training loss: 0.06976468861103058\n",
      "epoch: 2 trial 685 training loss: 0.011284636217169464\n",
      "epoch: 2 trial 686 training loss: 0.021908664610236883\n",
      "epoch: 2 trial 687 training loss: 0.01728315046057105\n",
      "epoch: 2 trial 688 training loss: 0.0873270109295845\n",
      "epoch: 2 trial 689 training loss: 0.013020985759794712\n",
      "epoch: 2 trial 690 training loss: 0.032128490041941404\n",
      "epoch: 2 trial 691 training loss: 0.0377348680049181\n",
      "epoch: 2 trial 692 training loss: 0.012191552901640534\n",
      "epoch: 2 trial 693 training loss: 0.08689241856336594\n",
      "epoch: 2 trial 694 training loss: 0.09399637207388878\n",
      "epoch: 2 trial 695 training loss: 0.14016645029187202\n",
      "epoch: 2 trial 696 training loss: 0.0633663758635521\n",
      "epoch: 2 trial 697 training loss: 0.26840342581272125\n",
      "epoch: 2 trial 698 training loss: 0.011184414033778012\n",
      "epoch: 2 trial 699 training loss: 0.02067049127072096\n",
      "epoch: 2 trial 700 training loss: 0.01812413102015853\n",
      "epoch: 2 trial 701 training loss: 0.022399505600333214\n",
      "epoch: 2 trial 702 training loss: 0.01889726845547557\n",
      "epoch: 2 trial 703 training loss: 0.04862826969474554\n",
      "epoch: 2 trial 704 training loss: 0.047039344906806946\n",
      "epoch: 2 trial 705 training loss: 0.03778070863336325\n",
      "epoch: 2 trial 706 training loss: 0.09343940392136574\n",
      "epoch: 2 trial 707 training loss: 0.0774852056056261\n",
      "epoch: 2 trial 708 training loss: 0.062392085790634155\n",
      "epoch: 2 trial 709 training loss: 0.02913691569119692\n",
      "epoch: 2 trial 710 training loss: 0.03027183748781681\n",
      "epoch: 2 trial 711 training loss: 0.0286436821334064\n",
      "epoch: 2 trial 712 training loss: 0.009817944723181427\n",
      "epoch: 2 trial 713 training loss: 0.020221258513629436\n",
      "epoch: 2 trial 714 training loss: 0.021125690080225468\n",
      "epoch: 2 trial 715 training loss: 0.12441012263298035\n",
      "epoch: 2 trial 716 training loss: 0.060803456231951714\n",
      "epoch: 2 trial 717 training loss: 0.13688907399773598\n",
      "epoch: 2 trial 718 training loss: 0.01931880187476054\n",
      "epoch: 2 trial 719 training loss: 0.013906821608543396\n",
      "epoch: 2 trial 720 training loss: 0.033762832172214985\n",
      "epoch: 2 trial 721 training loss: 0.059250373393297195\n",
      "epoch: 2 trial 722 training loss: 0.04687212593853474\n",
      "epoch: 2 trial 723 training loss: 0.04063469264656305\n",
      "epoch: 2 trial 724 training loss: 0.011645240243524313\n",
      "epoch: 2 trial 725 training loss: 0.009010941488668323\n",
      "epoch: 2 trial 726 training loss: 0.03922049421817064\n",
      "epoch: 2 trial 727 training loss: 0.04486946761608124\n",
      "epoch: 2 trial 728 training loss: 0.0052315808134153485\n",
      "epoch: 2 trial 729 training loss: 0.08146611787378788\n",
      "epoch: 2 trial 730 training loss: 0.013831851538270712\n",
      "epoch: 2 trial 731 training loss: 0.02075955318287015\n",
      "epoch: 2 trial 732 training loss: 0.023044670931994915\n",
      "epoch: 2 trial 733 training loss: 0.034605312161147594\n",
      "epoch: 2 trial 734 training loss: 0.05374619923532009\n",
      "epoch: 2 trial 735 training loss: 0.01070006936788559\n",
      "epoch: 2 trial 736 training loss: 0.012361868284642696\n",
      "epoch: 2 trial 737 training loss: 0.026573233772069216\n",
      "epoch: 2 trial 738 training loss: 0.08963733911514282\n",
      "epoch: 2 trial 739 training loss: 0.022047499660402536\n",
      "epoch: 2 trial 740 training loss: 0.06509925983846188\n",
      "epoch: 2 trial 741 training loss: 0.02632814832031727\n",
      "epoch: 2 trial 742 training loss: 0.015863188076764345\n",
      "epoch: 2 trial 743 training loss: 0.25788913667201996\n",
      "epoch: 2 trial 744 training loss: 0.012970003997907043\n",
      "epoch: 2 trial 745 training loss: 0.019707716070115566\n",
      "epoch: 2 trial 746 training loss: 0.008003655821084976\n",
      "epoch: 2 trial 747 training loss: 0.005157373147085309\n",
      "epoch: 2 trial 748 training loss: 0.014761912170797586\n",
      "epoch: 2 trial 749 training loss: 0.03668359760195017\n",
      "epoch: 2 trial 750 training loss: 0.03717454895377159\n",
      "epoch: 2 trial 751 training loss: 0.04214775562286377\n",
      "epoch: 2 trial 752 training loss: 0.0413575591519475\n",
      "epoch: 2 trial 753 training loss: 0.04134782962501049\n",
      "epoch: 2 trial 754 training loss: 0.07815432362258434\n",
      "epoch: 2 trial 755 training loss: 0.016000244300812483\n",
      "epoch: 2 trial 756 training loss: 0.012457187753170729\n",
      "epoch: 2 trial 757 training loss: 0.022762822918593884\n",
      "epoch: 2 trial 758 training loss: 0.0049332373309880495\n",
      "epoch: 2 trial 759 training loss: 0.05062111280858517\n",
      "epoch: 2 trial 760 training loss: 0.004169584717601538\n",
      "epoch: 2 trial 761 training loss: 0.020182816311717033\n",
      "epoch: 2 trial 762 training loss: 0.015280259773135185\n",
      "epoch: 2 trial 763 training loss: 0.06790272146463394\n",
      "epoch: 2 trial 764 training loss: 0.022469970863312483\n",
      "epoch: 2 trial 765 training loss: 0.004822170769330114\n",
      "epoch: 2 trial 766 training loss: 0.10389428585767746\n",
      "epoch: 2 trial 767 training loss: 0.11679356172680855\n",
      "epoch: 2 trial 768 training loss: 0.020833192858844995\n",
      "epoch: 2 trial 769 training loss: 0.15683607384562492\n",
      "epoch: 2 trial 770 training loss: 0.02975285379216075\n",
      "epoch: 2 trial 771 training loss: 0.07999794743955135\n",
      "epoch: 2 trial 772 training loss: 0.04529592674225569\n",
      "epoch: 2 trial 773 training loss: 0.0247703418135643\n",
      "epoch: 2 trial 774 training loss: 0.12253639101982117\n",
      "epoch: 2 trial 775 training loss: 0.03255764581263065\n",
      "epoch: 2 trial 776 training loss: 0.015573360025882721\n",
      "epoch: 2 trial 777 training loss: 0.015463895630091429\n",
      "epoch: 2 trial 778 training loss: 0.0170154117513448\n",
      "epoch: 2 trial 779 training loss: 0.025460840202867985\n",
      "epoch: 2 trial 780 training loss: 0.025332733988761902\n",
      "epoch: 2 trial 781 training loss: 0.009373129578307271\n",
      "epoch: 2 trial 782 training loss: 0.010554203996434808\n",
      "epoch: 2 trial 783 training loss: 0.044437577947974205\n",
      "epoch: 2 trial 784 training loss: 0.028882430866360664\n",
      "epoch: 2 trial 785 training loss: 0.022629500832408667\n",
      "epoch: 2 trial 786 training loss: 0.0031336641404777765\n",
      "epoch: 2 trial 787 training loss: 0.013884213287383318\n",
      "epoch: 2 trial 788 training loss: 0.030679023824632168\n",
      "epoch: 2 trial 789 training loss: 0.052801220677793026\n",
      "epoch: 2 trial 790 training loss: 0.05576963908970356\n",
      "epoch: 2 trial 791 training loss: 0.003708216128870845\n",
      "epoch: 2 trial 792 training loss: 0.025896377861499786\n",
      "epoch: 2 trial 793 training loss: 0.04851544462144375\n",
      "epoch: 2 trial 794 training loss: 0.017802932299673557\n",
      "epoch: 2 trial 795 training loss: 0.009097824338823557\n",
      "epoch: 2 trial 796 training loss: 0.013067977502942085\n",
      "epoch: 2 trial 797 training loss: 0.044505772180855274\n",
      "epoch: 2 trial 798 training loss: 0.02827305532991886\n",
      "epoch: 2 trial 799 training loss: 0.02996760793030262\n",
      "epoch: 2 trial 800 training loss: 0.05218515545129776\n",
      "epoch: 2 trial 801 training loss: 0.014862676616758108\n",
      "epoch: 2 trial 802 training loss: 0.0493692122399807\n",
      "epoch: 2 trial 803 training loss: 0.07331929355859756\n",
      "epoch: 2 trial 804 training loss: 0.03339059092104435\n",
      "epoch: 2 trial 805 training loss: 0.09700018540024757\n",
      "epoch: 2 trial 806 training loss: 0.030590035021305084\n",
      "epoch: 2 trial 807 training loss: 0.039103640243411064\n",
      "epoch: 2 trial 808 training loss: 0.0371004668995738\n",
      "epoch: 2 trial 809 training loss: 0.02949783392250538\n",
      "epoch: 2 trial 810 training loss: 0.013725325698032975\n",
      "epoch: 2 trial 811 training loss: 0.08429984748363495\n",
      "epoch: 2 trial 812 training loss: 0.015637872274965048\n",
      "epoch: 2 trial 813 training loss: 0.12078619375824928\n",
      "epoch: 2 trial 814 training loss: 0.02889133431017399\n",
      "epoch: 2 trial 815 training loss: 0.015641762875020504\n",
      "epoch: 2 trial 816 training loss: 0.07696862705051899\n",
      "epoch: 2 trial 817 training loss: 0.05426104739308357\n",
      "epoch: 2 trial 818 training loss: 0.013273223768919706\n",
      "epoch: 2 trial 819 training loss: 0.03176444675773382\n",
      "epoch: 2 trial 820 training loss: 0.01387456408701837\n",
      "epoch: 2 trial 821 training loss: 0.038043065927922726\n",
      "epoch: 2 trial 822 training loss: 0.023144079837948084\n",
      "epoch: 2 trial 823 training loss: 0.02932447800412774\n",
      "epoch: 2 trial 824 training loss: 0.2779991403222084\n",
      "epoch: 2 trial 825 training loss: 0.0461317403241992\n",
      "epoch: 2 trial 826 training loss: 0.01420749444514513\n",
      "epoch: 2 trial 827 training loss: 0.020556935109198093\n",
      "epoch: 2 trial 828 training loss: 0.2116989754140377\n",
      "epoch: 2 trial 829 training loss: 0.05173508543521166\n",
      "epoch: 2 trial 830 training loss: 0.05143336113542318\n",
      "epoch: 2 trial 831 training loss: 0.012673472752794623\n",
      "epoch: 2 trial 832 training loss: 0.06361901946365833\n",
      "epoch: 2 trial 833 training loss: 0.025296723004430532\n",
      "epoch: 2 trial 834 training loss: 0.1113358587026596\n",
      "epoch: 2 trial 835 training loss: 0.017023475840687752\n",
      "epoch: 2 trial 836 training loss: 0.07072129659354687\n",
      "epoch: 2 trial 837 training loss: 0.0258007044903934\n",
      "epoch: 2 trial 838 training loss: 0.014988153241574764\n",
      "epoch: 2 trial 839 training loss: 0.046756829135119915\n",
      "epoch: 2 trial 840 training loss: 0.010397596983239055\n",
      "epoch: 2 trial 841 training loss: 0.0371537646278739\n",
      "epoch: 2 trial 842 training loss: 0.07189173065125942\n",
      "epoch: 2 trial 843 training loss: 0.046285444870591164\n",
      "epoch: 2 trial 844 training loss: 0.021307550836354494\n",
      "epoch: 2 trial 845 training loss: 0.10463015362620354\n",
      "epoch: 2 trial 846 training loss: 0.09626567550003529\n",
      "epoch: 2 trial 847 training loss: 0.07971753552556038\n",
      "epoch: 2 trial 848 training loss: 0.09411563351750374\n",
      "epoch: 2 trial 849 training loss: 0.08254618383944035\n",
      "epoch: 2 trial 850 training loss: 0.028091199696063995\n",
      "epoch: 2 trial 851 training loss: 0.010874479310587049\n",
      "epoch: 2 trial 852 training loss: 0.1957450546324253\n",
      "epoch: 2 trial 853 training loss: 0.07111437246203423\n",
      "epoch: 2 trial 854 training loss: 0.06127618066966534\n",
      "epoch: 2 trial 855 training loss: 0.05251547507941723\n",
      "epoch: 2 trial 856 training loss: 0.03217314463108778\n",
      "epoch: 2 trial 857 training loss: 0.021606952883303165\n",
      "epoch: 2 trial 858 training loss: 0.025567303877323866\n",
      "epoch: 2 trial 859 training loss: 0.03681276552379131\n",
      "epoch: 2 trial 860 training loss: 0.04615654982626438\n",
      "epoch: 2 trial 861 training loss: 0.046414878219366074\n",
      "epoch: 2 trial 862 training loss: 0.06775320135056973\n",
      "epoch: 2 trial 863 training loss: 0.07015850581228733\n",
      "epoch: 2 trial 864 training loss: 0.025483996607363224\n",
      "epoch: 2 trial 865 training loss: 0.029722481966018677\n",
      "epoch: 2 trial 866 training loss: 0.0061066600028425455\n",
      "epoch: 2 trial 867 training loss: 0.04590505547821522\n",
      "epoch: 2 trial 868 training loss: 0.03670420218259096\n",
      "epoch: 2 trial 869 training loss: 0.01116920099593699\n",
      "epoch: 2 trial 870 training loss: 0.002892034885007888\n",
      "epoch: 2 trial 871 training loss: 0.017928692046552896\n",
      "epoch: 2 trial 872 training loss: 0.015342013444751501\n",
      "epoch: 2 trial 873 training loss: 0.04171080980449915\n",
      "epoch: 2 trial 874 training loss: 0.00644015718717128\n",
      "epoch: 2 trial 875 training loss: 0.003434744547121227\n",
      "epoch: 2 trial 876 training loss: 0.004135844646953046\n",
      "epoch: 2 trial 877 training loss: 0.008310288190841675\n",
      "epoch: 2 trial 878 training loss: 0.013194646686315536\n",
      "epoch: 2 trial 879 training loss: 0.03798574302345514\n",
      "epoch: 2 trial 880 training loss: 0.11352835409343243\n",
      "epoch: 2 trial 881 training loss: 0.052054332569241524\n",
      "epoch: 2 trial 882 training loss: 0.02458740444853902\n",
      "epoch: 2 trial 883 training loss: 0.04113429319113493\n",
      "epoch: 2 trial 884 training loss: 0.02662591403350234\n",
      "epoch: 2 trial 885 training loss: 0.01939258212223649\n",
      "epoch: 2 trial 886 training loss: 0.03485012240707874\n",
      "epoch: 2 trial 887 training loss: 0.07973281852900982\n",
      "epoch: 2 trial 888 training loss: 0.1527783926576376\n",
      "epoch: 2 trial 889 training loss: 0.08559019397944212\n",
      "epoch: 2 trial 890 training loss: 0.02508603362366557\n",
      "epoch: 2 trial 891 training loss: 0.13655856624245644\n",
      "epoch: 2 trial 892 training loss: 0.038292763754725456\n",
      "epoch: 2 trial 893 training loss: 0.025948218069970608\n",
      "epoch: 2 trial 894 training loss: 0.08176787383854389\n",
      "epoch: 2 trial 895 training loss: 0.09849828109145164\n",
      "epoch: 2 trial 896 training loss: 0.07312872260808945\n",
      "epoch: 2 trial 897 training loss: 0.018458427861332893\n",
      "epoch: 2 trial 898 training loss: 0.06627750024199486\n",
      "epoch: 2 trial 899 training loss: 0.032177215442061424\n",
      "epoch: 2 trial 900 training loss: 0.12628063932061195\n",
      "epoch: 2 trial 901 training loss: 0.03024959284812212\n",
      "epoch: 2 trial 902 training loss: 0.05801622476428747\n",
      "epoch: 2 trial 903 training loss: 0.06558233872056007\n",
      "epoch: 2 trial 904 training loss: 0.004846353316679597\n",
      "epoch: 2 trial 905 training loss: 0.029250815510749817\n",
      "epoch: 2 trial 906 training loss: 0.037662323564291\n",
      "epoch: 2 trial 907 training loss: 0.04219590686261654\n",
      "epoch: 2 trial 908 training loss: 0.029949002899229527\n",
      "epoch: 2 trial 909 training loss: 0.010106656467542052\n",
      "epoch: 2 trial 910 training loss: 0.04730267729610205\n",
      "epoch: 2 trial 911 training loss: 0.03595595061779022\n",
      "epoch: 2 trial 912 training loss: 0.009728431003168225\n",
      "epoch: 2 trial 913 training loss: 0.02296618791297078\n",
      "epoch: 2 trial 914 training loss: 0.01062986720353365\n",
      "epoch: 2 trial 915 training loss: 0.016750832088291645\n",
      "epoch: 2 trial 916 training loss: 0.03860324248671532\n",
      "epoch: 2 trial 917 training loss: 0.03373218607157469\n",
      "epoch: 2 trial 918 training loss: 0.022599556483328342\n",
      "epoch: 2 trial 919 training loss: 0.018793604336678982\n",
      "epoch: 2 trial 920 training loss: 0.019431588239967823\n",
      "epoch: 2 trial 921 training loss: 0.00940203107893467\n",
      "epoch: 2 trial 922 training loss: 0.022471132688224316\n",
      "epoch: 2 trial 923 training loss: 0.02556443866342306\n",
      "epoch: 2 trial 924 training loss: 0.011614597169682384\n",
      "epoch: 2 trial 925 training loss: 0.012018508743494749\n",
      "epoch: 2 trial 926 training loss: 0.01665705069899559\n",
      "epoch: 2 trial 927 training loss: 0.09625965543091297\n",
      "epoch: 2 trial 928 training loss: 0.013653113273903728\n",
      "epoch: 2 trial 929 training loss: 0.048650605604052544\n",
      "epoch: 2 trial 930 training loss: 0.012847727630287409\n",
      "epoch: 2 trial 931 training loss: 0.04734484851360321\n",
      "epoch: 2 trial 932 training loss: 0.03962611872702837\n",
      "epoch: 2 trial 933 training loss: 0.044015586376190186\n",
      "epoch: 2 trial 934 training loss: 0.027381836902350187\n",
      "epoch: 2 trial 935 training loss: 0.017890612594783306\n",
      "epoch: 2 trial 936 training loss: 0.12211987748742104\n",
      "epoch: 2 trial 937 training loss: 0.00670237559825182\n",
      "epoch: 2 trial 938 training loss: 0.01898367004469037\n",
      "epoch: 2 trial 939 training loss: 0.14259076118469238\n",
      "epoch: 2 trial 940 training loss: 0.03147703316062689\n",
      "epoch: 2 trial 941 training loss: 0.02146746963262558\n",
      "epoch: 2 trial 942 training loss: 0.014017685316503048\n",
      "epoch: 2 trial 943 training loss: 0.017340472899377346\n",
      "epoch: 2 trial 944 training loss: 0.022596594877541065\n",
      "epoch: 2 trial 945 training loss: 0.017883362714201212\n",
      "epoch: 2 trial 946 training loss: 0.04715945292264223\n",
      "epoch: 2 trial 947 training loss: 0.1418173350393772\n",
      "epoch: 2 trial 948 training loss: 0.05893433652818203\n",
      "epoch: 2 trial 949 training loss: 0.021826835814863443\n",
      "epoch: 2 trial 950 training loss: 0.20216362178325653\n",
      "epoch: 2 trial 951 training loss: 0.13797695189714432\n",
      "epoch: 2 trial 952 training loss: 0.024354648310691118\n",
      "epoch: 2 trial 953 training loss: 0.06662309914827347\n",
      "epoch: 2 trial 954 training loss: 0.0050470412243157625\n",
      "epoch: 2 trial 955 training loss: 0.10096905194222927\n",
      "epoch: 2 trial 956 training loss: 0.01851597335189581\n",
      "epoch: 2 trial 957 training loss: 0.03266445454210043\n",
      "epoch: 2 trial 958 training loss: 0.1149904690682888\n",
      "epoch: 2 trial 959 training loss: 0.032135830260813236\n",
      "epoch: 2 trial 960 training loss: 0.009337535593658686\n",
      "epoch: 2 trial 961 training loss: 0.03771376609802246\n",
      "epoch: 2 trial 962 training loss: 0.09939439967274666\n",
      "epoch: 2 trial 963 training loss: 0.02637112094089389\n",
      "epoch: 2 trial 964 training loss: 0.021564431488513947\n",
      "epoch: 2 trial 965 training loss: 0.04321111738681793\n",
      "epoch: 2 trial 966 training loss: 0.04180042538791895\n",
      "epoch: 2 trial 967 training loss: 0.008076547179371119\n",
      "epoch: 2 trial 968 training loss: 0.03939718846231699\n",
      "epoch: 3 trial 969 training loss: 0.0274979998357594\n",
      "epoch: 3 trial 970 training loss: 0.014903847593814135\n",
      "epoch: 3 trial 971 training loss: 0.020854538306593895\n",
      "epoch: 3 trial 972 training loss: 0.06629488803446293\n",
      "epoch: 3 trial 973 training loss: 0.007388586993329227\n",
      "epoch: 3 trial 974 training loss: 0.006494799861684442\n",
      "epoch: 3 trial 975 training loss: 0.0312933474779129\n",
      "epoch: 3 trial 976 training loss: 0.01726421480998397\n",
      "epoch: 3 trial 977 training loss: 0.004638181766495109\n",
      "epoch: 3 trial 978 training loss: 0.019053840544074774\n",
      "epoch: 3 trial 979 training loss: 0.042490712366998196\n",
      "epoch: 3 trial 980 training loss: 0.02778968447819352\n",
      "epoch: 3 trial 981 training loss: 0.013745603151619434\n",
      "epoch: 3 trial 982 training loss: 0.08341696485877037\n",
      "epoch: 3 trial 983 training loss: 0.018502204213291407\n",
      "epoch: 3 trial 984 training loss: 0.029482845216989517\n",
      "epoch: 3 trial 985 training loss: 0.13643180206418037\n",
      "epoch: 3 trial 986 training loss: 0.0648602582514286\n",
      "epoch: 3 trial 987 training loss: 0.017549138516187668\n",
      "epoch: 3 trial 988 training loss: 0.018638450652360916\n",
      "epoch: 3 trial 989 training loss: 0.01723766466602683\n",
      "epoch: 3 trial 990 training loss: 0.07227997668087482\n",
      "epoch: 3 trial 991 training loss: 0.01670300867408514\n",
      "epoch: 3 trial 992 training loss: 0.003320766205433756\n",
      "epoch: 3 trial 993 training loss: 0.15190866589546204\n",
      "epoch: 3 trial 994 training loss: 0.02335473382845521\n",
      "epoch: 3 trial 995 training loss: 0.013410306302830577\n",
      "epoch: 3 trial 996 training loss: 0.1302035003900528\n",
      "epoch: 3 trial 997 training loss: 0.01521494856569916\n",
      "epoch: 3 trial 998 training loss: 0.033219301141798496\n",
      "epoch: 3 trial 999 training loss: 0.038911434821784496\n",
      "epoch: 3 trial 1000 training loss: 0.013756679370999336\n",
      "epoch: 3 trial 1001 training loss: 0.03469380084425211\n",
      "epoch: 3 trial 1002 training loss: 0.03798171039670706\n",
      "epoch: 3 trial 1003 training loss: 0.04220606479793787\n",
      "epoch: 3 trial 1004 training loss: 0.06541580893099308\n",
      "epoch: 3 trial 1005 training loss: 0.1490996703505516\n",
      "epoch: 3 trial 1006 training loss: 0.3752755746245384\n",
      "epoch: 3 trial 1007 training loss: 0.04190404247492552\n",
      "epoch: 3 trial 1008 training loss: 0.17808225750923157\n",
      "epoch: 3 trial 1009 training loss: 0.14187862165272236\n",
      "epoch: 3 trial 1010 training loss: 0.01336772320792079\n",
      "epoch: 3 trial 1011 training loss: 0.16338887810707092\n",
      "epoch: 3 trial 1012 training loss: 0.04587171413004398\n",
      "epoch: 3 trial 1013 training loss: 0.03493989445269108\n",
      "epoch: 3 trial 1014 training loss: 0.05026550963521004\n",
      "epoch: 3 trial 1015 training loss: 0.02608673507347703\n",
      "epoch: 3 trial 1016 training loss: 0.0221291477791965\n",
      "epoch: 3 trial 1017 training loss: 0.02262768754735589\n",
      "epoch: 3 trial 1018 training loss: 0.07842874526977539\n",
      "epoch: 3 trial 1019 training loss: 0.05873364955186844\n",
      "epoch: 3 trial 1020 training loss: 0.07643117103725672\n",
      "epoch: 3 trial 1021 training loss: 0.06956608034670353\n",
      "epoch: 3 trial 1022 training loss: 0.08473632484674454\n",
      "epoch: 3 trial 1023 training loss: 0.03268231265246868\n",
      "epoch: 3 trial 1024 training loss: 0.09874380379915237\n",
      "epoch: 3 trial 1025 training loss: 0.019222037866711617\n",
      "epoch: 3 trial 1026 training loss: 0.015878377482295036\n",
      "epoch: 3 trial 1027 training loss: 0.030144909396767616\n",
      "epoch: 3 trial 1028 training loss: 0.08129512891173363\n",
      "epoch: 3 trial 1029 training loss: 0.019421268720179796\n",
      "epoch: 3 trial 1030 training loss: 0.13025888800621033\n",
      "epoch: 3 trial 1031 training loss: 0.01720912614837289\n",
      "epoch: 3 trial 1032 training loss: 0.012555520981550217\n",
      "epoch: 3 trial 1033 training loss: 0.04297616705298424\n",
      "epoch: 3 trial 1034 training loss: 0.06135858781635761\n",
      "epoch: 3 trial 1035 training loss: 0.02301348652690649\n",
      "epoch: 3 trial 1036 training loss: 0.0197375426068902\n",
      "epoch: 3 trial 1037 training loss: 0.05952450633049011\n",
      "epoch: 3 trial 1038 training loss: 0.03037723246961832\n",
      "epoch: 3 trial 1039 training loss: 0.042336082085967064\n",
      "epoch: 3 trial 1040 training loss: 0.02273818850517273\n",
      "epoch: 3 trial 1041 training loss: 0.016974205151200294\n",
      "epoch: 3 trial 1042 training loss: 0.07034661993384361\n",
      "epoch: 3 trial 1043 training loss: 0.018772853072732687\n",
      "epoch: 3 trial 1044 training loss: 0.02384346816688776\n",
      "epoch: 3 trial 1045 training loss: 0.025702405720949173\n",
      "epoch: 3 trial 1046 training loss: 0.13284486532211304\n",
      "epoch: 3 trial 1047 training loss: 0.03060162626206875\n",
      "epoch: 3 trial 1048 training loss: 0.06515627726912498\n",
      "epoch: 3 trial 1049 training loss: 0.036056469194591045\n",
      "epoch: 3 trial 1050 training loss: 0.03828221745789051\n",
      "epoch: 3 trial 1051 training loss: 0.07452917844057083\n",
      "epoch: 3 trial 1052 training loss: 0.019795064348727465\n",
      "epoch: 3 trial 1053 training loss: 0.06822388619184494\n",
      "epoch: 3 trial 1054 training loss: 0.012478993739932775\n",
      "epoch: 3 trial 1055 training loss: 0.04074608255177736\n",
      "epoch: 3 trial 1056 training loss: 0.0819987878203392\n",
      "epoch: 3 trial 1057 training loss: 0.052933333441615105\n",
      "epoch: 3 trial 1058 training loss: 0.010547386948019266\n",
      "epoch: 3 trial 1059 training loss: 0.036745629739016294\n",
      "epoch: 3 trial 1060 training loss: 0.024534044321626425\n",
      "epoch: 3 trial 1061 training loss: 0.018379625864326954\n",
      "epoch: 3 trial 1062 training loss: 0.08352624252438545\n",
      "epoch: 3 trial 1063 training loss: 0.04927756357938051\n",
      "epoch: 3 trial 1064 training loss: 0.05137172807008028\n",
      "epoch: 3 trial 1065 training loss: 0.017928113229572773\n",
      "epoch: 3 trial 1066 training loss: 0.12063499167561531\n",
      "epoch: 3 trial 1067 training loss: 0.05576187185943127\n",
      "epoch: 3 trial 1068 training loss: 0.03638170287013054\n",
      "epoch: 3 trial 1069 training loss: 0.06469873152673244\n",
      "epoch: 3 trial 1070 training loss: 0.12726371735334396\n",
      "epoch: 3 trial 1071 training loss: 0.039783989544957876\n",
      "epoch: 3 trial 1072 training loss: 0.05490310210734606\n",
      "epoch: 3 trial 1073 training loss: 0.30982276797294617\n",
      "epoch: 3 trial 1074 training loss: 0.08404252305626869\n",
      "epoch: 3 trial 1075 training loss: 0.04916237387806177\n",
      "epoch: 3 trial 1076 training loss: 0.12628359906375408\n",
      "epoch: 3 trial 1077 training loss: 0.1465293914079666\n",
      "epoch: 3 trial 1078 training loss: 0.021875116508454084\n",
      "epoch: 3 trial 1079 training loss: 0.012119305785745382\n",
      "epoch: 3 trial 1080 training loss: 0.04411658737808466\n",
      "epoch: 3 trial 1081 training loss: 0.014882860239595175\n",
      "epoch: 3 trial 1082 training loss: 0.016430148389190435\n",
      "epoch: 3 trial 1083 training loss: 0.046707646921277046\n",
      "epoch: 3 trial 1084 training loss: 0.023258385248482227\n",
      "epoch: 3 trial 1085 training loss: 0.030989159364253283\n",
      "epoch: 3 trial 1086 training loss: 0.0349109242670238\n",
      "epoch: 3 trial 1087 training loss: 0.02795118559151888\n",
      "epoch: 3 trial 1088 training loss: 0.018464649096131325\n",
      "epoch: 3 trial 1089 training loss: 0.008190791122615337\n",
      "epoch: 3 trial 1090 training loss: 0.07000783830881119\n",
      "epoch: 3 trial 1091 training loss: 0.08371767029166222\n",
      "epoch: 3 trial 1092 training loss: 0.026945135556161404\n",
      "epoch: 3 trial 1093 training loss: 0.01004258543252945\n",
      "epoch: 3 trial 1094 training loss: 0.06281832046806812\n",
      "epoch: 3 trial 1095 training loss: 0.043897734954953194\n",
      "epoch: 3 trial 1096 training loss: 0.030178116634488106\n",
      "epoch: 3 trial 1097 training loss: 0.13699262589216232\n",
      "epoch: 3 trial 1098 training loss: 0.03494633175432682\n",
      "epoch: 3 trial 1099 training loss: 0.17434853315353394\n",
      "epoch: 3 trial 1100 training loss: 0.05517539847642183\n",
      "epoch: 3 trial 1101 training loss: 0.03630806319415569\n",
      "epoch: 3 trial 1102 training loss: 0.04600740969181061\n",
      "epoch: 3 trial 1103 training loss: 0.014815316535532475\n",
      "epoch: 3 trial 1104 training loss: 0.03375187981873751\n",
      "epoch: 3 trial 1105 training loss: 0.015373863512650132\n",
      "epoch: 3 trial 1106 training loss: 0.14852715283632278\n",
      "epoch: 3 trial 1107 training loss: 0.02255478221923113\n",
      "epoch: 3 trial 1108 training loss: 0.07615363784134388\n",
      "epoch: 3 trial 1109 training loss: 0.05543427728116512\n",
      "epoch: 3 trial 1110 training loss: 0.029688429087400436\n",
      "epoch: 3 trial 1111 training loss: 0.06422919407486916\n",
      "epoch: 3 trial 1112 training loss: 0.054827877320349216\n",
      "epoch: 3 trial 1113 training loss: 0.04399346932768822\n",
      "epoch: 3 trial 1114 training loss: 0.02211152808740735\n",
      "epoch: 3 trial 1115 training loss: 0.10756216570734978\n",
      "epoch: 3 trial 1116 training loss: 0.026562687009572983\n",
      "epoch: 3 trial 1117 training loss: 0.04227818176150322\n",
      "epoch: 3 trial 1118 training loss: 0.07846897654235363\n",
      "epoch: 3 trial 1119 training loss: 0.0322636216878891\n",
      "epoch: 3 trial 1120 training loss: 0.05975334346294403\n",
      "epoch: 3 trial 1121 training loss: 0.024174649734050035\n",
      "epoch: 3 trial 1122 training loss: 0.05001160968095064\n",
      "epoch: 3 trial 1123 training loss: 0.01031950255855918\n",
      "epoch: 3 trial 1124 training loss: 0.008687202353030443\n",
      "epoch: 3 trial 1125 training loss: 0.023798957467079163\n",
      "epoch: 3 trial 1126 training loss: 0.007074344786815345\n",
      "epoch: 3 trial 1127 training loss: 0.014726311899721622\n",
      "epoch: 3 trial 1128 training loss: 0.013787074014544487\n",
      "epoch: 3 trial 1129 training loss: 0.007307664258405566\n",
      "epoch: 3 trial 1130 training loss: 0.06633046828210354\n",
      "epoch: 3 trial 1131 training loss: 0.002815469109918922\n",
      "epoch: 3 trial 1132 training loss: 0.05510713905096054\n",
      "epoch: 3 trial 1133 training loss: 0.01975808572024107\n",
      "epoch: 3 trial 1134 training loss: 0.02413722639903426\n",
      "epoch: 3 trial 1135 training loss: 0.029808630235493183\n",
      "epoch: 3 trial 1136 training loss: 0.012542378157377243\n",
      "epoch: 3 trial 1137 training loss: 0.028823193162679672\n",
      "epoch: 3 trial 1138 training loss: 0.1572054699063301\n",
      "epoch: 3 trial 1139 training loss: 0.05228628870099783\n",
      "epoch: 3 trial 1140 training loss: 0.007927087601274252\n",
      "epoch: 3 trial 1141 training loss: 0.05290541239082813\n",
      "epoch: 3 trial 1142 training loss: 0.042700414545834064\n",
      "epoch: 3 trial 1143 training loss: 0.05184741970151663\n",
      "epoch: 3 trial 1144 training loss: 0.02898268122226\n",
      "epoch: 3 trial 1145 training loss: 0.0049984188517555594\n",
      "epoch: 3 trial 1146 training loss: 0.04982451815158129\n",
      "epoch: 3 trial 1147 training loss: 0.05545693077147007\n",
      "epoch: 3 trial 1148 training loss: 0.03229228314012289\n",
      "epoch: 3 trial 1149 training loss: 0.024879484437406063\n",
      "epoch: 3 trial 1150 training loss: 0.025899038184434175\n",
      "epoch: 3 trial 1151 training loss: 0.0358258793130517\n",
      "epoch: 3 trial 1152 training loss: 0.02299915486946702\n",
      "epoch: 3 trial 1153 training loss: 0.023224296048283577\n",
      "epoch: 3 trial 1154 training loss: 0.08899787440896034\n",
      "epoch: 3 trial 1155 training loss: 0.06831329502165318\n",
      "epoch: 3 trial 1156 training loss: 0.033383955247700214\n",
      "epoch: 3 trial 1157 training loss: 0.0620062742382288\n",
      "epoch: 3 trial 1158 training loss: 0.050197308883070946\n",
      "epoch: 3 trial 1159 training loss: 0.07635034620761871\n",
      "epoch: 3 trial 1160 training loss: 0.052290789783000946\n",
      "epoch: 3 trial 1161 training loss: 0.046390178613364697\n",
      "epoch: 3 trial 1162 training loss: 0.019223715644329786\n",
      "epoch: 3 trial 1163 training loss: 0.03580597881227732\n",
      "epoch: 3 trial 1164 training loss: 0.02479926124215126\n",
      "epoch: 3 trial 1165 training loss: 0.020648678299039602\n",
      "epoch: 3 trial 1166 training loss: 0.05444898456335068\n",
      "epoch: 3 trial 1167 training loss: 0.09274173155426979\n",
      "epoch: 3 trial 1168 training loss: 0.031277009285986423\n",
      "epoch: 3 trial 1169 training loss: 0.0062996650813147426\n",
      "epoch: 3 trial 1170 training loss: 0.01255753543227911\n",
      "epoch: 3 trial 1171 training loss: 0.00833889958448708\n",
      "epoch: 3 trial 1172 training loss: 0.050648135133087635\n",
      "epoch: 3 trial 1173 training loss: 0.01179120084270835\n",
      "epoch: 3 trial 1174 training loss: 0.04033063352108002\n",
      "epoch: 3 trial 1175 training loss: 0.039928536862134933\n",
      "epoch: 3 trial 1176 training loss: 0.015086147468537092\n",
      "epoch: 3 trial 1177 training loss: 0.08711953461170197\n",
      "epoch: 3 trial 1178 training loss: 0.060049342922866344\n",
      "epoch: 3 trial 1179 training loss: 0.10000253282487392\n",
      "epoch: 3 trial 1180 training loss: 0.062485575675964355\n",
      "epoch: 3 trial 1181 training loss: 0.17937033623456955\n",
      "epoch: 3 trial 1182 training loss: 0.008529922924935818\n",
      "epoch: 3 trial 1183 training loss: 0.022263746708631516\n",
      "epoch: 3 trial 1184 training loss: 0.021367254201322794\n",
      "epoch: 3 trial 1185 training loss: 0.023670041002333164\n",
      "epoch: 3 trial 1186 training loss: 0.015914469491690397\n",
      "epoch: 3 trial 1187 training loss: 0.05452519655227661\n",
      "epoch: 3 trial 1188 training loss: 0.054139045998454094\n",
      "epoch: 3 trial 1189 training loss: 0.03384228143841028\n",
      "epoch: 3 trial 1190 training loss: 0.07328904792666435\n",
      "epoch: 3 trial 1191 training loss: 0.04949203226715326\n",
      "epoch: 3 trial 1192 training loss: 0.06533081363886595\n",
      "epoch: 3 trial 1193 training loss: 0.011901800520718098\n",
      "epoch: 3 trial 1194 training loss: 0.015453446190804243\n",
      "epoch: 3 trial 1195 training loss: 0.03199868183583021\n",
      "epoch: 3 trial 1196 training loss: 0.014312009792774916\n",
      "epoch: 3 trial 1197 training loss: 0.03993117995560169\n",
      "epoch: 3 trial 1198 training loss: 0.011571772396564484\n",
      "epoch: 3 trial 1199 training loss: 0.13329970836639404\n",
      "epoch: 3 trial 1200 training loss: 0.05905054323375225\n",
      "epoch: 3 trial 1201 training loss: 0.15843859314918518\n",
      "epoch: 3 trial 1202 training loss: 0.08372214250266552\n",
      "epoch: 3 trial 1203 training loss: 0.013308111112564802\n",
      "epoch: 3 trial 1204 training loss: 0.034097377210855484\n",
      "epoch: 3 trial 1205 training loss: 0.05311035644263029\n",
      "epoch: 3 trial 1206 training loss: 0.041388905607163906\n",
      "epoch: 3 trial 1207 training loss: 0.04417125042527914\n",
      "epoch: 3 trial 1208 training loss: 0.06717472337186337\n",
      "epoch: 3 trial 1209 training loss: 0.030715056229382753\n",
      "epoch: 3 trial 1210 training loss: 0.032781919464468956\n",
      "epoch: 3 trial 1211 training loss: 0.0565970279276371\n",
      "epoch: 3 trial 1212 training loss: 0.0030072148656472564\n",
      "epoch: 3 trial 1213 training loss: 0.0534933190792799\n",
      "epoch: 3 trial 1214 training loss: 0.01478788093663752\n",
      "epoch: 3 trial 1215 training loss: 0.04798474442213774\n",
      "epoch: 3 trial 1216 training loss: 0.02244612295180559\n",
      "epoch: 3 trial 1217 training loss: 0.021826021373271942\n",
      "epoch: 3 trial 1218 training loss: 0.06697898171842098\n",
      "epoch: 3 trial 1219 training loss: 0.01414717617444694\n",
      "epoch: 3 trial 1220 training loss: 0.006043607369065285\n",
      "epoch: 3 trial 1221 training loss: 0.028210137970745564\n",
      "epoch: 3 trial 1222 training loss: 0.07957188971340656\n",
      "epoch: 3 trial 1223 training loss: 0.01851129438728094\n",
      "epoch: 3 trial 1224 training loss: 0.07243502512574196\n",
      "epoch: 3 trial 1225 training loss: 0.03191183228045702\n",
      "epoch: 3 trial 1226 training loss: 0.019540459848940372\n",
      "epoch: 3 trial 1227 training loss: 0.1947103887796402\n",
      "epoch: 3 trial 1228 training loss: 0.013940214645117521\n",
      "epoch: 3 trial 1229 training loss: 0.01762638706713915\n",
      "epoch: 3 trial 1230 training loss: 0.006751741631887853\n",
      "epoch: 3 trial 1231 training loss: 0.003494292264804244\n",
      "epoch: 3 trial 1232 training loss: 0.01671525789424777\n",
      "epoch: 3 trial 1233 training loss: 0.03334465064108372\n",
      "epoch: 3 trial 1234 training loss: 0.03365506790578365\n",
      "epoch: 3 trial 1235 training loss: 0.04035289213061333\n",
      "epoch: 3 trial 1236 training loss: 0.03019066248089075\n",
      "epoch: 3 trial 1237 training loss: 0.02866772748529911\n",
      "epoch: 3 trial 1238 training loss: 0.07389898598194122\n",
      "epoch: 3 trial 1239 training loss: 0.01537712849676609\n",
      "epoch: 3 trial 1240 training loss: 0.009943876415491104\n",
      "epoch: 3 trial 1241 training loss: 0.020916494075208902\n",
      "epoch: 3 trial 1242 training loss: 0.0066304843639954925\n",
      "epoch: 3 trial 1243 training loss: 0.05747573636472225\n",
      "epoch: 3 trial 1244 training loss: 0.00426301802508533\n",
      "epoch: 3 trial 1245 training loss: 0.021466138307005167\n",
      "epoch: 3 trial 1246 training loss: 0.013032345799729228\n",
      "epoch: 3 trial 1247 training loss: 0.06675951182842255\n",
      "epoch: 3 trial 1248 training loss: 0.019396592862904072\n",
      "epoch: 3 trial 1249 training loss: 0.012096688151359558\n",
      "epoch: 3 trial 1250 training loss: 0.08089893311262131\n",
      "epoch: 3 trial 1251 training loss: 0.11059644818305969\n",
      "epoch: 3 trial 1252 training loss: 0.028101333416998386\n",
      "epoch: 3 trial 1253 training loss: 0.1028369888663292\n",
      "epoch: 3 trial 1254 training loss: 0.02771272137761116\n",
      "epoch: 3 trial 1255 training loss: 0.09583581611514091\n",
      "epoch: 3 trial 1256 training loss: 0.01847419934347272\n",
      "epoch: 3 trial 1257 training loss: 0.019142238423228264\n",
      "epoch: 3 trial 1258 training loss: 0.08869069442152977\n",
      "epoch: 3 trial 1259 training loss: 0.03356238920241594\n",
      "epoch: 3 trial 1260 training loss: 0.01583766844123602\n",
      "epoch: 3 trial 1261 training loss: 0.012269201688468456\n",
      "epoch: 3 trial 1262 training loss: 0.011145877419039607\n",
      "epoch: 3 trial 1263 training loss: 0.02319119917228818\n",
      "epoch: 3 trial 1264 training loss: 0.02831236831843853\n",
      "epoch: 3 trial 1265 training loss: 0.018220945727080107\n",
      "epoch: 3 trial 1266 training loss: 0.030815962236374617\n",
      "epoch: 3 trial 1267 training loss: 0.05037176422774792\n",
      "epoch: 3 trial 1268 training loss: 0.02786765806376934\n",
      "epoch: 3 trial 1269 training loss: 0.03601651266217232\n",
      "epoch: 3 trial 1270 training loss: 0.009091481333598495\n",
      "epoch: 3 trial 1271 training loss: 0.010703671257942915\n",
      "epoch: 3 trial 1272 training loss: 0.02962552011013031\n",
      "epoch: 3 trial 1273 training loss: 0.04561044182628393\n",
      "epoch: 3 trial 1274 training loss: 0.056559596210718155\n",
      "epoch: 3 trial 1275 training loss: 0.002847350435331464\n",
      "epoch: 3 trial 1276 training loss: 0.02933323383331299\n",
      "epoch: 3 trial 1277 training loss: 0.046286177821457386\n",
      "epoch: 3 trial 1278 training loss: 0.016929448582232\n",
      "epoch: 3 trial 1279 training loss: 0.009324808022938669\n",
      "epoch: 3 trial 1280 training loss: 0.015513378195464611\n",
      "epoch: 3 trial 1281 training loss: 0.0422759847715497\n",
      "epoch: 3 trial 1282 training loss: 0.025104171596467495\n",
      "epoch: 3 trial 1283 training loss: 0.03408274333924055\n",
      "epoch: 3 trial 1284 training loss: 0.05648319609463215\n",
      "epoch: 3 trial 1285 training loss: 0.021246192511171103\n",
      "epoch: 3 trial 1286 training loss: 0.035709881223738194\n",
      "epoch: 3 trial 1287 training loss: 0.07401602156460285\n",
      "epoch: 3 trial 1288 training loss: 0.03258403856307268\n",
      "epoch: 3 trial 1289 training loss: 0.07162167318165302\n",
      "epoch: 3 trial 1290 training loss: 0.020057605113834143\n",
      "epoch: 3 trial 1291 training loss: 0.03104013204574585\n",
      "epoch: 3 trial 1292 training loss: 0.026173151098191738\n",
      "epoch: 3 trial 1293 training loss: 0.023082968778908253\n",
      "epoch: 3 trial 1294 training loss: 0.0168978963047266\n",
      "epoch: 3 trial 1295 training loss: 0.07811793126165867\n",
      "epoch: 3 trial 1296 training loss: 0.013969796244055033\n",
      "epoch: 3 trial 1297 training loss: 0.09645679593086243\n",
      "epoch: 3 trial 1298 training loss: 0.029840877279639244\n",
      "epoch: 3 trial 1299 training loss: 0.013036783318966627\n",
      "epoch: 3 trial 1300 training loss: 0.060405055060982704\n",
      "epoch: 3 trial 1301 training loss: 0.05254820827394724\n",
      "epoch: 3 trial 1302 training loss: 0.010528599843382835\n",
      "epoch: 3 trial 1303 training loss: 0.029522711411118507\n",
      "epoch: 3 trial 1304 training loss: 0.01041642832569778\n",
      "epoch: 3 trial 1305 training loss: 0.035690655931830406\n",
      "epoch: 3 trial 1306 training loss: 0.020374844782054424\n",
      "epoch: 3 trial 1307 training loss: 0.029131332878023386\n",
      "epoch: 3 trial 1308 training loss: 0.2632751017808914\n",
      "epoch: 3 trial 1309 training loss: 0.04497843701392412\n",
      "epoch: 3 trial 1310 training loss: 0.010639271000400186\n",
      "epoch: 3 trial 1311 training loss: 0.019201489631086588\n",
      "epoch: 3 trial 1312 training loss: 0.15935951843857765\n",
      "epoch: 3 trial 1313 training loss: 0.04802311584353447\n",
      "epoch: 3 trial 1314 training loss: 0.06003067269921303\n",
      "epoch: 3 trial 1315 training loss: 0.01951284846290946\n",
      "epoch: 3 trial 1316 training loss: 0.049895997159183025\n",
      "epoch: 3 trial 1317 training loss: 0.022906226105988026\n",
      "epoch: 3 trial 1318 training loss: 0.0974038764834404\n",
      "epoch: 3 trial 1319 training loss: 0.010640634456649423\n",
      "epoch: 3 trial 1320 training loss: 0.0690981037914753\n",
      "epoch: 3 trial 1321 training loss: 0.026043968740850687\n",
      "epoch: 3 trial 1322 training loss: 0.015843677800148726\n",
      "epoch: 3 trial 1323 training loss: 0.04983432590961456\n",
      "epoch: 3 trial 1324 training loss: 0.012933720368891954\n",
      "epoch: 3 trial 1325 training loss: 0.03378668427467346\n",
      "epoch: 3 trial 1326 training loss: 0.08623842895030975\n",
      "epoch: 3 trial 1327 training loss: 0.04428204707801342\n",
      "epoch: 3 trial 1328 training loss: 0.025207712315022945\n",
      "epoch: 3 trial 1329 training loss: 0.10941658541560173\n",
      "epoch: 3 trial 1330 training loss: 0.09103627502918243\n",
      "epoch: 3 trial 1331 training loss: 0.04796630144119263\n",
      "epoch: 3 trial 1332 training loss: 0.08218631520867348\n",
      "epoch: 3 trial 1333 training loss: 0.06913352385163307\n",
      "epoch: 3 trial 1334 training loss: 0.020674899220466614\n",
      "epoch: 3 trial 1335 training loss: 0.010005827527493238\n",
      "epoch: 3 trial 1336 training loss: 0.18249720335006714\n",
      "epoch: 3 trial 1337 training loss: 0.07334276288747787\n",
      "epoch: 3 trial 1338 training loss: 0.046528964303433895\n",
      "epoch: 3 trial 1339 training loss: 0.05114180035889149\n",
      "epoch: 3 trial 1340 training loss: 0.026811215095221996\n",
      "epoch: 3 trial 1341 training loss: 0.027084709145128727\n",
      "epoch: 3 trial 1342 training loss: 0.02267519710585475\n",
      "epoch: 3 trial 1343 training loss: 0.041855912655591965\n",
      "epoch: 3 trial 1344 training loss: 0.038479968905448914\n",
      "epoch: 3 trial 1345 training loss: 0.04804863780736923\n",
      "epoch: 3 trial 1346 training loss: 0.07374873384833336\n",
      "epoch: 3 trial 1347 training loss: 0.047648295760154724\n",
      "epoch: 3 trial 1348 training loss: 0.01740477792918682\n",
      "epoch: 3 trial 1349 training loss: 0.028942754492163658\n",
      "epoch: 3 trial 1350 training loss: 0.007952419109642506\n",
      "epoch: 3 trial 1351 training loss: 0.05234663002192974\n",
      "epoch: 3 trial 1352 training loss: 0.03628367371857166\n",
      "epoch: 3 trial 1353 training loss: 0.009875544346868992\n",
      "epoch: 3 trial 1354 training loss: 0.0027907490730285645\n",
      "epoch: 3 trial 1355 training loss: 0.014808535110205412\n",
      "epoch: 3 trial 1356 training loss: 0.015239713713526726\n",
      "epoch: 3 trial 1357 training loss: 0.02961895242333412\n",
      "epoch: 3 trial 1358 training loss: 0.006901947315782309\n",
      "epoch: 3 trial 1359 training loss: 0.003333244880195707\n",
      "epoch: 3 trial 1360 training loss: 0.004642028361558914\n",
      "epoch: 3 trial 1361 training loss: 0.009399543981999159\n",
      "epoch: 3 trial 1362 training loss: 0.014106570277363062\n",
      "epoch: 3 trial 1363 training loss: 0.03241283632814884\n",
      "epoch: 3 trial 1364 training loss: 0.11435344070196152\n",
      "epoch: 3 trial 1365 training loss: 0.034486654214560986\n",
      "epoch: 3 trial 1366 training loss: 0.023133731447160244\n",
      "epoch: 3 trial 1367 training loss: 0.03988224547356367\n",
      "epoch: 3 trial 1368 training loss: 0.03879705164581537\n",
      "epoch: 3 trial 1369 training loss: 0.02192117180675268\n",
      "epoch: 3 trial 1370 training loss: 0.02813617791980505\n",
      "epoch: 3 trial 1371 training loss: 0.06338121555745602\n",
      "epoch: 3 trial 1372 training loss: 0.16391031071543694\n",
      "epoch: 3 trial 1373 training loss: 0.12086516432464123\n",
      "epoch: 3 trial 1374 training loss: 0.024387615267187357\n",
      "epoch: 3 trial 1375 training loss: 0.09359567426145077\n",
      "epoch: 3 trial 1376 training loss: 0.044088308699429035\n",
      "epoch: 3 trial 1377 training loss: 0.029094218276441097\n",
      "epoch: 3 trial 1378 training loss: 0.09225637651979923\n",
      "epoch: 3 trial 1379 training loss: 0.10467661172151566\n",
      "epoch: 3 trial 1380 training loss: 0.08797970972955227\n",
      "epoch: 3 trial 1381 training loss: 0.02376473182812333\n",
      "epoch: 3 trial 1382 training loss: 0.07214691210538149\n",
      "epoch: 3 trial 1383 training loss: 0.030363471247255802\n",
      "epoch: 3 trial 1384 training loss: 0.1063103899359703\n",
      "epoch: 3 trial 1385 training loss: 0.0349644897505641\n",
      "epoch: 3 trial 1386 training loss: 0.0690580029040575\n",
      "epoch: 3 trial 1387 training loss: 0.03142020385712385\n",
      "epoch: 3 trial 1388 training loss: 0.003622063435614109\n",
      "epoch: 3 trial 1389 training loss: 0.025673778261989355\n",
      "epoch: 3 trial 1390 training loss: 0.029820657335221767\n",
      "epoch: 3 trial 1391 training loss: 0.04717476759105921\n",
      "epoch: 3 trial 1392 training loss: 0.038633719086647034\n",
      "epoch: 3 trial 1393 training loss: 0.01145347603596747\n",
      "epoch: 3 trial 1394 training loss: 0.047524440102279186\n",
      "epoch: 3 trial 1395 training loss: 0.03501056879758835\n",
      "epoch: 3 trial 1396 training loss: 0.010403371648862958\n",
      "epoch: 3 trial 1397 training loss: 0.019859882537275553\n",
      "epoch: 3 trial 1398 training loss: 0.011626237072050571\n",
      "epoch: 3 trial 1399 training loss: 0.018026049248874187\n",
      "epoch: 3 trial 1400 training loss: 0.03886072710156441\n",
      "epoch: 3 trial 1401 training loss: 0.03475791588425636\n",
      "epoch: 3 trial 1402 training loss: 0.025695867836475372\n",
      "epoch: 3 trial 1403 training loss: 0.018984314985573292\n",
      "epoch: 3 trial 1404 training loss: 0.02018391666933894\n",
      "epoch: 3 trial 1405 training loss: 0.011694350512698293\n",
      "epoch: 3 trial 1406 training loss: 0.018692309502512217\n",
      "epoch: 3 trial 1407 training loss: 0.027643140871077776\n",
      "epoch: 3 trial 1408 training loss: 0.01655277283862233\n",
      "epoch: 3 trial 1409 training loss: 0.0078009492717683315\n",
      "epoch: 3 trial 1410 training loss: 0.014601723523810506\n",
      "epoch: 3 trial 1411 training loss: 0.10053038969635963\n",
      "epoch: 3 trial 1412 training loss: 0.016296306625008583\n",
      "epoch: 3 trial 1413 training loss: 0.03801549877971411\n",
      "epoch: 3 trial 1414 training loss: 0.016271519009023905\n",
      "epoch: 3 trial 1415 training loss: 0.052349286153912544\n",
      "epoch: 3 trial 1416 training loss: 0.040091884322464466\n",
      "epoch: 3 trial 1417 training loss: 0.03668602183461189\n",
      "epoch: 3 trial 1418 training loss: 0.03138939570635557\n",
      "epoch: 3 trial 1419 training loss: 0.01617284258827567\n",
      "epoch: 3 trial 1420 training loss: 0.11041583307087421\n",
      "epoch: 3 trial 1421 training loss: 0.007673508021980524\n",
      "epoch: 3 trial 1422 training loss: 0.019686798099428415\n",
      "epoch: 3 trial 1423 training loss: 0.14065825194120407\n",
      "epoch: 3 trial 1424 training loss: 0.031731970608234406\n",
      "epoch: 3 trial 1425 training loss: 0.023004969116300344\n",
      "epoch: 3 trial 1426 training loss: 0.012567940633744001\n",
      "epoch: 3 trial 1427 training loss: 0.015367159154266119\n",
      "epoch: 3 trial 1428 training loss: 0.02286461368203163\n",
      "epoch: 3 trial 1429 training loss: 0.015112841036170721\n",
      "epoch: 3 trial 1430 training loss: 0.03922409284859896\n",
      "epoch: 3 trial 1431 training loss: 0.15443234890699387\n",
      "epoch: 3 trial 1432 training loss: 0.06787958554923534\n",
      "epoch: 3 trial 1433 training loss: 0.025827329605817795\n",
      "epoch: 3 trial 1434 training loss: 0.13710083067417145\n",
      "epoch: 3 trial 1435 training loss: 0.12230003625154495\n",
      "epoch: 3 trial 1436 training loss: 0.030320373363792896\n",
      "epoch: 3 trial 1437 training loss: 0.0765820611268282\n",
      "epoch: 3 trial 1438 training loss: 0.007398973219096661\n",
      "epoch: 3 trial 1439 training loss: 0.07618994452059269\n",
      "epoch: 3 trial 1440 training loss: 0.016222395468503237\n",
      "epoch: 3 trial 1441 training loss: 0.028309078887104988\n",
      "epoch: 3 trial 1442 training loss: 0.11206601187586784\n",
      "epoch: 3 trial 1443 training loss: 0.03547909203916788\n",
      "epoch: 3 trial 1444 training loss: 0.012557562440633774\n",
      "epoch: 3 trial 1445 training loss: 0.03202437236905098\n",
      "epoch: 3 trial 1446 training loss: 0.10380401089787483\n",
      "epoch: 3 trial 1447 training loss: 0.022081592120230198\n",
      "epoch: 3 trial 1448 training loss: 0.03484358265995979\n",
      "epoch: 3 trial 1449 training loss: 0.03821686189621687\n",
      "epoch: 3 trial 1450 training loss: 0.02143507031723857\n",
      "epoch: 3 trial 1451 training loss: 0.010426304070279002\n",
      "epoch: 3 trial 1452 training loss: 0.06002804450690746\n",
      "epoch: 4 trial 1453 training loss: 0.006943484535440803\n",
      "epoch: 4 trial 1454 training loss: 0.013749624136835337\n",
      "epoch: 4 trial 1455 training loss: 0.020539636723697186\n",
      "epoch: 4 trial 1456 training loss: 0.06357152946293354\n",
      "epoch: 4 trial 1457 training loss: 0.004739762749522924\n",
      "epoch: 4 trial 1458 training loss: 0.005483105778694153\n",
      "epoch: 4 trial 1459 training loss: 0.03231789264827967\n",
      "epoch: 4 trial 1460 training loss: 0.026814620476216078\n",
      "epoch: 4 trial 1461 training loss: 0.010284424060955644\n",
      "epoch: 4 trial 1462 training loss: 0.019004995469003916\n",
      "epoch: 4 trial 1463 training loss: 0.05232086591422558\n",
      "epoch: 4 trial 1464 training loss: 0.02758903708308935\n",
      "epoch: 4 trial 1465 training loss: 0.02221660641953349\n",
      "epoch: 4 trial 1466 training loss: 0.05401748698204756\n",
      "epoch: 4 trial 1467 training loss: 0.015328493900597095\n",
      "epoch: 4 trial 1468 training loss: 0.022326205857098103\n",
      "epoch: 4 trial 1469 training loss: 0.12806709110736847\n",
      "epoch: 4 trial 1470 training loss: 0.0630050003528595\n",
      "epoch: 4 trial 1471 training loss: 0.018964175134897232\n",
      "epoch: 4 trial 1472 training loss: 0.019718298688530922\n",
      "epoch: 4 trial 1473 training loss: 0.0162787064909935\n",
      "epoch: 4 trial 1474 training loss: 0.077298978343606\n",
      "epoch: 4 trial 1475 training loss: 0.017125160433351994\n",
      "epoch: 4 trial 1476 training loss: 0.004681683843955398\n",
      "epoch: 4 trial 1477 training loss: 0.15445062890648842\n",
      "epoch: 4 trial 1478 training loss: 0.025317064486443996\n",
      "epoch: 4 trial 1479 training loss: 0.013686871388927102\n",
      "epoch: 4 trial 1480 training loss: 0.11968994140625\n",
      "epoch: 4 trial 1481 training loss: 0.02023368515074253\n",
      "epoch: 4 trial 1482 training loss: 0.024226500187069178\n",
      "epoch: 4 trial 1483 training loss: 0.04161118250340223\n",
      "epoch: 4 trial 1484 training loss: 0.013201167806982994\n",
      "epoch: 4 trial 1485 training loss: 0.030107769183814526\n",
      "epoch: 4 trial 1486 training loss: 0.04064053110778332\n",
      "epoch: 4 trial 1487 training loss: 0.03150665760040283\n",
      "epoch: 4 trial 1488 training loss: 0.06783834286034107\n",
      "epoch: 4 trial 1489 training loss: 0.14685499295592308\n",
      "epoch: 4 trial 1490 training loss: 0.3641771376132965\n",
      "epoch: 4 trial 1491 training loss: 0.0474387276917696\n",
      "epoch: 4 trial 1492 training loss: 0.14666328579187393\n",
      "epoch: 4 trial 1493 training loss: 0.14928775280714035\n",
      "epoch: 4 trial 1494 training loss: 0.013323275605216622\n",
      "epoch: 4 trial 1495 training loss: 0.0739027839154005\n",
      "epoch: 4 trial 1496 training loss: 0.04835558868944645\n",
      "epoch: 4 trial 1497 training loss: 0.06732730939984322\n",
      "epoch: 4 trial 1498 training loss: 0.05375523492693901\n",
      "epoch: 4 trial 1499 training loss: 0.007775551872327924\n",
      "epoch: 4 trial 1500 training loss: 0.014250271487981081\n",
      "epoch: 4 trial 1501 training loss: 0.011957742972299457\n",
      "epoch: 4 trial 1502 training loss: 0.0873331893235445\n",
      "epoch: 4 trial 1503 training loss: 0.04121989570558071\n",
      "epoch: 4 trial 1504 training loss: 0.08608681336045265\n",
      "epoch: 4 trial 1505 training loss: 0.06480276957154274\n",
      "epoch: 4 trial 1506 training loss: 0.09417720697820187\n",
      "epoch: 4 trial 1507 training loss: 0.03788670524954796\n",
      "epoch: 4 trial 1508 training loss: 0.09280481189489365\n",
      "epoch: 4 trial 1509 training loss: 0.02437178371474147\n",
      "epoch: 4 trial 1510 training loss: 0.0192695208825171\n",
      "epoch: 4 trial 1511 training loss: 0.026436588261276484\n",
      "epoch: 4 trial 1512 training loss: 0.07263817079365253\n",
      "epoch: 4 trial 1513 training loss: 0.020761311519891024\n",
      "epoch: 4 trial 1514 training loss: 0.12539280951023102\n",
      "epoch: 4 trial 1515 training loss: 0.017743540462106466\n",
      "epoch: 4 trial 1516 training loss: 0.012831809930503368\n",
      "epoch: 4 trial 1517 training loss: 0.04149943310767412\n",
      "epoch: 4 trial 1518 training loss: 0.06340340711176395\n",
      "epoch: 4 trial 1519 training loss: 0.021597473416477442\n",
      "epoch: 4 trial 1520 training loss: 0.01950421230867505\n",
      "epoch: 4 trial 1521 training loss: 0.06856529973447323\n",
      "epoch: 4 trial 1522 training loss: 0.03282738756388426\n",
      "epoch: 4 trial 1523 training loss: 0.04735254216939211\n",
      "epoch: 4 trial 1524 training loss: 0.026786381844431162\n",
      "epoch: 4 trial 1525 training loss: 0.0181114231236279\n",
      "epoch: 4 trial 1526 training loss: 0.06882959045469761\n",
      "epoch: 4 trial 1527 training loss: 0.019026482477784157\n",
      "epoch: 4 trial 1528 training loss: 0.02358011668547988\n",
      "epoch: 4 trial 1529 training loss: 0.025516124442219734\n",
      "epoch: 4 trial 1530 training loss: 0.12391352280974388\n",
      "epoch: 4 trial 1531 training loss: 0.016523408703505993\n",
      "epoch: 4 trial 1532 training loss: 0.06543276272714138\n",
      "epoch: 4 trial 1533 training loss: 0.032042406033724546\n",
      "epoch: 4 trial 1534 training loss: 0.044393992982804775\n",
      "epoch: 4 trial 1535 training loss: 0.1015114039182663\n",
      "epoch: 4 trial 1536 training loss: 0.022604361176490784\n",
      "epoch: 4 trial 1537 training loss: 0.06943638250231743\n",
      "epoch: 4 trial 1538 training loss: 0.013213124126195908\n",
      "epoch: 4 trial 1539 training loss: 0.03942031506448984\n",
      "epoch: 4 trial 1540 training loss: 0.059978460893034935\n",
      "epoch: 4 trial 1541 training loss: 0.051398297771811485\n",
      "epoch: 4 trial 1542 training loss: 0.007162773050367832\n",
      "epoch: 4 trial 1543 training loss: 0.038235737942159176\n",
      "epoch: 4 trial 1544 training loss: 0.024415822234004736\n",
      "epoch: 4 trial 1545 training loss: 0.021481990814208984\n",
      "epoch: 4 trial 1546 training loss: 0.07377306558191776\n",
      "epoch: 4 trial 1547 training loss: 0.05336418375372887\n",
      "epoch: 4 trial 1548 training loss: 0.05053623300045729\n",
      "epoch: 4 trial 1549 training loss: 0.017296587117016315\n",
      "epoch: 4 trial 1550 training loss: 0.11447574198246002\n",
      "epoch: 4 trial 1551 training loss: 0.052457170560956\n",
      "epoch: 4 trial 1552 training loss: 0.0387214245274663\n",
      "epoch: 4 trial 1553 training loss: 0.06833972409367561\n",
      "epoch: 4 trial 1554 training loss: 0.10501828417181969\n",
      "epoch: 4 trial 1555 training loss: 0.03973371908068657\n",
      "epoch: 4 trial 1556 training loss: 0.04730845242738724\n",
      "epoch: 4 trial 1557 training loss: 0.2841981127858162\n",
      "epoch: 4 trial 1558 training loss: 0.08819487132132053\n",
      "epoch: 4 trial 1559 training loss: 0.049531929194927216\n",
      "epoch: 4 trial 1560 training loss: 0.1460740715265274\n",
      "epoch: 4 trial 1561 training loss: 0.13502704724669456\n",
      "epoch: 4 trial 1562 training loss: 0.018941261805593967\n",
      "epoch: 4 trial 1563 training loss: 0.011439648922532797\n",
      "epoch: 4 trial 1564 training loss: 0.034421215765178204\n",
      "epoch: 4 trial 1565 training loss: 0.012269331840798259\n",
      "epoch: 4 trial 1566 training loss: 0.013205860508605838\n",
      "epoch: 4 trial 1567 training loss: 0.05023570917546749\n",
      "epoch: 4 trial 1568 training loss: 0.018768225330859423\n",
      "epoch: 4 trial 1569 training loss: 0.029393771663308144\n",
      "epoch: 4 trial 1570 training loss: 0.03936913888901472\n",
      "epoch: 4 trial 1571 training loss: 0.02880209358409047\n",
      "epoch: 4 trial 1572 training loss: 0.021274587139487267\n",
      "epoch: 4 trial 1573 training loss: 0.01117932004854083\n",
      "epoch: 4 trial 1574 training loss: 0.06993559747934341\n",
      "epoch: 4 trial 1575 training loss: 0.08421428129076958\n",
      "epoch: 4 trial 1576 training loss: 0.03572873771190643\n",
      "epoch: 4 trial 1577 training loss: 0.010003868490457535\n",
      "epoch: 4 trial 1578 training loss: 0.053182028234004974\n",
      "epoch: 4 trial 1579 training loss: 0.034058322198688984\n",
      "epoch: 4 trial 1580 training loss: 0.029609134420752525\n",
      "epoch: 4 trial 1581 training loss: 0.14133765548467636\n",
      "epoch: 4 trial 1582 training loss: 0.03252565208822489\n",
      "epoch: 4 trial 1583 training loss: 0.13755569979548454\n",
      "epoch: 4 trial 1584 training loss: 0.04420982673764229\n",
      "epoch: 4 trial 1585 training loss: 0.03376529738306999\n",
      "epoch: 4 trial 1586 training loss: 0.04975918773561716\n",
      "epoch: 4 trial 1587 training loss: 0.0160561369266361\n",
      "epoch: 4 trial 1588 training loss: 0.03345631808042526\n",
      "epoch: 4 trial 1589 training loss: 0.015593981370329857\n",
      "epoch: 4 trial 1590 training loss: 0.12996704131364822\n",
      "epoch: 4 trial 1591 training loss: 0.022148971911519766\n",
      "epoch: 4 trial 1592 training loss: 0.0623395424336195\n",
      "epoch: 4 trial 1593 training loss: 0.050700774416327477\n",
      "epoch: 4 trial 1594 training loss: 0.02821036195382476\n",
      "epoch: 4 trial 1595 training loss: 0.04846356064081192\n",
      "epoch: 4 trial 1596 training loss: 0.054640425369143486\n",
      "epoch: 4 trial 1597 training loss: 0.053653350099921227\n",
      "epoch: 4 trial 1598 training loss: 0.022218563593924046\n",
      "epoch: 4 trial 1599 training loss: 0.08797208406031132\n",
      "epoch: 4 trial 1600 training loss: 0.01862593204714358\n",
      "epoch: 4 trial 1601 training loss: 0.03830177988857031\n",
      "epoch: 4 trial 1602 training loss: 0.062484508380293846\n",
      "epoch: 4 trial 1603 training loss: 0.028610993176698685\n",
      "epoch: 4 trial 1604 training loss: 0.058487776666879654\n",
      "epoch: 4 trial 1605 training loss: 0.026818749494850636\n",
      "epoch: 4 trial 1606 training loss: 0.04464400280267\n",
      "epoch: 4 trial 1607 training loss: 0.01192110707052052\n",
      "epoch: 4 trial 1608 training loss: 0.0086253029294312\n",
      "epoch: 4 trial 1609 training loss: 0.01719671953469515\n",
      "epoch: 4 trial 1610 training loss: 0.0042940895073115826\n",
      "epoch: 4 trial 1611 training loss: 0.013677168637514114\n",
      "epoch: 4 trial 1612 training loss: 0.012902823742479086\n",
      "epoch: 4 trial 1613 training loss: 0.007249745074659586\n",
      "epoch: 4 trial 1614 training loss: 0.06216910108923912\n",
      "epoch: 4 trial 1615 training loss: 0.0026107991579920053\n",
      "epoch: 4 trial 1616 training loss: 0.04945824760943651\n",
      "epoch: 4 trial 1617 training loss: 0.01508101960644126\n",
      "epoch: 4 trial 1618 training loss: 0.025399135891348124\n",
      "epoch: 4 trial 1619 training loss: 0.025836422108113766\n",
      "epoch: 4 trial 1620 training loss: 0.012828608741983771\n",
      "epoch: 4 trial 1621 training loss: 0.0312929879873991\n",
      "epoch: 4 trial 1622 training loss: 0.14585700258612633\n",
      "epoch: 4 trial 1623 training loss: 0.050445158034563065\n",
      "epoch: 4 trial 1624 training loss: 0.008548643440008163\n",
      "epoch: 4 trial 1625 training loss: 0.05680359248071909\n",
      "epoch: 4 trial 1626 training loss: 0.04146101698279381\n",
      "epoch: 4 trial 1627 training loss: 0.052301245741546154\n",
      "epoch: 4 trial 1628 training loss: 0.03327365778386593\n",
      "epoch: 4 trial 1629 training loss: 0.007920377072878182\n",
      "epoch: 4 trial 1630 training loss: 0.03790254518389702\n",
      "epoch: 4 trial 1631 training loss: 0.044913074001669884\n",
      "epoch: 4 trial 1632 training loss: 0.03469807654619217\n",
      "epoch: 4 trial 1633 training loss: 0.024063297547399998\n",
      "epoch: 4 trial 1634 training loss: 0.026042668148875237\n",
      "epoch: 4 trial 1635 training loss: 0.036835298873484135\n",
      "epoch: 4 trial 1636 training loss: 0.01950529171153903\n",
      "epoch: 4 trial 1637 training loss: 0.023326450958848\n",
      "epoch: 4 trial 1638 training loss: 0.10334118269383907\n",
      "epoch: 4 trial 1639 training loss: 0.06296868436038494\n",
      "epoch: 4 trial 1640 training loss: 0.036191657185554504\n",
      "epoch: 4 trial 1641 training loss: 0.07062948495149612\n",
      "epoch: 4 trial 1642 training loss: 0.04759884625673294\n",
      "epoch: 4 trial 1643 training loss: 0.09027505107223988\n",
      "epoch: 4 trial 1644 training loss: 0.05148273892700672\n",
      "epoch: 4 trial 1645 training loss: 0.04446021746844053\n",
      "epoch: 4 trial 1646 training loss: 0.019063114188611507\n",
      "epoch: 4 trial 1647 training loss: 0.028186019510030746\n",
      "epoch: 4 trial 1648 training loss: 0.02243408700451255\n",
      "epoch: 4 trial 1649 training loss: 0.019160871393978596\n",
      "epoch: 4 trial 1650 training loss: 0.04936278238892555\n",
      "epoch: 4 trial 1651 training loss: 0.09788908250629902\n",
      "epoch: 4 trial 1652 training loss: 0.019473151303827763\n",
      "epoch: 4 trial 1653 training loss: 0.006056388956494629\n",
      "epoch: 4 trial 1654 training loss: 0.010095744859427214\n",
      "epoch: 4 trial 1655 training loss: 0.00780345662496984\n",
      "epoch: 4 trial 1656 training loss: 0.034788633696734905\n",
      "epoch: 4 trial 1657 training loss: 0.012433709809556603\n",
      "epoch: 4 trial 1658 training loss: 0.03816026169806719\n",
      "epoch: 4 trial 1659 training loss: 0.035926119424402714\n",
      "epoch: 4 trial 1660 training loss: 0.015428710728883743\n",
      "epoch: 4 trial 1661 training loss: 0.09632168523967266\n",
      "epoch: 4 trial 1662 training loss: 0.05378432013094425\n",
      "epoch: 4 trial 1663 training loss: 0.08312282711267471\n",
      "epoch: 4 trial 1664 training loss: 0.06220785342156887\n",
      "epoch: 4 trial 1665 training loss: 0.16059299930930138\n",
      "epoch: 4 trial 1666 training loss: 0.0095994567964226\n",
      "epoch: 4 trial 1667 training loss: 0.019859194289892912\n",
      "epoch: 4 trial 1668 training loss: 0.022719481959939003\n",
      "epoch: 4 trial 1669 training loss: 0.022325171157717705\n",
      "epoch: 4 trial 1670 training loss: 0.017534136306494474\n",
      "epoch: 4 trial 1671 training loss: 0.046616412699222565\n",
      "epoch: 4 trial 1672 training loss: 0.05812128819525242\n",
      "epoch: 4 trial 1673 training loss: 0.038217159919440746\n",
      "epoch: 4 trial 1674 training loss: 0.07324670627713203\n",
      "epoch: 4 trial 1675 training loss: 0.04870309494435787\n",
      "epoch: 4 trial 1676 training loss: 0.06923234835267067\n",
      "epoch: 4 trial 1677 training loss: 0.00938033894635737\n",
      "epoch: 4 trial 1678 training loss: 0.015730383805930614\n",
      "epoch: 4 trial 1679 training loss: 0.032525138929486275\n",
      "epoch: 4 trial 1680 training loss: 0.015080658718943596\n",
      "epoch: 4 trial 1681 training loss: 0.042100110091269016\n",
      "epoch: 4 trial 1682 training loss: 0.013935137307271361\n",
      "epoch: 4 trial 1683 training loss: 0.14806177467107773\n",
      "epoch: 4 trial 1684 training loss: 0.050994401797652245\n",
      "epoch: 4 trial 1685 training loss: 0.13490214571356773\n",
      "epoch: 4 trial 1686 training loss: 0.09056328237056732\n",
      "epoch: 4 trial 1687 training loss: 0.01939242286607623\n",
      "epoch: 4 trial 1688 training loss: 0.040189046412706375\n",
      "epoch: 4 trial 1689 training loss: 0.026869273278862238\n",
      "epoch: 4 trial 1690 training loss: 0.04625401645898819\n",
      "epoch: 4 trial 1691 training loss: 0.04479459207504988\n",
      "epoch: 4 trial 1692 training loss: 0.10933424159884453\n",
      "epoch: 4 trial 1693 training loss: 0.028672227170318365\n",
      "epoch: 4 trial 1694 training loss: 0.034334066323935986\n",
      "epoch: 4 trial 1695 training loss: 0.05376154184341431\n",
      "epoch: 4 trial 1696 training loss: 0.004326439229771495\n",
      "epoch: 4 trial 1697 training loss: 0.05418645869940519\n",
      "epoch: 4 trial 1698 training loss: 0.01888137124478817\n",
      "epoch: 4 trial 1699 training loss: 0.05207248590886593\n",
      "epoch: 4 trial 1700 training loss: 0.027883749920874834\n",
      "epoch: 4 trial 1701 training loss: 0.028682767413556576\n",
      "epoch: 4 trial 1702 training loss: 0.06405330635607243\n",
      "epoch: 4 trial 1703 training loss: 0.014376185368746519\n",
      "epoch: 4 trial 1704 training loss: 0.005904640769585967\n",
      "epoch: 4 trial 1705 training loss: 0.028200766071677208\n",
      "epoch: 4 trial 1706 training loss: 0.07732236199080944\n",
      "epoch: 4 trial 1707 training loss: 0.021178937517106533\n",
      "epoch: 4 trial 1708 training loss: 0.08176622353494167\n",
      "epoch: 4 trial 1709 training loss: 0.030266791582107544\n",
      "epoch: 4 trial 1710 training loss: 0.01751699298620224\n",
      "epoch: 4 trial 1711 training loss: 0.17459620907902718\n",
      "epoch: 4 trial 1712 training loss: 0.01447586016729474\n",
      "epoch: 4 trial 1713 training loss: 0.014293766347691417\n",
      "epoch: 4 trial 1714 training loss: 0.008805850986391306\n",
      "epoch: 4 trial 1715 training loss: 0.004089825553819537\n",
      "epoch: 4 trial 1716 training loss: 0.016787642613053322\n",
      "epoch: 4 trial 1717 training loss: 0.038029332645237446\n",
      "epoch: 4 trial 1718 training loss: 0.036841701716184616\n",
      "epoch: 4 trial 1719 training loss: 0.0360707426443696\n",
      "epoch: 4 trial 1720 training loss: 0.034595318138599396\n",
      "epoch: 4 trial 1721 training loss: 0.02380506368353963\n",
      "epoch: 4 trial 1722 training loss: 0.0839474406093359\n",
      "epoch: 4 trial 1723 training loss: 0.01512426370754838\n",
      "epoch: 4 trial 1724 training loss: 0.011178135639056563\n",
      "epoch: 4 trial 1725 training loss: 0.02308409195393324\n",
      "epoch: 4 trial 1726 training loss: 0.006925385911017656\n",
      "epoch: 4 trial 1727 training loss: 0.06608717888593674\n",
      "epoch: 4 trial 1728 training loss: 0.005682577204424888\n",
      "epoch: 4 trial 1729 training loss: 0.02458776580169797\n",
      "epoch: 4 trial 1730 training loss: 0.012248949613422155\n",
      "epoch: 4 trial 1731 training loss: 0.06870114803314209\n",
      "epoch: 4 trial 1732 training loss: 0.01883663423359394\n",
      "epoch: 4 trial 1733 training loss: 0.018747368827462196\n",
      "epoch: 4 trial 1734 training loss: 0.05940789543092251\n",
      "epoch: 4 trial 1735 training loss: 0.10314145311713219\n",
      "epoch: 4 trial 1736 training loss: 0.029376894235610962\n",
      "epoch: 4 trial 1737 training loss: 0.08472000993788242\n",
      "epoch: 4 trial 1738 training loss: 0.026073449291288853\n",
      "epoch: 4 trial 1739 training loss: 0.08805213496088982\n",
      "epoch: 4 trial 1740 training loss: 0.01796915987506509\n",
      "epoch: 4 trial 1741 training loss: 0.017624718602746725\n",
      "epoch: 4 trial 1742 training loss: 0.06121359393000603\n",
      "epoch: 4 trial 1743 training loss: 0.03142714034765959\n",
      "epoch: 4 trial 1744 training loss: 0.01839796034619212\n",
      "epoch: 4 trial 1745 training loss: 0.015887994086369872\n",
      "epoch: 4 trial 1746 training loss: 0.010046909330412745\n",
      "epoch: 4 trial 1747 training loss: 0.020042998250573874\n",
      "epoch: 4 trial 1748 training loss: 0.02863589208573103\n",
      "epoch: 4 trial 1749 training loss: 0.019166903104633093\n",
      "epoch: 4 trial 1750 training loss: 0.03413254860788584\n",
      "epoch: 4 trial 1751 training loss: 0.052678611129522324\n",
      "epoch: 4 trial 1752 training loss: 0.028094890527427197\n",
      "epoch: 4 trial 1753 training loss: 0.02178934495896101\n",
      "epoch: 4 trial 1754 training loss: 0.006924903253093362\n",
      "epoch: 4 trial 1755 training loss: 0.012502649798989296\n",
      "epoch: 4 trial 1756 training loss: 0.03070789948105812\n",
      "epoch: 4 trial 1757 training loss: 0.05082269199192524\n",
      "epoch: 4 trial 1758 training loss: 0.04914787597954273\n",
      "epoch: 4 trial 1759 training loss: 0.002658662269823253\n",
      "epoch: 4 trial 1760 training loss: 0.03316378686577082\n",
      "epoch: 4 trial 1761 training loss: 0.05006030015647411\n",
      "epoch: 4 trial 1762 training loss: 0.016212410992011428\n",
      "epoch: 4 trial 1763 training loss: 0.013367183739319444\n",
      "epoch: 4 trial 1764 training loss: 0.01909568812698126\n",
      "epoch: 4 trial 1765 training loss: 0.043773552402853966\n",
      "epoch: 4 trial 1766 training loss: 0.020533562172204256\n",
      "epoch: 4 trial 1767 training loss: 0.04002195596694946\n",
      "epoch: 4 trial 1768 training loss: 0.058120572939515114\n",
      "epoch: 4 trial 1769 training loss: 0.022866482846438885\n",
      "epoch: 4 trial 1770 training loss: 0.03163289837539196\n",
      "epoch: 4 trial 1771 training loss: 0.06852203421294689\n",
      "epoch: 4 trial 1772 training loss: 0.0345822349190712\n",
      "epoch: 4 trial 1773 training loss: 0.06920760683715343\n",
      "epoch: 4 trial 1774 training loss: 0.017787856049835682\n",
      "epoch: 4 trial 1775 training loss: 0.030669795349240303\n",
      "epoch: 4 trial 1776 training loss: 0.027536937035620213\n",
      "epoch: 4 trial 1777 training loss: 0.022898835595697165\n",
      "epoch: 4 trial 1778 training loss: 0.0163216779474169\n",
      "epoch: 4 trial 1779 training loss: 0.08304030448198318\n",
      "epoch: 4 trial 1780 training loss: 0.013723168056458235\n",
      "epoch: 4 trial 1781 training loss: 0.09793856739997864\n",
      "epoch: 4 trial 1782 training loss: 0.0273564625531435\n",
      "epoch: 4 trial 1783 training loss: 0.013330816756933928\n",
      "epoch: 4 trial 1784 training loss: 0.07242379803210497\n",
      "epoch: 4 trial 1785 training loss: 0.05421761702746153\n",
      "epoch: 4 trial 1786 training loss: 0.01074883621186018\n",
      "epoch: 4 trial 1787 training loss: 0.029753643553704023\n",
      "epoch: 4 trial 1788 training loss: 0.010868715588003397\n",
      "epoch: 4 trial 1789 training loss: 0.035109044052660465\n",
      "epoch: 4 trial 1790 training loss: 0.01827575685456395\n",
      "epoch: 4 trial 1791 training loss: 0.033253578934818506\n",
      "epoch: 4 trial 1792 training loss: 0.25880637764930725\n",
      "epoch: 4 trial 1793 training loss: 0.050503870472311974\n",
      "epoch: 4 trial 1794 training loss: 0.01052528852596879\n",
      "epoch: 4 trial 1795 training loss: 0.017453564796596766\n",
      "epoch: 4 trial 1796 training loss: 0.14756708964705467\n",
      "epoch: 4 trial 1797 training loss: 0.04971466399729252\n",
      "epoch: 4 trial 1798 training loss: 0.06282598525285721\n",
      "epoch: 4 trial 1799 training loss: 0.024892685003578663\n",
      "epoch: 4 trial 1800 training loss: 0.03764980938285589\n",
      "epoch: 4 trial 1801 training loss: 0.01870693266391754\n",
      "epoch: 4 trial 1802 training loss: 0.1003502607345581\n",
      "epoch: 4 trial 1803 training loss: 0.012479639146476984\n",
      "epoch: 4 trial 1804 training loss: 0.07554393261671066\n",
      "epoch: 4 trial 1805 training loss: 0.023907480761408806\n",
      "epoch: 4 trial 1806 training loss: 0.014968067407608032\n",
      "epoch: 4 trial 1807 training loss: 0.0494709312915802\n",
      "epoch: 4 trial 1808 training loss: 0.011867256136611104\n",
      "epoch: 4 trial 1809 training loss: 0.02791447378695011\n",
      "epoch: 4 trial 1810 training loss: 0.10086780227720737\n",
      "epoch: 4 trial 1811 training loss: 0.04714099410921335\n",
      "epoch: 4 trial 1812 training loss: 0.030111336149275303\n",
      "epoch: 4 trial 1813 training loss: 0.10994463413953781\n",
      "epoch: 4 trial 1814 training loss: 0.09386209398508072\n",
      "epoch: 4 trial 1815 training loss: 0.03631614614278078\n",
      "epoch: 4 trial 1816 training loss: 0.07889088802039623\n",
      "epoch: 4 trial 1817 training loss: 0.05495636165142059\n",
      "epoch: 4 trial 1818 training loss: 0.01705341972410679\n",
      "epoch: 4 trial 1819 training loss: 0.006286652875132859\n",
      "epoch: 4 trial 1820 training loss: 0.1893531084060669\n",
      "epoch: 4 trial 1821 training loss: 0.074185686185956\n",
      "epoch: 4 trial 1822 training loss: 0.03943609446287155\n",
      "epoch: 4 trial 1823 training loss: 0.05098698940128088\n",
      "epoch: 4 trial 1824 training loss: 0.028049486689269543\n",
      "epoch: 4 trial 1825 training loss: 0.03286421485245228\n",
      "epoch: 4 trial 1826 training loss: 0.02119983732700348\n",
      "epoch: 4 trial 1827 training loss: 0.04682808741927147\n",
      "epoch: 4 trial 1828 training loss: 0.04416985157877207\n",
      "epoch: 4 trial 1829 training loss: 0.050722384825348854\n",
      "epoch: 4 trial 1830 training loss: 0.07143509201705456\n",
      "epoch: 4 trial 1831 training loss: 0.04800570663064718\n",
      "epoch: 4 trial 1832 training loss: 0.014339214656502008\n",
      "epoch: 4 trial 1833 training loss: 0.029153469018638134\n",
      "epoch: 4 trial 1834 training loss: 0.007902998710051179\n",
      "epoch: 4 trial 1835 training loss: 0.047851039096713066\n",
      "epoch: 4 trial 1836 training loss: 0.0372895197942853\n",
      "epoch: 4 trial 1837 training loss: 0.011576147051528096\n",
      "epoch: 4 trial 1838 training loss: 0.0028188241994939744\n",
      "epoch: 4 trial 1839 training loss: 0.014421258587390184\n",
      "epoch: 4 trial 1840 training loss: 0.016876217909157276\n",
      "epoch: 4 trial 1841 training loss: 0.02658880827948451\n",
      "epoch: 4 trial 1842 training loss: 0.006318449508398771\n",
      "epoch: 4 trial 1843 training loss: 0.003333465429022908\n",
      "epoch: 4 trial 1844 training loss: 0.004306863178499043\n",
      "epoch: 4 trial 1845 training loss: 0.009158328175544739\n",
      "epoch: 4 trial 1846 training loss: 0.01293755928054452\n",
      "epoch: 4 trial 1847 training loss: 0.028625715523958206\n",
      "epoch: 4 trial 1848 training loss: 0.10362710058689117\n",
      "epoch: 4 trial 1849 training loss: 0.029607616364955902\n",
      "epoch: 4 trial 1850 training loss: 0.021709184162318707\n",
      "epoch: 4 trial 1851 training loss: 0.03237972781062126\n",
      "epoch: 4 trial 1852 training loss: 0.042713578790426254\n",
      "epoch: 4 trial 1853 training loss: 0.025943092070519924\n",
      "epoch: 4 trial 1854 training loss: 0.026260980870574713\n",
      "epoch: 4 trial 1855 training loss: 0.05985676683485508\n",
      "epoch: 4 trial 1856 training loss: 0.13303347304463387\n",
      "epoch: 4 trial 1857 training loss: 0.12671826779842377\n",
      "epoch: 4 trial 1858 training loss: 0.02648757677525282\n",
      "epoch: 4 trial 1859 training loss: 0.05702694598585367\n",
      "epoch: 4 trial 1860 training loss: 0.05076939146965742\n",
      "epoch: 4 trial 1861 training loss: 0.03462076559662819\n",
      "epoch: 4 trial 1862 training loss: 0.09083177335560322\n",
      "epoch: 4 trial 1863 training loss: 0.10981723479926586\n",
      "epoch: 4 trial 1864 training loss: 0.07980633713304996\n",
      "epoch: 4 trial 1865 training loss: 0.026005761697888374\n",
      "epoch: 4 trial 1866 training loss: 0.06587913818657398\n",
      "epoch: 4 trial 1867 training loss: 0.06550471298396587\n",
      "epoch: 4 trial 1868 training loss: 0.06670382246375084\n",
      "epoch: 4 trial 1869 training loss: 0.024408718571066856\n",
      "epoch: 4 trial 1870 training loss: 0.039670790545642376\n",
      "epoch: 4 trial 1871 training loss: 0.036828791722655296\n",
      "epoch: 4 trial 1872 training loss: 0.0035639083944261074\n",
      "epoch: 4 trial 1873 training loss: 0.018938672728836536\n",
      "epoch: 4 trial 1874 training loss: 0.03776093851774931\n",
      "epoch: 4 trial 1875 training loss: 0.04242026526480913\n",
      "epoch: 4 trial 1876 training loss: 0.02983321901410818\n",
      "epoch: 4 trial 1877 training loss: 0.012728665256872773\n",
      "epoch: 4 trial 1878 training loss: 0.05095923785120249\n",
      "epoch: 4 trial 1879 training loss: 0.02869797870516777\n",
      "epoch: 4 trial 1880 training loss: 0.01044073305092752\n",
      "epoch: 4 trial 1881 training loss: 0.020032888278365135\n",
      "epoch: 4 trial 1882 training loss: 0.012444925494492054\n",
      "epoch: 4 trial 1883 training loss: 0.01758553320541978\n",
      "epoch: 4 trial 1884 training loss: 0.03826953284442425\n",
      "epoch: 4 trial 1885 training loss: 0.032289160415530205\n",
      "epoch: 4 trial 1886 training loss: 0.02352969441562891\n",
      "epoch: 4 trial 1887 training loss: 0.017851319629698992\n",
      "epoch: 4 trial 1888 training loss: 0.018703436478972435\n",
      "epoch: 4 trial 1889 training loss: 0.011241346364840865\n",
      "epoch: 4 trial 1890 training loss: 0.01702951081097126\n",
      "epoch: 4 trial 1891 training loss: 0.026664862874895334\n",
      "epoch: 4 trial 1892 training loss: 0.018508001696318388\n",
      "epoch: 4 trial 1893 training loss: 0.00749480864033103\n",
      "epoch: 4 trial 1894 training loss: 0.016838816925883293\n",
      "epoch: 4 trial 1895 training loss: 0.09523043036460876\n",
      "epoch: 4 trial 1896 training loss: 0.016432465985417366\n",
      "epoch: 4 trial 1897 training loss: 0.029741830192506313\n",
      "epoch: 4 trial 1898 training loss: 0.01918035466223955\n",
      "epoch: 4 trial 1899 training loss: 0.04774891585111618\n",
      "epoch: 4 trial 1900 training loss: 0.03847929462790489\n",
      "epoch: 4 trial 1901 training loss: 0.03639069013297558\n",
      "epoch: 4 trial 1902 training loss: 0.02915429975837469\n",
      "epoch: 4 trial 1903 training loss: 0.017241457477211952\n",
      "epoch: 4 trial 1904 training loss: 0.10813049785792828\n",
      "epoch: 4 trial 1905 training loss: 0.006196488975547254\n",
      "epoch: 4 trial 1906 training loss: 0.018949750810861588\n",
      "epoch: 4 trial 1907 training loss: 0.1352502591907978\n",
      "epoch: 4 trial 1908 training loss: 0.02909495308995247\n",
      "epoch: 4 trial 1909 training loss: 0.022620304487645626\n",
      "epoch: 4 trial 1910 training loss: 0.013856489676982164\n",
      "epoch: 4 trial 1911 training loss: 0.01472610142081976\n",
      "epoch: 4 trial 1912 training loss: 0.025158345699310303\n",
      "epoch: 4 trial 1913 training loss: 0.01434593042358756\n",
      "epoch: 4 trial 1914 training loss: 0.04541651997715235\n",
      "epoch: 4 trial 1915 training loss: 0.15111958980560303\n",
      "epoch: 4 trial 1916 training loss: 0.06652624532580376\n",
      "epoch: 4 trial 1917 training loss: 0.02596123982220888\n",
      "epoch: 4 trial 1918 training loss: 0.12540634348988533\n",
      "epoch: 4 trial 1919 training loss: 0.11863571777939796\n",
      "epoch: 4 trial 1920 training loss: 0.027400375343859196\n",
      "epoch: 4 trial 1921 training loss: 0.07193002104759216\n",
      "epoch: 4 trial 1922 training loss: 0.006491354084573686\n",
      "epoch: 4 trial 1923 training loss: 0.06750788539648056\n",
      "epoch: 4 trial 1924 training loss: 0.015989544801414013\n",
      "epoch: 4 trial 1925 training loss: 0.026967971585690975\n",
      "epoch: 4 trial 1926 training loss: 0.11382385343313217\n",
      "epoch: 4 trial 1927 training loss: 0.038282325491309166\n",
      "epoch: 4 trial 1928 training loss: 0.013538285391405225\n",
      "epoch: 4 trial 1929 training loss: 0.025446271989494562\n",
      "epoch: 4 trial 1930 training loss: 0.09436731599271297\n",
      "epoch: 4 trial 1931 training loss: 0.018956209998577833\n",
      "epoch: 4 trial 1932 training loss: 0.03621755447238684\n",
      "epoch: 4 trial 1933 training loss: 0.03752224985510111\n",
      "epoch: 4 trial 1934 training loss: 0.016679976135492325\n",
      "epoch: 4 trial 1935 training loss: 0.010933157987892628\n",
      "epoch: 4 trial 1936 training loss: 0.057684944942593575\n",
      "epoch: 5 trial 1937 training loss: 0.005767593393102288\n",
      "epoch: 5 trial 1938 training loss: 0.013890627305954695\n",
      "epoch: 5 trial 1939 training loss: 0.022204968612641096\n",
      "epoch: 5 trial 1940 training loss: 0.06175587140023708\n",
      "epoch: 5 trial 1941 training loss: 0.00481998105533421\n",
      "epoch: 5 trial 1942 training loss: 0.00531271006911993\n",
      "epoch: 5 trial 1943 training loss: 0.03186746500432491\n",
      "epoch: 5 trial 1944 training loss: 0.025747102685272694\n",
      "epoch: 5 trial 1945 training loss: 0.00948576396331191\n",
      "epoch: 5 trial 1946 training loss: 0.019599208142608404\n",
      "epoch: 5 trial 1947 training loss: 0.04918334539979696\n",
      "epoch: 5 trial 1948 training loss: 0.027162999846041203\n",
      "epoch: 5 trial 1949 training loss: 0.0237508462741971\n",
      "epoch: 5 trial 1950 training loss: 0.04995555151253939\n",
      "epoch: 5 trial 1951 training loss: 0.014345163013786077\n",
      "epoch: 5 trial 1952 training loss: 0.023715458810329437\n",
      "epoch: 5 trial 1953 training loss: 0.1386670507490635\n",
      "epoch: 5 trial 1954 training loss: 0.05215279385447502\n",
      "epoch: 5 trial 1955 training loss: 0.014474440831691027\n",
      "epoch: 5 trial 1956 training loss: 0.01904179248958826\n",
      "epoch: 5 trial 1957 training loss: 0.01690332591533661\n",
      "epoch: 5 trial 1958 training loss: 0.08222915790975094\n",
      "epoch: 5 trial 1959 training loss: 0.01325856032781303\n",
      "epoch: 5 trial 1960 training loss: 0.005183609668165445\n",
      "epoch: 5 trial 1961 training loss: 0.1557270660996437\n",
      "epoch: 5 trial 1962 training loss: 0.02441050484776497\n",
      "epoch: 5 trial 1963 training loss: 0.013928454834967852\n",
      "epoch: 5 trial 1964 training loss: 0.12367658317089081\n",
      "epoch: 5 trial 1965 training loss: 0.02795282704755664\n",
      "epoch: 5 trial 1966 training loss: 0.02097547147423029\n",
      "epoch: 5 trial 1967 training loss: 0.04529198445379734\n",
      "epoch: 5 trial 1968 training loss: 0.014306398574262857\n",
      "epoch: 5 trial 1969 training loss: 0.03154869470745325\n",
      "epoch: 5 trial 1970 training loss: 0.04115439159795642\n",
      "epoch: 5 trial 1971 training loss: 0.024810368660837412\n",
      "epoch: 5 trial 1972 training loss: 0.05695791635662317\n",
      "epoch: 5 trial 1973 training loss: 0.1440434679389\n",
      "epoch: 5 trial 1974 training loss: 0.3187967389822006\n",
      "epoch: 5 trial 1975 training loss: 0.04474088363349438\n",
      "epoch: 5 trial 1976 training loss: 0.11421620287001133\n",
      "epoch: 5 trial 1977 training loss: 0.11819018237292767\n",
      "epoch: 5 trial 1978 training loss: 0.012559924274682999\n",
      "epoch: 5 trial 1979 training loss: 0.12032231315970421\n",
      "epoch: 5 trial 1980 training loss: 0.04830949194729328\n",
      "epoch: 5 trial 1981 training loss: 0.03293039556592703\n",
      "epoch: 5 trial 1982 training loss: 0.05104975961148739\n",
      "epoch: 5 trial 1983 training loss: 0.023060249630361795\n",
      "epoch: 5 trial 1984 training loss: 0.015294225420802832\n",
      "epoch: 5 trial 1985 training loss: 0.009864924475550652\n",
      "epoch: 5 trial 1986 training loss: 0.0874708816409111\n",
      "epoch: 5 trial 1987 training loss: 0.04100556764751673\n",
      "epoch: 5 trial 1988 training loss: 0.1079085972160101\n",
      "epoch: 5 trial 1989 training loss: 0.04609753005206585\n",
      "epoch: 5 trial 1990 training loss: 0.050026739947497845\n",
      "epoch: 5 trial 1991 training loss: 0.029985718429088593\n",
      "epoch: 5 trial 1992 training loss: 0.0745878629386425\n",
      "epoch: 5 trial 1993 training loss: 0.025053763762116432\n",
      "epoch: 5 trial 1994 training loss: 0.02424745913594961\n",
      "epoch: 5 trial 1995 training loss: 0.02427024208009243\n",
      "epoch: 5 trial 1996 training loss: 0.0739907305687666\n",
      "epoch: 5 trial 1997 training loss: 0.024643818847835064\n",
      "epoch: 5 trial 1998 training loss: 0.12551988661289215\n",
      "epoch: 5 trial 1999 training loss: 0.019633354619145393\n",
      "epoch: 5 trial 2000 training loss: 0.017541888169944286\n",
      "epoch: 5 trial 2001 training loss: 0.03266429342329502\n",
      "epoch: 5 trial 2002 training loss: 0.05814937874674797\n",
      "epoch: 5 trial 2003 training loss: 0.022505980916321278\n",
      "epoch: 5 trial 2004 training loss: 0.021095816511660814\n",
      "epoch: 5 trial 2005 training loss: 0.057903990149497986\n",
      "epoch: 5 trial 2006 training loss: 0.03223551623523235\n",
      "epoch: 5 trial 2007 training loss: 0.04506757948547602\n",
      "epoch: 5 trial 2008 training loss: 0.03128307266160846\n",
      "epoch: 5 trial 2009 training loss: 0.023225202225148678\n",
      "epoch: 5 trial 2010 training loss: 0.06728712283074856\n",
      "epoch: 5 trial 2011 training loss: 0.013586620800197124\n",
      "epoch: 5 trial 2012 training loss: 0.02351956395432353\n",
      "epoch: 5 trial 2013 training loss: 0.026531234849244356\n",
      "epoch: 5 trial 2014 training loss: 0.10120101645588875\n",
      "epoch: 5 trial 2015 training loss: 0.023258022498339415\n",
      "epoch: 5 trial 2016 training loss: 0.059059230610728264\n",
      "epoch: 5 trial 2017 training loss: 0.030465196818113327\n",
      "epoch: 5 trial 2018 training loss: 0.04539977665990591\n",
      "epoch: 5 trial 2019 training loss: 0.07058346271514893\n",
      "epoch: 5 trial 2020 training loss: 0.01822094339877367\n",
      "epoch: 5 trial 2021 training loss: 0.07212013751268387\n",
      "epoch: 5 trial 2022 training loss: 0.012588252313435078\n",
      "epoch: 5 trial 2023 training loss: 0.0380078824236989\n",
      "epoch: 5 trial 2024 training loss: 0.07855113595724106\n",
      "epoch: 5 trial 2025 training loss: 0.0495002344250679\n",
      "epoch: 5 trial 2026 training loss: 0.007960950024425983\n",
      "epoch: 5 trial 2027 training loss: 0.03708446957170963\n",
      "epoch: 5 trial 2028 training loss: 0.023724768310785294\n",
      "epoch: 5 trial 2029 training loss: 0.018847845029085875\n",
      "epoch: 5 trial 2030 training loss: 0.07818428054451942\n",
      "epoch: 5 trial 2031 training loss: 0.052260502241551876\n",
      "epoch: 5 trial 2032 training loss: 0.0588038694113493\n",
      "epoch: 5 trial 2033 training loss: 0.014792241621762514\n",
      "epoch: 5 trial 2034 training loss: 0.10457305237650871\n",
      "epoch: 5 trial 2035 training loss: 0.04731817450374365\n",
      "epoch: 5 trial 2036 training loss: 0.04166881740093231\n",
      "epoch: 5 trial 2037 training loss: 0.05452491156756878\n",
      "epoch: 5 trial 2038 training loss: 0.08488966710865498\n",
      "epoch: 5 trial 2039 training loss: 0.0358639289624989\n",
      "epoch: 5 trial 2040 training loss: 0.04314687103033066\n",
      "epoch: 5 trial 2041 training loss: 0.2663399204611778\n",
      "epoch: 5 trial 2042 training loss: 0.07022271119058132\n",
      "epoch: 5 trial 2043 training loss: 0.04626226890832186\n",
      "epoch: 5 trial 2044 training loss: 0.138409323990345\n",
      "epoch: 5 trial 2045 training loss: 0.12564682960510254\n",
      "epoch: 5 trial 2046 training loss: 0.017937153112143278\n",
      "epoch: 5 trial 2047 training loss: 0.01172692165710032\n",
      "epoch: 5 trial 2048 training loss: 0.030566767789423466\n",
      "epoch: 5 trial 2049 training loss: 0.014499015640467405\n",
      "epoch: 5 trial 2050 training loss: 0.01431043865159154\n",
      "epoch: 5 trial 2051 training loss: 0.05392664112150669\n",
      "epoch: 5 trial 2052 training loss: 0.019474693574011326\n",
      "epoch: 5 trial 2053 training loss: 0.02886342443525791\n",
      "epoch: 5 trial 2054 training loss: 0.041673154570162296\n",
      "epoch: 5 trial 2055 training loss: 0.0283671747893095\n",
      "epoch: 5 trial 2056 training loss: 0.020735494792461395\n",
      "epoch: 5 trial 2057 training loss: 0.011108028702437878\n",
      "epoch: 5 trial 2058 training loss: 0.06096779741346836\n",
      "epoch: 5 trial 2059 training loss: 0.08258924260735512\n",
      "epoch: 5 trial 2060 training loss: 0.03398749092593789\n",
      "epoch: 5 trial 2061 training loss: 0.009740872774273157\n",
      "epoch: 5 trial 2062 training loss: 0.04962989501655102\n",
      "epoch: 5 trial 2063 training loss: 0.028984817676246166\n",
      "epoch: 5 trial 2064 training loss: 0.02686194982379675\n",
      "epoch: 5 trial 2065 training loss: 0.13805558905005455\n",
      "epoch: 5 trial 2066 training loss: 0.03043606597930193\n",
      "epoch: 5 trial 2067 training loss: 0.11357822269201279\n",
      "epoch: 5 trial 2068 training loss: 0.04084308352321386\n",
      "epoch: 5 trial 2069 training loss: 0.0342288613319397\n",
      "epoch: 5 trial 2070 training loss: 0.052870312705636024\n",
      "epoch: 5 trial 2071 training loss: 0.01624100748449564\n",
      "epoch: 5 trial 2072 training loss: 0.038503196090459824\n",
      "epoch: 5 trial 2073 training loss: 0.016067583113908768\n",
      "epoch: 5 trial 2074 training loss: 0.12704751268029213\n",
      "epoch: 5 trial 2075 training loss: 0.021765633951872587\n",
      "epoch: 5 trial 2076 training loss: 0.05617436394095421\n",
      "epoch: 5 trial 2077 training loss: 0.05254646297544241\n",
      "epoch: 5 trial 2078 training loss: 0.030052577145397663\n",
      "epoch: 5 trial 2079 training loss: 0.040985848754644394\n",
      "epoch: 5 trial 2080 training loss: 0.05163787119090557\n",
      "epoch: 5 trial 2081 training loss: 0.054118746891617775\n",
      "epoch: 5 trial 2082 training loss: 0.0245599583722651\n",
      "epoch: 5 trial 2083 training loss: 0.08918759226799011\n",
      "epoch: 5 trial 2084 training loss: 0.01574978046119213\n",
      "epoch: 5 trial 2085 training loss: 0.04435650631785393\n",
      "epoch: 5 trial 2086 training loss: 0.055434826761484146\n",
      "epoch: 5 trial 2087 training loss: 0.02871150430291891\n",
      "epoch: 5 trial 2088 training loss: 0.05295453127473593\n",
      "epoch: 5 trial 2089 training loss: 0.02286761160939932\n",
      "epoch: 5 trial 2090 training loss: 0.039519017562270164\n",
      "epoch: 5 trial 2091 training loss: 0.01157914730720222\n",
      "epoch: 5 trial 2092 training loss: 0.009782010689377785\n",
      "epoch: 5 trial 2093 training loss: 0.01465356140397489\n",
      "epoch: 5 trial 2094 training loss: 0.003988221869803965\n",
      "epoch: 5 trial 2095 training loss: 0.013911692425608635\n",
      "epoch: 5 trial 2096 training loss: 0.012028693221509457\n",
      "epoch: 5 trial 2097 training loss: 0.006931474665179849\n",
      "epoch: 5 trial 2098 training loss: 0.06557810120284557\n",
      "epoch: 5 trial 2099 training loss: 0.002690456691198051\n",
      "epoch: 5 trial 2100 training loss: 0.0572578813880682\n",
      "epoch: 5 trial 2101 training loss: 0.0129940384067595\n",
      "epoch: 5 trial 2102 training loss: 0.026144572999328375\n",
      "epoch: 5 trial 2103 training loss: 0.031552014872431755\n",
      "epoch: 5 trial 2104 training loss: 0.0124586820602417\n",
      "epoch: 5 trial 2105 training loss: 0.03423443343490362\n",
      "epoch: 5 trial 2106 training loss: 0.1488286592066288\n",
      "epoch: 5 trial 2107 training loss: 0.0503803500905633\n",
      "epoch: 5 trial 2108 training loss: 0.007937828311696649\n",
      "epoch: 5 trial 2109 training loss: 0.054912716150283813\n",
      "epoch: 5 trial 2110 training loss: 0.035816350020468235\n",
      "epoch: 5 trial 2111 training loss: 0.053988439962267876\n",
      "epoch: 5 trial 2112 training loss: 0.03170693852007389\n",
      "epoch: 5 trial 2113 training loss: 0.009133178507909179\n",
      "epoch: 5 trial 2114 training loss: 0.030677150934934616\n",
      "epoch: 5 trial 2115 training loss: 0.04108708258718252\n",
      "epoch: 5 trial 2116 training loss: 0.037739803083240986\n",
      "epoch: 5 trial 2117 training loss: 0.03217294532805681\n",
      "epoch: 5 trial 2118 training loss: 0.01690347446128726\n",
      "epoch: 5 trial 2119 training loss: 0.0433749295771122\n",
      "epoch: 5 trial 2120 training loss: 0.019426811020821333\n",
      "epoch: 5 trial 2121 training loss: 0.022080065682530403\n",
      "epoch: 5 trial 2122 training loss: 0.10021314397454262\n",
      "epoch: 5 trial 2123 training loss: 0.058405306190252304\n",
      "epoch: 5 trial 2124 training loss: 0.03495163843035698\n",
      "epoch: 5 trial 2125 training loss: 0.0705408900976181\n",
      "epoch: 5 trial 2126 training loss: 0.04723313730210066\n",
      "epoch: 5 trial 2127 training loss: 0.09602936170995235\n",
      "epoch: 5 trial 2128 training loss: 0.05175557266920805\n",
      "epoch: 5 trial 2129 training loss: 0.048435735516250134\n",
      "epoch: 5 trial 2130 training loss: 0.020225469954311848\n",
      "epoch: 5 trial 2131 training loss: 0.03705765772610903\n",
      "epoch: 5 trial 2132 training loss: 0.025429822504520416\n",
      "epoch: 5 trial 2133 training loss: 0.02160928538069129\n",
      "epoch: 5 trial 2134 training loss: 0.05350862070918083\n",
      "epoch: 5 trial 2135 training loss: 0.09559732861816883\n",
      "epoch: 5 trial 2136 training loss: 0.009951982880011201\n",
      "epoch: 5 trial 2137 training loss: 0.006691637565381825\n",
      "epoch: 5 trial 2138 training loss: 0.010172282112762332\n",
      "epoch: 5 trial 2139 training loss: 0.008520913077518344\n",
      "epoch: 5 trial 2140 training loss: 0.022962949238717556\n",
      "epoch: 5 trial 2141 training loss: 0.010707286186516285\n",
      "epoch: 5 trial 2142 training loss: 0.03891967702656984\n",
      "epoch: 5 trial 2143 training loss: 0.03366017621010542\n",
      "epoch: 5 trial 2144 training loss: 0.014375538332387805\n",
      "epoch: 5 trial 2145 training loss: 0.08269576355814934\n",
      "epoch: 5 trial 2146 training loss: 0.04012706270441413\n",
      "epoch: 5 trial 2147 training loss: 0.0678466409444809\n",
      "epoch: 5 trial 2148 training loss: 0.06361373327672482\n",
      "epoch: 5 trial 2149 training loss: 0.14328724890947342\n",
      "epoch: 5 trial 2150 training loss: 0.011606837389990687\n",
      "epoch: 5 trial 2151 training loss: 0.018501512240618467\n",
      "epoch: 5 trial 2152 training loss: 0.023137939162552357\n",
      "epoch: 5 trial 2153 training loss: 0.02157018566504121\n",
      "epoch: 5 trial 2154 training loss: 0.018338685855269432\n",
      "epoch: 5 trial 2155 training loss: 0.04780402220785618\n",
      "epoch: 5 trial 2156 training loss: 0.05647352710366249\n",
      "epoch: 5 trial 2157 training loss: 0.035531884990632534\n",
      "epoch: 5 trial 2158 training loss: 0.06852667033672333\n",
      "epoch: 5 trial 2159 training loss: 0.045260610058903694\n",
      "epoch: 5 trial 2160 training loss: 0.07489022240042686\n",
      "epoch: 5 trial 2161 training loss: 0.008965952903963625\n",
      "epoch: 5 trial 2162 training loss: 0.016881545539945364\n",
      "epoch: 5 trial 2163 training loss: 0.03074327390640974\n",
      "epoch: 5 trial 2164 training loss: 0.015692600049078465\n",
      "epoch: 5 trial 2165 training loss: 0.05169382691383362\n",
      "epoch: 5 trial 2166 training loss: 0.014526442857459188\n",
      "epoch: 5 trial 2167 training loss: 0.14204149693250656\n",
      "epoch: 5 trial 2168 training loss: 0.05052479263395071\n",
      "epoch: 5 trial 2169 training loss: 0.14545341208577156\n",
      "epoch: 5 trial 2170 training loss: 0.09614671766757965\n",
      "epoch: 5 trial 2171 training loss: 0.018666601506993175\n",
      "epoch: 5 trial 2172 training loss: 0.04316742718219757\n",
      "epoch: 5 trial 2173 training loss: 0.02456650882959366\n",
      "epoch: 5 trial 2174 training loss: 0.04664831701666117\n",
      "epoch: 5 trial 2175 training loss: 0.04885152727365494\n",
      "epoch: 5 trial 2176 training loss: 0.11132813803851604\n",
      "epoch: 5 trial 2177 training loss: 0.03337316773831844\n",
      "epoch: 5 trial 2178 training loss: 0.03551718033850193\n",
      "epoch: 5 trial 2179 training loss: 0.05306039750576019\n",
      "epoch: 5 trial 2180 training loss: 0.00409013160970062\n",
      "epoch: 5 trial 2181 training loss: 0.05367629695683718\n",
      "epoch: 5 trial 2182 training loss: 0.01874787826091051\n",
      "epoch: 5 trial 2183 training loss: 0.04905587527900934\n",
      "epoch: 5 trial 2184 training loss: 0.0299093178473413\n",
      "epoch: 5 trial 2185 training loss: 0.029193616472184658\n",
      "epoch: 5 trial 2186 training loss: 0.06736738234758377\n",
      "epoch: 5 trial 2187 training loss: 0.014753898605704308\n",
      "epoch: 5 trial 2188 training loss: 0.005778874037787318\n",
      "epoch: 5 trial 2189 training loss: 0.02945002168416977\n",
      "epoch: 5 trial 2190 training loss: 0.08227086067199707\n",
      "epoch: 5 trial 2191 training loss: 0.022405068390071392\n",
      "epoch: 5 trial 2192 training loss: 0.07698890753090382\n",
      "epoch: 5 trial 2193 training loss: 0.030245047993957996\n",
      "epoch: 5 trial 2194 training loss: 0.018114177510142326\n",
      "epoch: 5 trial 2195 training loss: 0.1664866954088211\n",
      "epoch: 5 trial 2196 training loss: 0.014153279829770327\n",
      "epoch: 5 trial 2197 training loss: 0.015554739162325859\n",
      "epoch: 5 trial 2198 training loss: 0.00828104605898261\n",
      "epoch: 5 trial 2199 training loss: 0.004145419341512024\n",
      "epoch: 5 trial 2200 training loss: 0.01822891365736723\n",
      "epoch: 5 trial 2201 training loss: 0.03973891772329807\n",
      "epoch: 5 trial 2202 training loss: 0.035953959450125694\n",
      "epoch: 5 trial 2203 training loss: 0.03809174243360758\n",
      "epoch: 5 trial 2204 training loss: 0.030166690703481436\n",
      "epoch: 5 trial 2205 training loss: 0.025930945295840502\n",
      "epoch: 5 trial 2206 training loss: 0.0828170869499445\n",
      "epoch: 5 trial 2207 training loss: 0.014902188442647457\n",
      "epoch: 5 trial 2208 training loss: 0.011311972513794899\n",
      "epoch: 5 trial 2209 training loss: 0.022654449567198753\n",
      "epoch: 5 trial 2210 training loss: 0.005827418761327863\n",
      "epoch: 5 trial 2211 training loss: 0.07107835449278355\n",
      "epoch: 5 trial 2212 training loss: 0.00517793302424252\n",
      "epoch: 5 trial 2213 training loss: 0.022397574968636036\n",
      "epoch: 5 trial 2214 training loss: 0.011591721326112747\n",
      "epoch: 5 trial 2215 training loss: 0.07059112377464771\n",
      "epoch: 5 trial 2216 training loss: 0.016990273725241423\n",
      "epoch: 5 trial 2217 training loss: 0.017035407479852438\n",
      "epoch: 5 trial 2218 training loss: 0.05962924845516682\n",
      "epoch: 5 trial 2219 training loss: 0.09944915771484375\n",
      "epoch: 5 trial 2220 training loss: 0.03050668118521571\n",
      "epoch: 5 trial 2221 training loss: 0.08723649010062218\n",
      "epoch: 5 trial 2222 training loss: 0.026065890211611986\n",
      "epoch: 5 trial 2223 training loss: 0.09203030169010162\n",
      "epoch: 5 trial 2224 training loss: 0.018936078529804945\n",
      "epoch: 5 trial 2225 training loss: 0.019603498745709658\n",
      "epoch: 5 trial 2226 training loss: 0.06082608737051487\n",
      "epoch: 5 trial 2227 training loss: 0.030308926478028297\n",
      "epoch: 5 trial 2228 training loss: 0.018082561902701855\n",
      "epoch: 5 trial 2229 training loss: 0.01616215007379651\n",
      "epoch: 5 trial 2230 training loss: 0.008871987694874406\n",
      "epoch: 5 trial 2231 training loss: 0.01701824669726193\n",
      "epoch: 5 trial 2232 training loss: 0.026708812452852726\n",
      "epoch: 5 trial 2233 training loss: 0.021612287033349276\n",
      "epoch: 5 trial 2234 training loss: 0.03287913789972663\n",
      "epoch: 5 trial 2235 training loss: 0.053790184669196606\n",
      "epoch: 5 trial 2236 training loss: 0.028838138096034527\n",
      "epoch: 5 trial 2237 training loss: 0.02147212205454707\n",
      "epoch: 5 trial 2238 training loss: 0.00786310713738203\n",
      "epoch: 5 trial 2239 training loss: 0.013471728656440973\n",
      "epoch: 5 trial 2240 training loss: 0.030573147349059582\n",
      "epoch: 5 trial 2241 training loss: 0.0540704932063818\n",
      "epoch: 5 trial 2242 training loss: 0.052730742841959\n",
      "epoch: 5 trial 2243 training loss: 0.002422325895167887\n",
      "epoch: 5 trial 2244 training loss: 0.03306775074452162\n",
      "epoch: 5 trial 2245 training loss: 0.04818948917090893\n",
      "epoch: 5 trial 2246 training loss: 0.017351376824080944\n",
      "epoch: 5 trial 2247 training loss: 0.01090627140365541\n",
      "epoch: 5 trial 2248 training loss: 0.016338806599378586\n",
      "epoch: 5 trial 2249 training loss: 0.044320400804281235\n",
      "epoch: 5 trial 2250 training loss: 0.02375839091837406\n",
      "epoch: 5 trial 2251 training loss: 0.03398918453603983\n",
      "epoch: 5 trial 2252 training loss: 0.05557733029127121\n",
      "epoch: 5 trial 2253 training loss: 0.02456489810720086\n",
      "epoch: 5 trial 2254 training loss: 0.03192329593002796\n",
      "epoch: 5 trial 2255 training loss: 0.07110796310007572\n",
      "epoch: 5 trial 2256 training loss: 0.03359704837203026\n",
      "epoch: 5 trial 2257 training loss: 0.06866603903472424\n",
      "epoch: 5 trial 2258 training loss: 0.018458670005202293\n",
      "epoch: 5 trial 2259 training loss: 0.029892466962337494\n",
      "epoch: 5 trial 2260 training loss: 0.028137714602053165\n",
      "epoch: 5 trial 2261 training loss: 0.022672745864838362\n",
      "epoch: 5 trial 2262 training loss: 0.016486695036292076\n",
      "epoch: 5 trial 2263 training loss: 0.09669825248420238\n",
      "epoch: 5 trial 2264 training loss: 0.014616707805544138\n",
      "epoch: 5 trial 2265 training loss: 0.10079719871282578\n",
      "epoch: 5 trial 2266 training loss: 0.0300392871722579\n",
      "epoch: 5 trial 2267 training loss: 0.011572398012503982\n",
      "epoch: 5 trial 2268 training loss: 0.06601738557219505\n",
      "epoch: 5 trial 2269 training loss: 0.050086657516658306\n",
      "epoch: 5 trial 2270 training loss: 0.009744699345901608\n",
      "epoch: 5 trial 2271 training loss: 0.030232582241296768\n",
      "epoch: 5 trial 2272 training loss: 0.009887751890346408\n",
      "epoch: 5 trial 2273 training loss: 0.03513370640575886\n",
      "epoch: 5 trial 2274 training loss: 0.0191478431224823\n",
      "epoch: 5 trial 2275 training loss: 0.027860520407557487\n",
      "epoch: 5 trial 2276 training loss: 0.25711672008037567\n",
      "epoch: 5 trial 2277 training loss: 0.04864322021603584\n",
      "epoch: 5 trial 2278 training loss: 0.01172312325797975\n",
      "epoch: 5 trial 2279 training loss: 0.020885311532765627\n",
      "epoch: 5 trial 2280 training loss: 0.14320804551243782\n",
      "epoch: 5 trial 2281 training loss: 0.049028776586055756\n",
      "epoch: 5 trial 2282 training loss: 0.05848250538110733\n",
      "epoch: 5 trial 2283 training loss: 0.02306411601603031\n",
      "epoch: 5 trial 2284 training loss: 0.03987228870391846\n",
      "epoch: 5 trial 2285 training loss: 0.019589544739574194\n",
      "epoch: 5 trial 2286 training loss: 0.09500748105347157\n",
      "epoch: 5 trial 2287 training loss: 0.011638984084129333\n",
      "epoch: 5 trial 2288 training loss: 0.06935033574700356\n",
      "epoch: 5 trial 2289 training loss: 0.026069448329508305\n",
      "epoch: 5 trial 2290 training loss: 0.015342417871579528\n",
      "epoch: 5 trial 2291 training loss: 0.05181729607284069\n",
      "epoch: 5 trial 2292 training loss: 0.011671805754303932\n",
      "epoch: 5 trial 2293 training loss: 0.03186400234699249\n",
      "epoch: 5 trial 2294 training loss: 0.09658307209610939\n",
      "epoch: 5 trial 2295 training loss: 0.0467531643807888\n",
      "epoch: 5 trial 2296 training loss: 0.029378174804151058\n",
      "epoch: 5 trial 2297 training loss: 0.10842832177877426\n",
      "epoch: 5 trial 2298 training loss: 0.09146223776042461\n",
      "epoch: 5 trial 2299 training loss: 0.03912976756691933\n",
      "epoch: 5 trial 2300 training loss: 0.08252941444516182\n",
      "epoch: 5 trial 2301 training loss: 0.05196991376578808\n",
      "epoch: 5 trial 2302 training loss: 0.015806999523192644\n",
      "epoch: 5 trial 2303 training loss: 0.007087793434038758\n",
      "epoch: 5 trial 2304 training loss: 0.19562020525336266\n",
      "epoch: 5 trial 2305 training loss: 0.06142587959766388\n",
      "epoch: 5 trial 2306 training loss: 0.03871997818350792\n",
      "epoch: 5 trial 2307 training loss: 0.053399582393467426\n",
      "epoch: 5 trial 2308 training loss: 0.029695495031774044\n",
      "epoch: 5 trial 2309 training loss: 0.03225262090563774\n",
      "epoch: 5 trial 2310 training loss: 0.02108644088730216\n",
      "epoch: 5 trial 2311 training loss: 0.04755521938204765\n",
      "epoch: 5 trial 2312 training loss: 0.035626817494630814\n",
      "epoch: 5 trial 2313 training loss: 0.04660035762935877\n",
      "epoch: 5 trial 2314 training loss: 0.082356296479702\n",
      "epoch: 5 trial 2315 training loss: 0.03638102300465107\n",
      "epoch: 5 trial 2316 training loss: 0.012259028851985931\n",
      "epoch: 5 trial 2317 training loss: 0.027964833192527294\n",
      "epoch: 5 trial 2318 training loss: 0.007985879899933934\n",
      "epoch: 5 trial 2319 training loss: 0.05072369985282421\n",
      "epoch: 5 trial 2320 training loss: 0.03946198523044586\n",
      "epoch: 5 trial 2321 training loss: 0.011794000631198287\n",
      "epoch: 5 trial 2322 training loss: 0.002747259277384728\n",
      "epoch: 5 trial 2323 training loss: 0.014029218349605799\n",
      "epoch: 5 trial 2324 training loss: 0.01702945213764906\n",
      "epoch: 5 trial 2325 training loss: 0.027070719748735428\n",
      "epoch: 5 trial 2326 training loss: 0.0062770271906629205\n",
      "epoch: 5 trial 2327 training loss: 0.0033776763011701405\n",
      "epoch: 5 trial 2328 training loss: 0.004733179230242968\n",
      "epoch: 5 trial 2329 training loss: 0.009129164041951299\n",
      "epoch: 5 trial 2330 training loss: 0.014333765022456646\n",
      "epoch: 5 trial 2331 training loss: 0.02854964230209589\n",
      "epoch: 5 trial 2332 training loss: 0.10850131511688232\n",
      "epoch: 5 trial 2333 training loss: 0.029939076863229275\n",
      "epoch: 5 trial 2334 training loss: 0.02498102653771639\n",
      "epoch: 5 trial 2335 training loss: 0.03212800435721874\n",
      "epoch: 5 trial 2336 training loss: 0.03931216150522232\n",
      "epoch: 5 trial 2337 training loss: 0.02517921570688486\n",
      "epoch: 5 trial 2338 training loss: 0.026977334171533585\n",
      "epoch: 5 trial 2339 training loss: 0.05725352466106415\n",
      "epoch: 5 trial 2340 training loss: 0.12941231578588486\n",
      "epoch: 5 trial 2341 training loss: 0.13314785808324814\n",
      "epoch: 5 trial 2342 training loss: 0.02314524771645665\n",
      "epoch: 5 trial 2343 training loss: 0.056093061342835426\n",
      "epoch: 5 trial 2344 training loss: 0.05175135191529989\n",
      "epoch: 5 trial 2345 training loss: 0.032619282603263855\n",
      "epoch: 5 trial 2346 training loss: 0.08891984261572361\n",
      "epoch: 5 trial 2347 training loss: 0.11245521903038025\n",
      "epoch: 5 trial 2348 training loss: 0.08048629388213158\n",
      "epoch: 5 trial 2349 training loss: 0.02266209525987506\n",
      "epoch: 5 trial 2350 training loss: 0.07505678571760654\n",
      "epoch: 5 trial 2351 training loss: 0.03666284494102001\n",
      "epoch: 5 trial 2352 training loss: 0.08667046390473843\n",
      "epoch: 5 trial 2353 training loss: 0.03338304813951254\n",
      "epoch: 5 trial 2354 training loss: 0.06214213743805885\n",
      "epoch: 5 trial 2355 training loss: 0.02153382310643792\n",
      "epoch: 5 trial 2356 training loss: 0.003607618506066501\n",
      "epoch: 5 trial 2357 training loss: 0.023062512278556824\n",
      "epoch: 5 trial 2358 training loss: 0.02637072652578354\n",
      "epoch: 5 trial 2359 training loss: 0.055477455258369446\n",
      "epoch: 5 trial 2360 training loss: 0.03996395692229271\n",
      "epoch: 5 trial 2361 training loss: 0.013236882630735636\n",
      "epoch: 5 trial 2362 training loss: 0.05108716245740652\n",
      "epoch: 5 trial 2363 training loss: 0.03446479421108961\n",
      "epoch: 5 trial 2364 training loss: 0.012771593406796455\n",
      "epoch: 5 trial 2365 training loss: 0.019838882610201836\n",
      "epoch: 5 trial 2366 training loss: 0.01267215283587575\n",
      "epoch: 5 trial 2367 training loss: 0.019303567707538605\n",
      "epoch: 5 trial 2368 training loss: 0.037942422553896904\n",
      "epoch: 5 trial 2369 training loss: 0.031036293134093285\n",
      "epoch: 5 trial 2370 training loss: 0.02441844344139099\n",
      "epoch: 5 trial 2371 training loss: 0.017525232397019863\n",
      "epoch: 5 trial 2372 training loss: 0.01684523932635784\n",
      "epoch: 5 trial 2373 training loss: 0.010700495913624763\n",
      "epoch: 5 trial 2374 training loss: 0.016640519257634878\n",
      "epoch: 5 trial 2375 training loss: 0.025339697487652302\n",
      "epoch: 5 trial 2376 training loss: 0.01767515204846859\n",
      "epoch: 5 trial 2377 training loss: 0.008163557853549719\n",
      "epoch: 5 trial 2378 training loss: 0.015189524041488767\n",
      "epoch: 5 trial 2379 training loss: 0.0986374132335186\n",
      "epoch: 5 trial 2380 training loss: 0.01487306458875537\n",
      "epoch: 5 trial 2381 training loss: 0.03318262845277786\n",
      "epoch: 5 trial 2382 training loss: 0.019279571250081062\n",
      "epoch: 5 trial 2383 training loss: 0.046015460044145584\n",
      "epoch: 5 trial 2384 training loss: 0.037886871956288815\n",
      "epoch: 5 trial 2385 training loss: 0.04111935663968325\n",
      "epoch: 5 trial 2386 training loss: 0.02731355745345354\n",
      "epoch: 5 trial 2387 training loss: 0.01574872527271509\n",
      "epoch: 5 trial 2388 training loss: 0.11861096322536469\n",
      "epoch: 5 trial 2389 training loss: 0.005513636744581163\n",
      "epoch: 5 trial 2390 training loss: 0.019902270287275314\n",
      "epoch: 5 trial 2391 training loss: 0.13190685212612152\n",
      "epoch: 5 trial 2392 training loss: 0.029132590629160404\n",
      "epoch: 5 trial 2393 training loss: 0.023655490949749947\n",
      "epoch: 5 trial 2394 training loss: 0.019094502553343773\n",
      "epoch: 5 trial 2395 training loss: 0.013105087913572788\n",
      "epoch: 5 trial 2396 training loss: 0.023435945622622967\n",
      "epoch: 5 trial 2397 training loss: 0.014917098451405764\n",
      "epoch: 5 trial 2398 training loss: 0.04428363125771284\n",
      "epoch: 5 trial 2399 training loss: 0.16034314408898354\n",
      "epoch: 5 trial 2400 training loss: 0.06543401814997196\n",
      "epoch: 5 trial 2401 training loss: 0.025202674325555563\n",
      "epoch: 5 trial 2402 training loss: 0.11061912402510643\n",
      "epoch: 5 trial 2403 training loss: 0.11610325053334236\n",
      "epoch: 5 trial 2404 training loss: 0.02419043332338333\n",
      "epoch: 5 trial 2405 training loss: 0.07383386325091124\n",
      "epoch: 5 trial 2406 training loss: 0.006180173950269818\n",
      "epoch: 5 trial 2407 training loss: 0.063367435708642\n",
      "epoch: 5 trial 2408 training loss: 0.015298692043870687\n",
      "epoch: 5 trial 2409 training loss: 0.029163812287151814\n",
      "epoch: 5 trial 2410 training loss: 0.10785626992583275\n",
      "epoch: 5 trial 2411 training loss: 0.03808274492621422\n",
      "epoch: 5 trial 2412 training loss: 0.014877745416015387\n",
      "epoch: 5 trial 2413 training loss: 0.026241649873554707\n",
      "epoch: 5 trial 2414 training loss: 0.08670581132173538\n",
      "epoch: 5 trial 2415 training loss: 0.0137299713678658\n",
      "epoch: 5 trial 2416 training loss: 0.03992810659110546\n",
      "epoch: 5 trial 2417 training loss: 0.03487496357411146\n",
      "epoch: 5 trial 2418 training loss: 0.01695925323292613\n",
      "epoch: 5 trial 2419 training loss: 0.011284119915217161\n",
      "epoch: 5 trial 2420 training loss: 0.059033061377704144\n",
      "epoch: 6 trial 2421 training loss: 0.0062459774781018496\n",
      "epoch: 6 trial 2422 training loss: 0.01336441608145833\n",
      "epoch: 6 trial 2423 training loss: 0.020101859234273434\n",
      "epoch: 6 trial 2424 training loss: 0.06012116465717554\n",
      "epoch: 6 trial 2425 training loss: 0.004944314248859882\n",
      "epoch: 6 trial 2426 training loss: 0.005675860680639744\n",
      "epoch: 6 trial 2427 training loss: 0.03042274061590433\n",
      "epoch: 6 trial 2428 training loss: 0.02357871737331152\n",
      "epoch: 6 trial 2429 training loss: 0.008863818133249879\n",
      "epoch: 6 trial 2430 training loss: 0.019147610291838646\n",
      "epoch: 6 trial 2431 training loss: 0.057002872228622437\n",
      "epoch: 6 trial 2432 training loss: 0.02713203802704811\n",
      "epoch: 6 trial 2433 training loss: 0.02527580503374338\n",
      "epoch: 6 trial 2434 training loss: 0.05116214230656624\n",
      "epoch: 6 trial 2435 training loss: 0.013658334501087666\n",
      "epoch: 6 trial 2436 training loss: 0.023519976995885372\n",
      "epoch: 6 trial 2437 training loss: 0.11979294754564762\n",
      "epoch: 6 trial 2438 training loss: 0.05473794415593147\n",
      "epoch: 6 trial 2439 training loss: 0.011783614056184888\n",
      "epoch: 6 trial 2440 training loss: 0.017171379644423723\n",
      "epoch: 6 trial 2441 training loss: 0.015670565888285637\n",
      "epoch: 6 trial 2442 training loss: 0.08053424395620823\n",
      "epoch: 6 trial 2443 training loss: 0.011042403755709529\n",
      "epoch: 6 trial 2444 training loss: 0.005357431131415069\n",
      "epoch: 6 trial 2445 training loss: 0.14070518873631954\n",
      "epoch: 6 trial 2446 training loss: 0.022888288367539644\n",
      "epoch: 6 trial 2447 training loss: 0.012014432344585657\n",
      "epoch: 6 trial 2448 training loss: 0.11891690269112587\n",
      "epoch: 6 trial 2449 training loss: 0.021846062038093805\n",
      "epoch: 6 trial 2450 training loss: 0.024591705296188593\n",
      "epoch: 6 trial 2451 training loss: 0.04322270769625902\n",
      "epoch: 6 trial 2452 training loss: 0.014046694152057171\n",
      "epoch: 6 trial 2453 training loss: 0.029900583904236555\n",
      "epoch: 6 trial 2454 training loss: 0.04075767006725073\n",
      "epoch: 6 trial 2455 training loss: 0.021602485794574022\n",
      "epoch: 6 trial 2456 training loss: 0.059860216453671455\n",
      "epoch: 6 trial 2457 training loss: 0.15643349662423134\n",
      "epoch: 6 trial 2458 training loss: 0.3377193287014961\n",
      "epoch: 6 trial 2459 training loss: 0.04009403567761183\n",
      "epoch: 6 trial 2460 training loss: 0.110528364777565\n",
      "epoch: 6 trial 2461 training loss: 0.1314271241426468\n",
      "epoch: 6 trial 2462 training loss: 0.011732148006558418\n",
      "epoch: 6 trial 2463 training loss: 0.10868344455957413\n",
      "epoch: 6 trial 2464 training loss: 0.04413160588592291\n",
      "epoch: 6 trial 2465 training loss: 0.031244256533682346\n",
      "epoch: 6 trial 2466 training loss: 0.053872665390372276\n",
      "epoch: 6 trial 2467 training loss: 0.020066668279469013\n",
      "epoch: 6 trial 2468 training loss: 0.014730833005160093\n",
      "epoch: 6 trial 2469 training loss: 0.011390130035579205\n",
      "epoch: 6 trial 2470 training loss: 0.08300863020122051\n",
      "epoch: 6 trial 2471 training loss: 0.0396391786634922\n",
      "epoch: 6 trial 2472 training loss: 0.10701712220907211\n",
      "epoch: 6 trial 2473 training loss: 0.047605281695723534\n",
      "epoch: 6 trial 2474 training loss: 0.04691567085683346\n",
      "epoch: 6 trial 2475 training loss: 0.030028270557522774\n",
      "epoch: 6 trial 2476 training loss: 0.06963225267827511\n",
      "epoch: 6 trial 2477 training loss: 0.023620434571057558\n",
      "epoch: 6 trial 2478 training loss: 0.020733166486024857\n",
      "epoch: 6 trial 2479 training loss: 0.025774468202143908\n",
      "epoch: 6 trial 2480 training loss: 0.07370462827384472\n",
      "epoch: 6 trial 2481 training loss: 0.024665778502821922\n",
      "epoch: 6 trial 2482 training loss: 0.12664325907826424\n",
      "epoch: 6 trial 2483 training loss: 0.0194990960881114\n",
      "epoch: 6 trial 2484 training loss: 0.016854710644111037\n",
      "epoch: 6 trial 2485 training loss: 0.03434156719595194\n",
      "epoch: 6 trial 2486 training loss: 0.05473775789141655\n",
      "epoch: 6 trial 2487 training loss: 0.021565961185842752\n",
      "epoch: 6 trial 2488 training loss: 0.02270083874464035\n",
      "epoch: 6 trial 2489 training loss: 0.05706816166639328\n",
      "epoch: 6 trial 2490 training loss: 0.02990469429641962\n",
      "epoch: 6 trial 2491 training loss: 0.04574101883918047\n",
      "epoch: 6 trial 2492 training loss: 0.03353061806410551\n",
      "epoch: 6 trial 2493 training loss: 0.02104256208986044\n",
      "epoch: 6 trial 2494 training loss: 0.0678890123963356\n",
      "epoch: 6 trial 2495 training loss: 0.012720419792458415\n",
      "epoch: 6 trial 2496 training loss: 0.023840594105422497\n",
      "epoch: 6 trial 2497 training loss: 0.029106509871780872\n",
      "epoch: 6 trial 2498 training loss: 0.09332151897251606\n",
      "epoch: 6 trial 2499 training loss: 0.02337603783234954\n",
      "epoch: 6 trial 2500 training loss: 0.05863705649971962\n",
      "epoch: 6 trial 2501 training loss: 0.027702206280082464\n",
      "epoch: 6 trial 2502 training loss: 0.046288393437862396\n",
      "epoch: 6 trial 2503 training loss: 0.07227128185331821\n",
      "epoch: 6 trial 2504 training loss: 0.018765511456876993\n",
      "epoch: 6 trial 2505 training loss: 0.07156771421432495\n",
      "epoch: 6 trial 2506 training loss: 0.013327181804925203\n",
      "epoch: 6 trial 2507 training loss: 0.03571801818907261\n",
      "epoch: 6 trial 2508 training loss: 0.06041785981506109\n",
      "epoch: 6 trial 2509 training loss: 0.05372919514775276\n",
      "epoch: 6 trial 2510 training loss: 0.006804302101954818\n",
      "epoch: 6 trial 2511 training loss: 0.03629950433969498\n",
      "epoch: 6 trial 2512 training loss: 0.02448688680306077\n",
      "epoch: 6 trial 2513 training loss: 0.01744968630373478\n",
      "epoch: 6 trial 2514 training loss: 0.07599907740950584\n",
      "epoch: 6 trial 2515 training loss: 0.05174344778060913\n",
      "epoch: 6 trial 2516 training loss: 0.05287731625139713\n",
      "epoch: 6 trial 2517 training loss: 0.014893409796059132\n",
      "epoch: 6 trial 2518 training loss: 0.11002575978636742\n",
      "epoch: 6 trial 2519 training loss: 0.051669967360794544\n",
      "epoch: 6 trial 2520 training loss: 0.041982573457062244\n",
      "epoch: 6 trial 2521 training loss: 0.055670736357569695\n",
      "epoch: 6 trial 2522 training loss: 0.08050310239195824\n",
      "epoch: 6 trial 2523 training loss: 0.034918871242552996\n",
      "epoch: 6 trial 2524 training loss: 0.04168779868632555\n",
      "epoch: 6 trial 2525 training loss: 0.26357533782720566\n",
      "epoch: 6 trial 2526 training loss: 0.06331947166472673\n",
      "epoch: 6 trial 2527 training loss: 0.042413716204464436\n",
      "epoch: 6 trial 2528 training loss: 0.1452898308634758\n",
      "epoch: 6 trial 2529 training loss: 0.1286458745598793\n",
      "epoch: 6 trial 2530 training loss: 0.01888084225356579\n",
      "epoch: 6 trial 2531 training loss: 0.010494747199118137\n",
      "epoch: 6 trial 2532 training loss: 0.02885483205318451\n",
      "epoch: 6 trial 2533 training loss: 0.013721791794523597\n",
      "epoch: 6 trial 2534 training loss: 0.013899513520300388\n",
      "epoch: 6 trial 2535 training loss: 0.05206643231213093\n",
      "epoch: 6 trial 2536 training loss: 0.02002111356705427\n",
      "epoch: 6 trial 2537 training loss: 0.02918909676373005\n",
      "epoch: 6 trial 2538 training loss: 0.03861695248633623\n",
      "epoch: 6 trial 2539 training loss: 0.028416925109922886\n",
      "epoch: 6 trial 2540 training loss: 0.021205639000982046\n",
      "epoch: 6 trial 2541 training loss: 0.011852727038785815\n",
      "epoch: 6 trial 2542 training loss: 0.055898480117321014\n",
      "epoch: 6 trial 2543 training loss: 0.08346940949559212\n",
      "epoch: 6 trial 2544 training loss: 0.03426746279001236\n",
      "epoch: 6 trial 2545 training loss: 0.01131672877818346\n",
      "epoch: 6 trial 2546 training loss: 0.048005327582359314\n",
      "epoch: 6 trial 2547 training loss: 0.021799540147185326\n",
      "epoch: 6 trial 2548 training loss: 0.025993863120675087\n",
      "epoch: 6 trial 2549 training loss: 0.1416669674217701\n",
      "epoch: 6 trial 2550 training loss: 0.03091825172305107\n",
      "epoch: 6 trial 2551 training loss: 0.10506640002131462\n",
      "epoch: 6 trial 2552 training loss: 0.03756342642009258\n",
      "epoch: 6 trial 2553 training loss: 0.03482894506305456\n",
      "epoch: 6 trial 2554 training loss: 0.0572986975312233\n",
      "epoch: 6 trial 2555 training loss: 0.014656814746558666\n",
      "epoch: 6 trial 2556 training loss: 0.038773790933191776\n",
      "epoch: 6 trial 2557 training loss: 0.01691584149375558\n",
      "epoch: 6 trial 2558 training loss: 0.12651323154568672\n",
      "epoch: 6 trial 2559 training loss: 0.021778395865112543\n",
      "epoch: 6 trial 2560 training loss: 0.05402879789471626\n",
      "epoch: 6 trial 2561 training loss: 0.045023027807474136\n",
      "epoch: 6 trial 2562 training loss: 0.030846888199448586\n",
      "epoch: 6 trial 2563 training loss: 0.03743981383740902\n",
      "epoch: 6 trial 2564 training loss: 0.05046511068940163\n",
      "epoch: 6 trial 2565 training loss: 0.045836811885237694\n",
      "epoch: 6 trial 2566 training loss: 0.02555560413748026\n",
      "epoch: 6 trial 2567 training loss: 0.09125915914773941\n",
      "epoch: 6 trial 2568 training loss: 0.015468591824173927\n",
      "epoch: 6 trial 2569 training loss: 0.05208836402744055\n",
      "epoch: 6 trial 2570 training loss: 0.06865881569683552\n",
      "epoch: 6 trial 2571 training loss: 0.029373202938586473\n",
      "epoch: 6 trial 2572 training loss: 0.0466745151206851\n",
      "epoch: 6 trial 2573 training loss: 0.01890512276440859\n",
      "epoch: 6 trial 2574 training loss: 0.04445835202932358\n",
      "epoch: 6 trial 2575 training loss: 0.011268420843407512\n",
      "epoch: 6 trial 2576 training loss: 0.009479963220655918\n",
      "epoch: 6 trial 2577 training loss: 0.01582043571397662\n",
      "epoch: 6 trial 2578 training loss: 0.004577770130708814\n",
      "epoch: 6 trial 2579 training loss: 0.016285530757158995\n",
      "epoch: 6 trial 2580 training loss: 0.014388895593583584\n",
      "epoch: 6 trial 2581 training loss: 0.008110647089779377\n",
      "epoch: 6 trial 2582 training loss: 0.0663404893130064\n",
      "epoch: 6 trial 2583 training loss: 0.003246042993851006\n",
      "epoch: 6 trial 2584 training loss: 0.0536746010184288\n",
      "epoch: 6 trial 2585 training loss: 0.013187768869102001\n",
      "epoch: 6 trial 2586 training loss: 0.02669622004032135\n",
      "epoch: 6 trial 2587 training loss: 0.027650373987853527\n",
      "epoch: 6 trial 2588 training loss: 0.012174312956631184\n",
      "epoch: 6 trial 2589 training loss: 0.031338979955762625\n",
      "epoch: 6 trial 2590 training loss: 0.14324437081813812\n",
      "epoch: 6 trial 2591 training loss: 0.04796486347913742\n",
      "epoch: 6 trial 2592 training loss: 0.008704108884558082\n",
      "epoch: 6 trial 2593 training loss: 0.05602259747684002\n",
      "epoch: 6 trial 2594 training loss: 0.03482844866812229\n",
      "epoch: 6 trial 2595 training loss: 0.05127560719847679\n",
      "epoch: 6 trial 2596 training loss: 0.03273322433233261\n",
      "epoch: 6 trial 2597 training loss: 0.010748976841568947\n",
      "epoch: 6 trial 2598 training loss: 0.035271442495286465\n",
      "epoch: 6 trial 2599 training loss: 0.042632958851754665\n",
      "epoch: 6 trial 2600 training loss: 0.04648539423942566\n",
      "epoch: 6 trial 2601 training loss: 0.03689124155789614\n",
      "epoch: 6 trial 2602 training loss: 0.017281191889196634\n",
      "epoch: 6 trial 2603 training loss: 0.03928190656006336\n",
      "epoch: 6 trial 2604 training loss: 0.020388645119965076\n",
      "epoch: 6 trial 2605 training loss: 0.02194921439513564\n",
      "epoch: 6 trial 2606 training loss: 0.08856819942593575\n",
      "epoch: 6 trial 2607 training loss: 0.057718973606824875\n",
      "epoch: 6 trial 2608 training loss: 0.02883464377373457\n",
      "epoch: 6 trial 2609 training loss: 0.06784568913280964\n",
      "epoch: 6 trial 2610 training loss: 0.040725101716816425\n",
      "epoch: 6 trial 2611 training loss: 0.08600694872438908\n",
      "epoch: 6 trial 2612 training loss: 0.0744918417185545\n",
      "epoch: 6 trial 2613 training loss: 0.04494362697005272\n",
      "epoch: 6 trial 2614 training loss: 0.019633390475064516\n",
      "epoch: 6 trial 2615 training loss: 0.034081882797181606\n",
      "epoch: 6 trial 2616 training loss: 0.024618607945740223\n",
      "epoch: 6 trial 2617 training loss: 0.021631908603012562\n",
      "epoch: 6 trial 2618 training loss: 0.0473009361885488\n",
      "epoch: 6 trial 2619 training loss: 0.09417419321835041\n",
      "epoch: 6 trial 2620 training loss: 0.01311886147595942\n",
      "epoch: 6 trial 2621 training loss: 0.006223776028491557\n",
      "epoch: 6 trial 2622 training loss: 0.009515271056443453\n",
      "epoch: 6 trial 2623 training loss: 0.008825038094073534\n",
      "epoch: 6 trial 2624 training loss: 0.023387661203742027\n",
      "epoch: 6 trial 2625 training loss: 0.012972605880349874\n",
      "epoch: 6 trial 2626 training loss: 0.036564611829817295\n",
      "epoch: 6 trial 2627 training loss: 0.03495587036013603\n",
      "epoch: 6 trial 2628 training loss: 0.011365959653630853\n",
      "epoch: 6 trial 2629 training loss: 0.08461430668830872\n",
      "epoch: 6 trial 2630 training loss: 0.044367811642587185\n",
      "epoch: 6 trial 2631 training loss: 0.06125435419380665\n",
      "epoch: 6 trial 2632 training loss: 0.06011980213224888\n",
      "epoch: 6 trial 2633 training loss: 0.15393182635307312\n",
      "epoch: 6 trial 2634 training loss: 0.012759686214849353\n",
      "epoch: 6 trial 2635 training loss: 0.020652147475630045\n",
      "epoch: 6 trial 2636 training loss: 0.020649719517678022\n",
      "epoch: 6 trial 2637 training loss: 0.018915091175585985\n",
      "epoch: 6 trial 2638 training loss: 0.01714432891458273\n",
      "epoch: 6 trial 2639 training loss: 0.045710221864283085\n",
      "epoch: 6 trial 2640 training loss: 0.05468108877539635\n",
      "epoch: 6 trial 2641 training loss: 0.0387389725074172\n",
      "epoch: 6 trial 2642 training loss: 0.06954825296998024\n",
      "epoch: 6 trial 2643 training loss: 0.048554737120866776\n",
      "epoch: 6 trial 2644 training loss: 0.07479738630354404\n",
      "epoch: 6 trial 2645 training loss: 0.01011634897440672\n",
      "epoch: 6 trial 2646 training loss: 0.015882285311818123\n",
      "epoch: 6 trial 2647 training loss: 0.030606629326939583\n",
      "epoch: 6 trial 2648 training loss: 0.014287904603406787\n",
      "epoch: 6 trial 2649 training loss: 0.049500773660838604\n",
      "epoch: 6 trial 2650 training loss: 0.014522732933983207\n",
      "epoch: 6 trial 2651 training loss: 0.15680133551359177\n",
      "epoch: 6 trial 2652 training loss: 0.047633430920541286\n",
      "epoch: 6 trial 2653 training loss: 0.1433148980140686\n",
      "epoch: 6 trial 2654 training loss: 0.09609423391520977\n",
      "epoch: 6 trial 2655 training loss: 0.019651567563414574\n",
      "epoch: 6 trial 2656 training loss: 0.044769867323338985\n",
      "epoch: 6 trial 2657 training loss: 0.02577644446864724\n",
      "epoch: 6 trial 2658 training loss: 0.044001881033182144\n",
      "epoch: 6 trial 2659 training loss: 0.04568858537822962\n",
      "epoch: 6 trial 2660 training loss: 0.10576219670474529\n",
      "epoch: 6 trial 2661 training loss: 0.028184978291392326\n",
      "epoch: 6 trial 2662 training loss: 0.033341423608362675\n",
      "epoch: 6 trial 2663 training loss: 0.05469244532287121\n",
      "epoch: 6 trial 2664 training loss: 0.004812904633581638\n",
      "epoch: 6 trial 2665 training loss: 0.058744109235703945\n",
      "epoch: 6 trial 2666 training loss: 0.017774165142327547\n",
      "epoch: 6 trial 2667 training loss: 0.04029527027159929\n",
      "epoch: 6 trial 2668 training loss: 0.028636552393436432\n",
      "epoch: 6 trial 2669 training loss: 0.029268354177474976\n",
      "epoch: 6 trial 2670 training loss: 0.06554063968360424\n",
      "epoch: 6 trial 2671 training loss: 0.013556126737967134\n",
      "epoch: 6 trial 2672 training loss: 0.006178113864734769\n",
      "epoch: 6 trial 2673 training loss: 0.0275729950517416\n",
      "epoch: 6 trial 2674 training loss: 0.08283980935811996\n",
      "epoch: 6 trial 2675 training loss: 0.02108718641102314\n",
      "epoch: 6 trial 2676 training loss: 0.07671496737748384\n",
      "epoch: 6 trial 2677 training loss: 0.030046436935663223\n",
      "epoch: 6 trial 2678 training loss: 0.019181343261152506\n",
      "epoch: 6 trial 2679 training loss: 0.1621486283838749\n",
      "epoch: 6 trial 2680 training loss: 0.012643170775845647\n",
      "epoch: 6 trial 2681 training loss: 0.015274802688509226\n",
      "epoch: 6 trial 2682 training loss: 0.008464685874059796\n",
      "epoch: 6 trial 2683 training loss: 0.004584681359119713\n",
      "epoch: 6 trial 2684 training loss: 0.01758401934057474\n",
      "epoch: 6 trial 2685 training loss: 0.041809892281889915\n",
      "epoch: 6 trial 2686 training loss: 0.03567946795374155\n",
      "epoch: 6 trial 2687 training loss: 0.03828472550958395\n",
      "epoch: 6 trial 2688 training loss: 0.03508504293859005\n",
      "epoch: 6 trial 2689 training loss: 0.02850674744695425\n",
      "epoch: 6 trial 2690 training loss: 0.07835365273058414\n",
      "epoch: 6 trial 2691 training loss: 0.01546822814270854\n",
      "epoch: 6 trial 2692 training loss: 0.011374500812962651\n",
      "epoch: 6 trial 2693 training loss: 0.023790954146534204\n",
      "epoch: 6 trial 2694 training loss: 0.00566403241828084\n",
      "epoch: 6 trial 2695 training loss: 0.0741417445242405\n",
      "epoch: 6 trial 2696 training loss: 0.005039618001319468\n",
      "epoch: 6 trial 2697 training loss: 0.020594452507793903\n",
      "epoch: 6 trial 2698 training loss: 0.012920104898512363\n",
      "epoch: 6 trial 2699 training loss: 0.06508577801287174\n",
      "epoch: 6 trial 2700 training loss: 0.018676468171179295\n",
      "epoch: 6 trial 2701 training loss: 0.017367130611091852\n",
      "epoch: 6 trial 2702 training loss: 0.05272812955081463\n",
      "epoch: 6 trial 2703 training loss: 0.09330590814352036\n",
      "epoch: 6 trial 2704 training loss: 0.02805868536233902\n",
      "epoch: 6 trial 2705 training loss: 0.08641814067959785\n",
      "epoch: 6 trial 2706 training loss: 0.02612937893718481\n",
      "epoch: 6 trial 2707 training loss: 0.08154479786753654\n",
      "epoch: 6 trial 2708 training loss: 0.01904975576326251\n",
      "epoch: 6 trial 2709 training loss: 0.018460935913026333\n",
      "epoch: 6 trial 2710 training loss: 0.057114576920866966\n",
      "epoch: 6 trial 2711 training loss: 0.028971563559025526\n",
      "epoch: 6 trial 2712 training loss: 0.017180539667606354\n",
      "epoch: 6 trial 2713 training loss: 0.016079175751656294\n",
      "epoch: 6 trial 2714 training loss: 0.008964196546003222\n",
      "epoch: 6 trial 2715 training loss: 0.01760741462931037\n",
      "epoch: 6 trial 2716 training loss: 0.025569770485162735\n",
      "epoch: 6 trial 2717 training loss: 0.01926139136776328\n",
      "epoch: 6 trial 2718 training loss: 0.03333092574030161\n",
      "epoch: 6 trial 2719 training loss: 0.05170312896370888\n",
      "epoch: 6 trial 2720 training loss: 0.02756370697170496\n",
      "epoch: 6 trial 2721 training loss: 0.01901513198390603\n",
      "epoch: 6 trial 2722 training loss: 0.007649949984624982\n",
      "epoch: 6 trial 2723 training loss: 0.01460824441164732\n",
      "epoch: 6 trial 2724 training loss: 0.030966300517320633\n",
      "epoch: 6 trial 2725 training loss: 0.055423665791749954\n",
      "epoch: 6 trial 2726 training loss: 0.05129620339721441\n",
      "epoch: 6 trial 2727 training loss: 0.0023891026503406465\n",
      "epoch: 6 trial 2728 training loss: 0.03121574129909277\n",
      "epoch: 6 trial 2729 training loss: 0.04222479090094566\n",
      "epoch: 6 trial 2730 training loss: 0.01716533163562417\n",
      "epoch: 6 trial 2731 training loss: 0.013263550121337175\n",
      "epoch: 6 trial 2732 training loss: 0.01811792515218258\n",
      "epoch: 6 trial 2733 training loss: 0.043948124162852764\n",
      "epoch: 6 trial 2734 training loss: 0.02218745881691575\n",
      "epoch: 6 trial 2735 training loss: 0.032839853316545486\n",
      "epoch: 6 trial 2736 training loss: 0.05894262343645096\n",
      "epoch: 6 trial 2737 training loss: 0.026718728244304657\n",
      "epoch: 6 trial 2738 training loss: 0.03450149204581976\n",
      "epoch: 6 trial 2739 training loss: 0.06819940730929375\n",
      "epoch: 6 trial 2740 training loss: 0.03195095807313919\n",
      "epoch: 6 trial 2741 training loss: 0.06546304188668728\n",
      "epoch: 6 trial 2742 training loss: 0.01770471641793847\n",
      "epoch: 6 trial 2743 training loss: 0.02982193697243929\n",
      "epoch: 6 trial 2744 training loss: 0.026192158460617065\n",
      "epoch: 6 trial 2745 training loss: 0.022802465595304966\n",
      "epoch: 6 trial 2746 training loss: 0.015688211424276233\n",
      "epoch: 6 trial 2747 training loss: 0.08359829150140285\n",
      "epoch: 6 trial 2748 training loss: 0.013726440258324146\n",
      "epoch: 6 trial 2749 training loss: 0.08621781133115292\n",
      "epoch: 6 trial 2750 training loss: 0.027136427350342274\n",
      "epoch: 6 trial 2751 training loss: 0.010658748215064406\n",
      "epoch: 6 trial 2752 training loss: 0.06324691139161587\n",
      "epoch: 6 trial 2753 training loss: 0.053168147802352905\n",
      "epoch: 6 trial 2754 training loss: 0.009774692822247744\n",
      "epoch: 6 trial 2755 training loss: 0.03197901975363493\n",
      "epoch: 6 trial 2756 training loss: 0.009322319412603974\n",
      "epoch: 6 trial 2757 training loss: 0.03190638544037938\n",
      "epoch: 6 trial 2758 training loss: 0.019090726971626282\n",
      "epoch: 6 trial 2759 training loss: 0.030420479364693165\n",
      "epoch: 6 trial 2760 training loss: 0.2587061822414398\n",
      "epoch: 6 trial 2761 training loss: 0.04671607073396444\n",
      "epoch: 6 trial 2762 training loss: 0.010706316214054823\n",
      "epoch: 6 trial 2763 training loss: 0.021490133833140135\n",
      "epoch: 6 trial 2764 training loss: 0.1520386040210724\n",
      "epoch: 6 trial 2765 training loss: 0.051959154196083546\n",
      "epoch: 6 trial 2766 training loss: 0.05873504187911749\n",
      "epoch: 6 trial 2767 training loss: 0.023748852778226137\n",
      "epoch: 6 trial 2768 training loss: 0.03758977074176073\n",
      "epoch: 6 trial 2769 training loss: 0.017818493768572807\n",
      "epoch: 6 trial 2770 training loss: 0.10080157779157162\n",
      "epoch: 6 trial 2771 training loss: 0.013295534532517195\n",
      "epoch: 6 trial 2772 training loss: 0.07296852581202984\n",
      "epoch: 6 trial 2773 training loss: 0.027718395926058292\n",
      "epoch: 6 trial 2774 training loss: 0.014779901830479503\n",
      "epoch: 6 trial 2775 training loss: 0.051599797792732716\n",
      "epoch: 6 trial 2776 training loss: 0.011020013596862555\n",
      "epoch: 6 trial 2777 training loss: 0.03178352024406195\n",
      "epoch: 6 trial 2778 training loss: 0.10116428881883621\n",
      "epoch: 6 trial 2779 training loss: 0.048169796355068684\n",
      "epoch: 6 trial 2780 training loss: 0.03013189509510994\n",
      "epoch: 6 trial 2781 training loss: 0.10291697829961777\n",
      "epoch: 6 trial 2782 training loss: 0.08572917059063911\n",
      "epoch: 6 trial 2783 training loss: 0.04495770204812288\n",
      "epoch: 6 trial 2784 training loss: 0.0786239318549633\n",
      "epoch: 6 trial 2785 training loss: 0.04540758207440376\n",
      "epoch: 6 trial 2786 training loss: 0.018477415665984154\n",
      "epoch: 6 trial 2787 training loss: 0.006298241205513477\n",
      "epoch: 6 trial 2788 training loss: 0.17966626957058907\n",
      "epoch: 6 trial 2789 training loss: 0.07089587673544884\n",
      "epoch: 6 trial 2790 training loss: 0.04409313201904297\n",
      "epoch: 6 trial 2791 training loss: 0.05337140895426273\n",
      "epoch: 6 trial 2792 training loss: 0.025958848651498556\n",
      "epoch: 6 trial 2793 training loss: 0.03267039451748133\n",
      "epoch: 6 trial 2794 training loss: 0.02051604725420475\n",
      "epoch: 6 trial 2795 training loss: 0.04863371513783932\n",
      "epoch: 6 trial 2796 training loss: 0.03606120031327009\n",
      "epoch: 6 trial 2797 training loss: 0.045387099497020245\n",
      "epoch: 6 trial 2798 training loss: 0.07729821838438511\n",
      "epoch: 6 trial 2799 training loss: 0.036276151426136494\n",
      "epoch: 6 trial 2800 training loss: 0.014262824784964323\n",
      "epoch: 6 trial 2801 training loss: 0.030558716040104628\n",
      "epoch: 6 trial 2802 training loss: 0.007766014663502574\n",
      "epoch: 6 trial 2803 training loss: 0.04896169248968363\n",
      "epoch: 6 trial 2804 training loss: 0.0381426140666008\n",
      "epoch: 6 trial 2805 training loss: 0.01041826931759715\n",
      "epoch: 6 trial 2806 training loss: 0.0026011489680968225\n",
      "epoch: 6 trial 2807 training loss: 0.011525922222062945\n",
      "epoch: 6 trial 2808 training loss: 0.01686056051403284\n",
      "epoch: 6 trial 2809 training loss: 0.028078182600438595\n",
      "epoch: 6 trial 2810 training loss: 0.006255903514102101\n",
      "epoch: 6 trial 2811 training loss: 0.003211038827430457\n",
      "epoch: 6 trial 2812 training loss: 0.004506844910793006\n",
      "epoch: 6 trial 2813 training loss: 0.008712570182979107\n",
      "epoch: 6 trial 2814 training loss: 0.013018736615777016\n",
      "epoch: 6 trial 2815 training loss: 0.027963293716311455\n",
      "epoch: 6 trial 2816 training loss: 0.10478064604103565\n",
      "epoch: 6 trial 2817 training loss: 0.02694429038092494\n",
      "epoch: 6 trial 2818 training loss: 0.022894755471497774\n",
      "epoch: 6 trial 2819 training loss: 0.032973610796034336\n",
      "epoch: 6 trial 2820 training loss: 0.03971869871020317\n",
      "epoch: 6 trial 2821 training loss: 0.026375372894108295\n",
      "epoch: 6 trial 2822 training loss: 0.02518792962655425\n",
      "epoch: 6 trial 2823 training loss: 0.054981738328933716\n",
      "epoch: 6 trial 2824 training loss: 0.13728032261133194\n",
      "epoch: 6 trial 2825 training loss: 0.13008074089884758\n",
      "epoch: 6 trial 2826 training loss: 0.027699030470103025\n",
      "epoch: 6 trial 2827 training loss: 0.05138333886861801\n",
      "epoch: 6 trial 2828 training loss: 0.054319703951478004\n",
      "epoch: 6 trial 2829 training loss: 0.030457228422164917\n",
      "epoch: 6 trial 2830 training loss: 0.08503863774240017\n",
      "epoch: 6 trial 2831 training loss: 0.11227012798190117\n",
      "epoch: 6 trial 2832 training loss: 0.07513213157653809\n",
      "epoch: 6 trial 2833 training loss: 0.02307714056223631\n",
      "epoch: 6 trial 2834 training loss: 0.0810956060886383\n",
      "epoch: 6 trial 2835 training loss: 0.03657589480280876\n",
      "epoch: 6 trial 2836 training loss: 0.07261375710368156\n",
      "epoch: 6 trial 2837 training loss: 0.030822099186480045\n",
      "epoch: 6 trial 2838 training loss: 0.0613599494099617\n",
      "epoch: 6 trial 2839 training loss: 0.019690697081387043\n",
      "epoch: 6 trial 2840 training loss: 0.003732614452019334\n",
      "epoch: 6 trial 2841 training loss: 0.022484085988253355\n",
      "epoch: 6 trial 2842 training loss: 0.024983673356473446\n",
      "epoch: 6 trial 2843 training loss: 0.05115120951086283\n",
      "epoch: 6 trial 2844 training loss: 0.03946151863783598\n",
      "epoch: 6 trial 2845 training loss: 0.01344875362701714\n",
      "epoch: 6 trial 2846 training loss: 0.05118538253009319\n",
      "epoch: 6 trial 2847 training loss: 0.03439013101160526\n",
      "epoch: 6 trial 2848 training loss: 0.013305473607033491\n",
      "epoch: 6 trial 2849 training loss: 0.020549375098198652\n",
      "epoch: 6 trial 2850 training loss: 0.012012132676318288\n",
      "epoch: 6 trial 2851 training loss: 0.016676450613886118\n",
      "epoch: 6 trial 2852 training loss: 0.035431353375315666\n",
      "epoch: 6 trial 2853 training loss: 0.03413264732807875\n",
      "epoch: 6 trial 2854 training loss: 0.02737328503280878\n",
      "epoch: 6 trial 2855 training loss: 0.018359966576099396\n",
      "epoch: 6 trial 2856 training loss: 0.016366264317184687\n",
      "epoch: 6 trial 2857 training loss: 0.010197811061516404\n",
      "epoch: 6 trial 2858 training loss: 0.016617546323686838\n",
      "epoch: 6 trial 2859 training loss: 0.02355705527588725\n",
      "epoch: 6 trial 2860 training loss: 0.01729693729430437\n",
      "epoch: 6 trial 2861 training loss: 0.00734325684607029\n",
      "epoch: 6 trial 2862 training loss: 0.017697451869025826\n",
      "epoch: 6 trial 2863 training loss: 0.1025688499212265\n",
      "epoch: 6 trial 2864 training loss: 0.014689585193991661\n",
      "epoch: 6 trial 2865 training loss: 0.02857805322855711\n",
      "epoch: 6 trial 2866 training loss: 0.017311844043433666\n",
      "epoch: 6 trial 2867 training loss: 0.04731674771755934\n",
      "epoch: 6 trial 2868 training loss: 0.03838563710451126\n",
      "epoch: 6 trial 2869 training loss: 0.03818263951689005\n",
      "epoch: 6 trial 2870 training loss: 0.02952905371785164\n",
      "epoch: 6 trial 2871 training loss: 0.017173179890960455\n",
      "epoch: 6 trial 2872 training loss: 0.11664948798716068\n",
      "epoch: 6 trial 2873 training loss: 0.005774151999503374\n",
      "epoch: 6 trial 2874 training loss: 0.019842038862407207\n",
      "epoch: 6 trial 2875 training loss: 0.1304001323878765\n",
      "epoch: 6 trial 2876 training loss: 0.028251145035028458\n",
      "epoch: 6 trial 2877 training loss: 0.023213425651192665\n",
      "epoch: 6 trial 2878 training loss: 0.016135720536112785\n",
      "epoch: 6 trial 2879 training loss: 0.014240785501897335\n",
      "epoch: 6 trial 2880 training loss: 0.024587144143879414\n",
      "epoch: 6 trial 2881 training loss: 0.015144208911806345\n",
      "epoch: 6 trial 2882 training loss: 0.0420864662155509\n",
      "epoch: 6 trial 2883 training loss: 0.15290462598204613\n",
      "epoch: 6 trial 2884 training loss: 0.0619202321395278\n",
      "epoch: 6 trial 2885 training loss: 0.023047946859151125\n",
      "epoch: 6 trial 2886 training loss: 0.11874072253704071\n",
      "epoch: 6 trial 2887 training loss: 0.11944109946489334\n",
      "epoch: 6 trial 2888 training loss: 0.02554049761965871\n",
      "epoch: 6 trial 2889 training loss: 0.0740504888817668\n",
      "epoch: 6 trial 2890 training loss: 0.006669007008895278\n",
      "epoch: 6 trial 2891 training loss: 0.059847062453627586\n",
      "epoch: 6 trial 2892 training loss: 0.014803237281739712\n",
      "epoch: 6 trial 2893 training loss: 0.02748463535681367\n",
      "epoch: 6 trial 2894 training loss: 0.10829305276274681\n",
      "epoch: 6 trial 2895 training loss: 0.039441902190446854\n",
      "epoch: 6 trial 2896 training loss: 0.014572534244507551\n",
      "epoch: 6 trial 2897 training loss: 0.02831826452165842\n",
      "epoch: 6 trial 2898 training loss: 0.08946467004716396\n",
      "epoch: 6 trial 2899 training loss: 0.01208217116072774\n",
      "epoch: 6 trial 2900 training loss: 0.04531721584498882\n",
      "epoch: 6 trial 2901 training loss: 0.0330704627558589\n",
      "epoch: 6 trial 2902 training loss: 0.018190468661487103\n",
      "epoch: 6 trial 2903 training loss: 0.012877554865553975\n",
      "epoch: 6 trial 2904 training loss: 0.05617659166455269\n",
      "epoch: 7 trial 2905 training loss: 0.007036804687231779\n",
      "epoch: 7 trial 2906 training loss: 0.014203303027898073\n",
      "epoch: 7 trial 2907 training loss: 0.019806260708719492\n",
      "epoch: 7 trial 2908 training loss: 0.0636702012270689\n",
      "epoch: 7 trial 2909 training loss: 0.004910866264253855\n",
      "epoch: 7 trial 2910 training loss: 0.005219069309532642\n",
      "epoch: 7 trial 2911 training loss: 0.03129133582115173\n",
      "epoch: 7 trial 2912 training loss: 0.023616309743374586\n",
      "epoch: 7 trial 2913 training loss: 0.009177716681733727\n",
      "epoch: 7 trial 2914 training loss: 0.018119676038622856\n",
      "epoch: 7 trial 2915 training loss: 0.05464128777384758\n",
      "epoch: 7 trial 2916 training loss: 0.02846758719533682\n",
      "epoch: 7 trial 2917 training loss: 0.02418075641617179\n",
      "epoch: 7 trial 2918 training loss: 0.049871666356921196\n",
      "epoch: 7 trial 2919 training loss: 0.012659283122047782\n",
      "epoch: 7 trial 2920 training loss: 0.022840100806206465\n",
      "epoch: 7 trial 2921 training loss: 0.11419559642672539\n",
      "epoch: 7 trial 2922 training loss: 0.050964364781975746\n",
      "epoch: 7 trial 2923 training loss: 0.011247271671891212\n",
      "epoch: 7 trial 2924 training loss: 0.016123287845402956\n",
      "epoch: 7 trial 2925 training loss: 0.016128784511238337\n",
      "epoch: 7 trial 2926 training loss: 0.07957873120903969\n",
      "epoch: 7 trial 2927 training loss: 0.01125886500813067\n",
      "epoch: 7 trial 2928 training loss: 0.005998997250571847\n",
      "epoch: 7 trial 2929 training loss: 0.15698133036494255\n",
      "epoch: 7 trial 2930 training loss: 0.02363738138228655\n",
      "epoch: 7 trial 2931 training loss: 0.011897330870851874\n",
      "epoch: 7 trial 2932 training loss: 0.12081700190901756\n",
      "epoch: 7 trial 2933 training loss: 0.022579366341233253\n",
      "epoch: 7 trial 2934 training loss: 0.019092252478003502\n",
      "epoch: 7 trial 2935 training loss: 0.04131561331450939\n",
      "epoch: 7 trial 2936 training loss: 0.013977182563394308\n",
      "epoch: 7 trial 2937 training loss: 0.033524597994983196\n",
      "epoch: 7 trial 2938 training loss: 0.04107889626175165\n",
      "epoch: 7 trial 2939 training loss: 0.02624124474823475\n",
      "epoch: 7 trial 2940 training loss: 0.05998057499527931\n",
      "epoch: 7 trial 2941 training loss: 0.14289365336298943\n",
      "epoch: 7 trial 2942 training loss: 0.3274487778544426\n",
      "epoch: 7 trial 2943 training loss: 0.03722727601416409\n",
      "epoch: 7 trial 2944 training loss: 0.09934693016111851\n",
      "epoch: 7 trial 2945 training loss: 0.11789577081799507\n",
      "epoch: 7 trial 2946 training loss: 0.01347811403684318\n",
      "epoch: 7 trial 2947 training loss: 0.09880495443940163\n",
      "epoch: 7 trial 2948 training loss: 0.046261669136583805\n",
      "epoch: 7 trial 2949 training loss: 0.030546341091394424\n",
      "epoch: 7 trial 2950 training loss: 0.054905546829104424\n",
      "epoch: 7 trial 2951 training loss: 0.017948993481695652\n",
      "epoch: 7 trial 2952 training loss: 0.015313762938603759\n",
      "epoch: 7 trial 2953 training loss: 0.011705224867910147\n",
      "epoch: 7 trial 2954 training loss: 0.09178962931036949\n",
      "epoch: 7 trial 2955 training loss: 0.041677589528262615\n",
      "epoch: 7 trial 2956 training loss: 0.13255784288048744\n",
      "epoch: 7 trial 2957 training loss: 0.04272296093404293\n",
      "epoch: 7 trial 2958 training loss: 0.04425508249551058\n",
      "epoch: 7 trial 2959 training loss: 0.03165809158235788\n",
      "epoch: 7 trial 2960 training loss: 0.06523875892162323\n",
      "epoch: 7 trial 2961 training loss: 0.026380985975265503\n",
      "epoch: 7 trial 2962 training loss: 0.02253130730241537\n",
      "epoch: 7 trial 2963 training loss: 0.0214492860250175\n",
      "epoch: 7 trial 2964 training loss: 0.07572633773088455\n",
      "epoch: 7 trial 2965 training loss: 0.024048789869993925\n",
      "epoch: 7 trial 2966 training loss: 0.12341428361833096\n",
      "epoch: 7 trial 2967 training loss: 0.017363660037517548\n",
      "epoch: 7 trial 2968 training loss: 0.0173119290266186\n",
      "epoch: 7 trial 2969 training loss: 0.03273833263665438\n",
      "epoch: 7 trial 2970 training loss: 0.05478326231241226\n",
      "epoch: 7 trial 2971 training loss: 0.020575757138431072\n",
      "epoch: 7 trial 2972 training loss: 0.02668557781726122\n",
      "epoch: 7 trial 2973 training loss: 0.054035973735153675\n",
      "epoch: 7 trial 2974 training loss: 0.02993557695299387\n",
      "epoch: 7 trial 2975 training loss: 0.04438446741551161\n",
      "epoch: 7 trial 2976 training loss: 0.03922043740749359\n",
      "epoch: 7 trial 2977 training loss: 0.024294359143823385\n",
      "epoch: 7 trial 2978 training loss: 0.0681732539087534\n",
      "epoch: 7 trial 2979 training loss: 0.013721499592065811\n",
      "epoch: 7 trial 2980 training loss: 0.024105544202029705\n",
      "epoch: 7 trial 2981 training loss: 0.029557496309280396\n",
      "epoch: 7 trial 2982 training loss: 0.08791073597967625\n",
      "epoch: 7 trial 2983 training loss: 0.024031606502830982\n",
      "epoch: 7 trial 2984 training loss: 0.05540415830910206\n",
      "epoch: 7 trial 2985 training loss: 0.024346373043954372\n",
      "epoch: 7 trial 2986 training loss: 0.04303899686783552\n",
      "epoch: 7 trial 2987 training loss: 0.06362973712384701\n",
      "epoch: 7 trial 2988 training loss: 0.016757140401750803\n",
      "epoch: 7 trial 2989 training loss: 0.06643310561776161\n",
      "epoch: 7 trial 2990 training loss: 0.012797816656529903\n",
      "epoch: 7 trial 2991 training loss: 0.03924576658755541\n",
      "epoch: 7 trial 2992 training loss: 0.04534505680203438\n",
      "epoch: 7 trial 2993 training loss: 0.049622245132923126\n",
      "epoch: 7 trial 2994 training loss: 0.006444192957133055\n",
      "epoch: 7 trial 2995 training loss: 0.035340769216418266\n",
      "epoch: 7 trial 2996 training loss: 0.024657028261572123\n",
      "epoch: 7 trial 2997 training loss: 0.017777887172996998\n",
      "epoch: 7 trial 2998 training loss: 0.07252094708383083\n",
      "epoch: 7 trial 2999 training loss: 0.05045086517930031\n",
      "epoch: 7 trial 3000 training loss: 0.05311321746557951\n",
      "epoch: 7 trial 3001 training loss: 0.01583139831200242\n",
      "epoch: 7 trial 3002 training loss: 0.10643308609724045\n",
      "epoch: 7 trial 3003 training loss: 0.05019726324826479\n",
      "epoch: 7 trial 3004 training loss: 0.04260533954948187\n",
      "epoch: 7 trial 3005 training loss: 0.04812929406762123\n",
      "epoch: 7 trial 3006 training loss: 0.07044643722474575\n",
      "epoch: 7 trial 3007 training loss: 0.033038835041224957\n",
      "epoch: 7 trial 3008 training loss: 0.0388905880972743\n",
      "epoch: 7 trial 3009 training loss: 0.2512545511126518\n",
      "epoch: 7 trial 3010 training loss: 0.05550695117563009\n",
      "epoch: 7 trial 3011 training loss: 0.038619667291641235\n",
      "epoch: 7 trial 3012 training loss: 0.13829420879483223\n",
      "epoch: 7 trial 3013 training loss: 0.1173705942928791\n",
      "epoch: 7 trial 3014 training loss: 0.018848505336791277\n",
      "epoch: 7 trial 3015 training loss: 0.014801736455410719\n",
      "epoch: 7 trial 3016 training loss: 0.022048572544008493\n",
      "epoch: 7 trial 3017 training loss: 0.013346201274544\n",
      "epoch: 7 trial 3018 training loss: 0.011764000868424773\n",
      "epoch: 7 trial 3019 training loss: 0.051372526213526726\n",
      "epoch: 7 trial 3020 training loss: 0.019236925058066845\n",
      "epoch: 7 trial 3021 training loss: 0.026980030350387096\n",
      "epoch: 7 trial 3022 training loss: 0.03915445227175951\n",
      "epoch: 7 trial 3023 training loss: 0.02889265725389123\n",
      "epoch: 7 trial 3024 training loss: 0.024090636987239122\n",
      "epoch: 7 trial 3025 training loss: 0.013009398709982634\n",
      "epoch: 7 trial 3026 training loss: 0.05151664838194847\n",
      "epoch: 7 trial 3027 training loss: 0.08434509299695492\n",
      "epoch: 7 trial 3028 training loss: 0.03820636495947838\n",
      "epoch: 7 trial 3029 training loss: 0.008628355572000146\n",
      "epoch: 7 trial 3030 training loss: 0.027620739303529263\n",
      "epoch: 7 trial 3031 training loss: 0.02245646109804511\n",
      "epoch: 7 trial 3032 training loss: 0.02662500273436308\n",
      "epoch: 7 trial 3033 training loss: 0.14751330390572548\n",
      "epoch: 7 trial 3034 training loss: 0.030083288438618183\n",
      "epoch: 7 trial 3035 training loss: 0.09053343161940575\n",
      "epoch: 7 trial 3036 training loss: 0.03800091240555048\n",
      "epoch: 7 trial 3037 training loss: 0.03636232390999794\n",
      "epoch: 7 trial 3038 training loss: 0.05813748016953468\n",
      "epoch: 7 trial 3039 training loss: 0.012701054802164435\n",
      "epoch: 7 trial 3040 training loss: 0.03784384671598673\n",
      "epoch: 7 trial 3041 training loss: 0.018056271132081747\n",
      "epoch: 7 trial 3042 training loss: 0.13349031284451485\n",
      "epoch: 7 trial 3043 training loss: 0.020014772191643715\n",
      "epoch: 7 trial 3044 training loss: 0.054509177803993225\n",
      "epoch: 7 trial 3045 training loss: 0.046910024248063564\n",
      "epoch: 7 trial 3046 training loss: 0.02993327844887972\n",
      "epoch: 7 trial 3047 training loss: 0.039908114820718765\n",
      "epoch: 7 trial 3048 training loss: 0.07399712316691875\n",
      "epoch: 7 trial 3049 training loss: 0.023307239171117544\n",
      "epoch: 7 trial 3050 training loss: 0.02139047533273697\n",
      "epoch: 7 trial 3051 training loss: 0.11119196377694607\n",
      "epoch: 7 trial 3052 training loss: 0.020289387553930283\n",
      "epoch: 7 trial 3053 training loss: 0.04427825100719929\n",
      "epoch: 7 trial 3054 training loss: 0.06358639895915985\n",
      "epoch: 7 trial 3055 training loss: 0.03395817335695028\n",
      "epoch: 7 trial 3056 training loss: 0.037395304068922997\n",
      "epoch: 7 trial 3057 training loss: 0.016109095886349678\n",
      "epoch: 7 trial 3058 training loss: 0.03890592139214277\n",
      "epoch: 7 trial 3059 training loss: 0.009930582484230399\n",
      "epoch: 7 trial 3060 training loss: 0.008939744322560728\n",
      "epoch: 7 trial 3061 training loss: 0.01984517276287079\n",
      "epoch: 7 trial 3062 training loss: 0.0037035219138488173\n",
      "epoch: 7 trial 3063 training loss: 0.015080945566296577\n",
      "epoch: 7 trial 3064 training loss: 0.02725316258147359\n",
      "epoch: 7 trial 3065 training loss: 0.007972500054165721\n",
      "epoch: 7 trial 3066 training loss: 0.06064886599779129\n",
      "epoch: 7 trial 3067 training loss: 0.003675443003885448\n",
      "epoch: 7 trial 3068 training loss: 0.05409043841063976\n",
      "epoch: 7 trial 3069 training loss: 0.01653756620362401\n",
      "epoch: 7 trial 3070 training loss: 0.024223207961767912\n",
      "epoch: 7 trial 3071 training loss: 0.027187053579837084\n",
      "epoch: 7 trial 3072 training loss: 0.012756156735122204\n",
      "epoch: 7 trial 3073 training loss: 0.03328188043087721\n",
      "epoch: 7 trial 3074 training loss: 0.14008280634880066\n",
      "epoch: 7 trial 3075 training loss: 0.04672546871006489\n",
      "epoch: 7 trial 3076 training loss: 0.006884780945256352\n",
      "epoch: 7 trial 3077 training loss: 0.059341730549931526\n",
      "epoch: 7 trial 3078 training loss: 0.02528580231592059\n",
      "epoch: 7 trial 3079 training loss: 0.03921887371689081\n",
      "epoch: 7 trial 3080 training loss: 0.024043970741331577\n",
      "epoch: 7 trial 3081 training loss: 0.023326054215431213\n",
      "epoch: 7 trial 3082 training loss: 0.03942250460386276\n",
      "epoch: 7 trial 3083 training loss: 0.04977081250399351\n",
      "epoch: 7 trial 3084 training loss: 0.03168784640729427\n",
      "epoch: 7 trial 3085 training loss: 0.050843531265854836\n",
      "epoch: 7 trial 3086 training loss: 0.017936614342033863\n",
      "epoch: 7 trial 3087 training loss: 0.039304038509726524\n",
      "epoch: 7 trial 3088 training loss: 0.02658039517700672\n",
      "epoch: 7 trial 3089 training loss: 0.02105933427810669\n",
      "epoch: 7 trial 3090 training loss: 0.06369820237159729\n",
      "epoch: 7 trial 3091 training loss: 0.05539588816463947\n",
      "epoch: 7 trial 3092 training loss: 0.01679167291149497\n",
      "epoch: 7 trial 3093 training loss: 0.06180092319846153\n",
      "epoch: 7 trial 3094 training loss: 0.03325944673269987\n",
      "epoch: 7 trial 3095 training loss: 0.08640254847705364\n",
      "epoch: 7 trial 3096 training loss: 0.09849032200872898\n",
      "epoch: 7 trial 3097 training loss: 0.03985295072197914\n",
      "epoch: 7 trial 3098 training loss: 0.018615252804011106\n",
      "epoch: 7 trial 3099 training loss: 0.048706465400755405\n",
      "epoch: 7 trial 3100 training loss: 0.019339578226208687\n",
      "epoch: 7 trial 3101 training loss: 0.020034255925565958\n",
      "epoch: 7 trial 3102 training loss: 0.059905124828219414\n",
      "epoch: 7 trial 3103 training loss: 0.09999253042042255\n",
      "epoch: 7 trial 3104 training loss: 0.008432590868324041\n",
      "epoch: 7 trial 3105 training loss: 0.007170475786551833\n",
      "epoch: 7 trial 3106 training loss: 0.010520352516323328\n",
      "epoch: 7 trial 3107 training loss: 0.008687135763466358\n",
      "epoch: 7 trial 3108 training loss: 0.01792806200683117\n",
      "epoch: 7 trial 3109 training loss: 0.01108325901441276\n",
      "epoch: 7 trial 3110 training loss: 0.06129084713757038\n",
      "epoch: 7 trial 3111 training loss: 0.033433922566473484\n",
      "epoch: 7 trial 3112 training loss: 0.02241878490895033\n",
      "epoch: 7 trial 3113 training loss: 0.08743863366544247\n",
      "epoch: 7 trial 3114 training loss: 0.04514707159250975\n",
      "epoch: 7 trial 3115 training loss: 0.06403609830886126\n",
      "epoch: 7 trial 3116 training loss: 0.06000641640275717\n",
      "epoch: 7 trial 3117 training loss: 0.1399783380329609\n",
      "epoch: 7 trial 3118 training loss: 0.010909704258665442\n",
      "epoch: 7 trial 3119 training loss: 0.017172297928482294\n",
      "epoch: 7 trial 3120 training loss: 0.020519368816167116\n",
      "epoch: 7 trial 3121 training loss: 0.0206610020250082\n",
      "epoch: 7 trial 3122 training loss: 0.018992010969668627\n",
      "epoch: 7 trial 3123 training loss: 0.04339652229100466\n",
      "epoch: 7 trial 3124 training loss: 0.052198564633727074\n",
      "epoch: 7 trial 3125 training loss: 0.0393900228664279\n",
      "epoch: 7 trial 3126 training loss: 0.07001913338899612\n",
      "epoch: 7 trial 3127 training loss: 0.050908736884593964\n",
      "epoch: 7 trial 3128 training loss: 0.06683594174683094\n",
      "epoch: 7 trial 3129 training loss: 0.008936131605878472\n",
      "epoch: 7 trial 3130 training loss: 0.013256308855488896\n",
      "epoch: 7 trial 3131 training loss: 0.030185013078153133\n",
      "epoch: 7 trial 3132 training loss: 0.017388661624863744\n",
      "epoch: 7 trial 3133 training loss: 0.051975712180137634\n",
      "epoch: 7 trial 3134 training loss: 0.014237869065254927\n",
      "epoch: 7 trial 3135 training loss: 0.1481434367597103\n",
      "epoch: 7 trial 3136 training loss: 0.043486492708325386\n",
      "epoch: 7 trial 3137 training loss: 0.1339425891637802\n",
      "epoch: 7 trial 3138 training loss: 0.09527982771396637\n",
      "epoch: 7 trial 3139 training loss: 0.018160882871598005\n",
      "epoch: 7 trial 3140 training loss: 0.044888196513056755\n",
      "epoch: 7 trial 3141 training loss: 0.021414324641227722\n",
      "epoch: 7 trial 3142 training loss: 0.04501040745526552\n",
      "epoch: 7 trial 3143 training loss: 0.04497868847101927\n",
      "epoch: 7 trial 3144 training loss: 0.10793547704815865\n",
      "epoch: 7 trial 3145 training loss: 0.0316492635756731\n",
      "epoch: 7 trial 3146 training loss: 0.03361676074564457\n",
      "epoch: 7 trial 3147 training loss: 0.04870487190783024\n",
      "epoch: 7 trial 3148 training loss: 0.004592140903696418\n",
      "epoch: 7 trial 3149 training loss: 0.052094693295657635\n",
      "epoch: 7 trial 3150 training loss: 0.01963332761079073\n",
      "epoch: 7 trial 3151 training loss: 0.05341639369726181\n",
      "epoch: 7 trial 3152 training loss: 0.029039802961051464\n",
      "epoch: 7 trial 3153 training loss: 0.026464303024113178\n",
      "epoch: 7 trial 3154 training loss: 0.06406497769057751\n",
      "epoch: 7 trial 3155 training loss: 0.014888604637235403\n",
      "epoch: 7 trial 3156 training loss: 0.005602575954981148\n",
      "epoch: 7 trial 3157 training loss: 0.028155082371085882\n",
      "epoch: 7 trial 3158 training loss: 0.08626410365104675\n",
      "epoch: 7 trial 3159 training loss: 0.021285823546350002\n",
      "epoch: 7 trial 3160 training loss: 0.0803129244595766\n",
      "epoch: 7 trial 3161 training loss: 0.03059257846325636\n",
      "epoch: 7 trial 3162 training loss: 0.019177669193595648\n",
      "epoch: 7 trial 3163 training loss: 0.17436130344867706\n",
      "epoch: 7 trial 3164 training loss: 0.014264042023569345\n",
      "epoch: 7 trial 3165 training loss: 0.016886352095752954\n",
      "epoch: 7 trial 3166 training loss: 0.008092611562460661\n",
      "epoch: 7 trial 3167 training loss: 0.004728538799099624\n",
      "epoch: 7 trial 3168 training loss: 0.017175224609673023\n",
      "epoch: 7 trial 3169 training loss: 0.0457036392763257\n",
      "epoch: 7 trial 3170 training loss: 0.04100839328020811\n",
      "epoch: 7 trial 3171 training loss: 0.03140312898904085\n",
      "epoch: 7 trial 3172 training loss: 0.03334355726838112\n",
      "epoch: 7 trial 3173 training loss: 0.02505496796220541\n",
      "epoch: 7 trial 3174 training loss: 0.07092898897826672\n",
      "epoch: 7 trial 3175 training loss: 0.01557157514616847\n",
      "epoch: 7 trial 3176 training loss: 0.008977210032753646\n",
      "epoch: 7 trial 3177 training loss: 0.022116171661764383\n",
      "epoch: 7 trial 3178 training loss: 0.006731103174388409\n",
      "epoch: 7 trial 3179 training loss: 0.07169118896126747\n",
      "epoch: 7 trial 3180 training loss: 0.006062890985049307\n",
      "epoch: 7 trial 3181 training loss: 0.02138050878420472\n",
      "epoch: 7 trial 3182 training loss: 0.011916503543034196\n",
      "epoch: 7 trial 3183 training loss: 0.06704074703156948\n",
      "epoch: 7 trial 3184 training loss: 0.018243869300931692\n",
      "epoch: 7 trial 3185 training loss: 0.01724553620442748\n",
      "epoch: 7 trial 3186 training loss: 0.05562859587371349\n",
      "epoch: 7 trial 3187 training loss: 0.09316208027303219\n",
      "epoch: 7 trial 3188 training loss: 0.030742441304028034\n",
      "epoch: 7 trial 3189 training loss: 0.08693715184926987\n",
      "epoch: 7 trial 3190 training loss: 0.0235285721719265\n",
      "epoch: 7 trial 3191 training loss: 0.07934517972171307\n",
      "epoch: 7 trial 3192 training loss: 0.019494589883834124\n",
      "epoch: 7 trial 3193 training loss: 0.014369345270097256\n",
      "epoch: 7 trial 3194 training loss: 0.05433155968785286\n",
      "epoch: 7 trial 3195 training loss: 0.027930633164942265\n",
      "epoch: 7 trial 3196 training loss: 0.01698338147252798\n",
      "epoch: 7 trial 3197 training loss: 0.017019358463585377\n",
      "epoch: 7 trial 3198 training loss: 0.008164550294168293\n",
      "epoch: 7 trial 3199 training loss: 0.020261066034436226\n",
      "epoch: 7 trial 3200 training loss: 0.02902065869420767\n",
      "epoch: 7 trial 3201 training loss: 0.020938037429004908\n",
      "epoch: 7 trial 3202 training loss: 0.03477501682937145\n",
      "epoch: 7 trial 3203 training loss: 0.0555066941305995\n",
      "epoch: 7 trial 3204 training loss: 0.029716581106185913\n",
      "epoch: 7 trial 3205 training loss: 0.020660447422415018\n",
      "epoch: 7 trial 3206 training loss: 0.009187409188598394\n",
      "epoch: 7 trial 3207 training loss: 0.01646964531391859\n",
      "epoch: 7 trial 3208 training loss: 0.02977582160383463\n",
      "epoch: 7 trial 3209 training loss: 0.05604250729084015\n",
      "epoch: 7 trial 3210 training loss: 0.050867242738604546\n",
      "epoch: 7 trial 3211 training loss: 0.002436590555589646\n",
      "epoch: 7 trial 3212 training loss: 0.02965153194963932\n",
      "epoch: 7 trial 3213 training loss: 0.0346712926402688\n",
      "epoch: 7 trial 3214 training loss: 0.015410088701173663\n",
      "epoch: 7 trial 3215 training loss: 0.013679197523742914\n",
      "epoch: 7 trial 3216 training loss: 0.01792063983157277\n",
      "epoch: 7 trial 3217 training loss: 0.0416802354156971\n",
      "epoch: 7 trial 3218 training loss: 0.021327176596969366\n",
      "epoch: 7 trial 3219 training loss: 0.03786428365856409\n",
      "epoch: 7 trial 3220 training loss: 0.05930258147418499\n",
      "epoch: 7 trial 3221 training loss: 0.027074160985648632\n",
      "epoch: 7 trial 3222 training loss: 0.03230035677552223\n",
      "epoch: 7 trial 3223 training loss: 0.06914680451154709\n",
      "epoch: 7 trial 3224 training loss: 0.034394048154354095\n",
      "epoch: 7 trial 3225 training loss: 0.06875552609562874\n",
      "epoch: 7 trial 3226 training loss: 0.01810809364542365\n",
      "epoch: 7 trial 3227 training loss: 0.03081248328089714\n",
      "epoch: 7 trial 3228 training loss: 0.025647377595305443\n",
      "epoch: 7 trial 3229 training loss: 0.02277285512536764\n",
      "epoch: 7 trial 3230 training loss: 0.018256699666380882\n",
      "epoch: 7 trial 3231 training loss: 0.08983411267399788\n",
      "epoch: 7 trial 3232 training loss: 0.01264305179938674\n",
      "epoch: 7 trial 3233 training loss: 0.09651872888207436\n",
      "epoch: 7 trial 3234 training loss: 0.0253730658441782\n",
      "epoch: 7 trial 3235 training loss: 0.009948217310011387\n",
      "epoch: 7 trial 3236 training loss: 0.06668347865343094\n",
      "epoch: 7 trial 3237 training loss: 0.055062493309378624\n",
      "epoch: 7 trial 3238 training loss: 0.009641313459724188\n",
      "epoch: 7 trial 3239 training loss: 0.03191980812698603\n",
      "epoch: 7 trial 3240 training loss: 0.008987563895061612\n",
      "epoch: 7 trial 3241 training loss: 0.03398211766034365\n",
      "epoch: 7 trial 3242 training loss: 0.018873081542551517\n",
      "epoch: 7 trial 3243 training loss: 0.03276466950774193\n",
      "epoch: 7 trial 3244 training loss: 0.2542526200413704\n",
      "epoch: 7 trial 3245 training loss: 0.0512812789529562\n",
      "epoch: 7 trial 3246 training loss: 0.010538722621276975\n",
      "epoch: 7 trial 3247 training loss: 0.0222021765075624\n",
      "epoch: 7 trial 3248 training loss: 0.1375628337264061\n",
      "epoch: 7 trial 3249 training loss: 0.05100026912987232\n",
      "epoch: 7 trial 3250 training loss: 0.06197772175073624\n",
      "epoch: 7 trial 3251 training loss: 0.028538988903164864\n",
      "epoch: 7 trial 3252 training loss: 0.03926315624266863\n",
      "epoch: 7 trial 3253 training loss: 0.016324013471603394\n",
      "epoch: 7 trial 3254 training loss: 0.09899583831429482\n",
      "epoch: 7 trial 3255 training loss: 0.013245533686131239\n",
      "epoch: 7 trial 3256 training loss: 0.07076689228415489\n",
      "epoch: 7 trial 3257 training loss: 0.03420507349073887\n",
      "epoch: 7 trial 3258 training loss: 0.01738274097442627\n",
      "epoch: 7 trial 3259 training loss: 0.043902586214244366\n",
      "epoch: 7 trial 3260 training loss: 0.011853175237774849\n",
      "epoch: 7 trial 3261 training loss: 0.03410369250923395\n",
      "epoch: 7 trial 3262 training loss: 0.10216526873409748\n",
      "epoch: 7 trial 3263 training loss: 0.04912054818123579\n",
      "epoch: 7 trial 3264 training loss: 0.03227903228253126\n",
      "epoch: 7 trial 3265 training loss: 0.09858807735145092\n",
      "epoch: 7 trial 3266 training loss: 0.08844326250255108\n",
      "epoch: 7 trial 3267 training loss: 0.03389859478920698\n",
      "epoch: 7 trial 3268 training loss: 0.07749101333320141\n",
      "epoch: 7 trial 3269 training loss: 0.04313990008085966\n",
      "epoch: 7 trial 3270 training loss: 0.018220103811472654\n",
      "epoch: 7 trial 3271 training loss: 0.005282015888951719\n",
      "epoch: 7 trial 3272 training loss: 0.18503014370799065\n",
      "epoch: 7 trial 3273 training loss: 0.06574058718979359\n",
      "epoch: 7 trial 3274 training loss: 0.041853065602481365\n",
      "epoch: 7 trial 3275 training loss: 0.053016312420368195\n",
      "epoch: 7 trial 3276 training loss: 0.025952524971216917\n",
      "epoch: 7 trial 3277 training loss: 0.031284586526453495\n",
      "epoch: 7 trial 3278 training loss: 0.019496716558933258\n",
      "epoch: 7 trial 3279 training loss: 0.04766257852315903\n",
      "epoch: 7 trial 3280 training loss: 0.03556198254227638\n",
      "epoch: 7 trial 3281 training loss: 0.04727817140519619\n",
      "epoch: 7 trial 3282 training loss: 0.07617572788149118\n",
      "epoch: 7 trial 3283 training loss: 0.041394906118512154\n",
      "epoch: 7 trial 3284 training loss: 0.01262939814478159\n",
      "epoch: 7 trial 3285 training loss: 0.03203436639159918\n",
      "epoch: 7 trial 3286 training loss: 0.007008914602920413\n",
      "epoch: 7 trial 3287 training loss: 0.048955436795949936\n",
      "epoch: 7 trial 3288 training loss: 0.03848615102469921\n",
      "epoch: 7 trial 3289 training loss: 0.011694966582581401\n",
      "epoch: 7 trial 3290 training loss: 0.002609361836221069\n",
      "epoch: 7 trial 3291 training loss: 0.012582774506881833\n",
      "epoch: 7 trial 3292 training loss: 0.016475915908813477\n",
      "epoch: 7 trial 3293 training loss: 0.029163310304284096\n",
      "epoch: 7 trial 3294 training loss: 0.006489173858426511\n",
      "epoch: 7 trial 3295 training loss: 0.0036505485186353326\n",
      "epoch: 7 trial 3296 training loss: 0.005142042995430529\n",
      "epoch: 7 trial 3297 training loss: 0.009076462360098958\n",
      "epoch: 7 trial 3298 training loss: 0.013739676680415869\n",
      "epoch: 7 trial 3299 training loss: 0.02723937015980482\n",
      "epoch: 7 trial 3300 training loss: 0.10576387122273445\n",
      "epoch: 7 trial 3301 training loss: 0.028129741549491882\n",
      "epoch: 7 trial 3302 training loss: 0.02543683908879757\n",
      "epoch: 7 trial 3303 training loss: 0.03271699883043766\n",
      "epoch: 7 trial 3304 training loss: 0.03767069010064006\n",
      "epoch: 7 trial 3305 training loss: 0.025385971646755934\n",
      "epoch: 7 trial 3306 training loss: 0.02690357156097889\n",
      "epoch: 7 trial 3307 training loss: 0.049964580684900284\n",
      "epoch: 7 trial 3308 training loss: 0.119756318628788\n",
      "epoch: 7 trial 3309 training loss: 0.1261455900967121\n",
      "epoch: 7 trial 3310 training loss: 0.026962249539792538\n",
      "epoch: 7 trial 3311 training loss: 0.044920057989656925\n",
      "epoch: 7 trial 3312 training loss: 0.05837702006101608\n",
      "epoch: 7 trial 3313 training loss: 0.02753676474094391\n",
      "epoch: 7 trial 3314 training loss: 0.08940304815769196\n",
      "epoch: 7 trial 3315 training loss: 0.11134960316121578\n",
      "epoch: 7 trial 3316 training loss: 0.07993999868631363\n",
      "epoch: 7 trial 3317 training loss: 0.025066038127988577\n",
      "epoch: 7 trial 3318 training loss: 0.08128315210342407\n",
      "epoch: 7 trial 3319 training loss: 0.037063123658299446\n",
      "epoch: 7 trial 3320 training loss: 0.07648122496902943\n",
      "epoch: 7 trial 3321 training loss: 0.03352210018783808\n",
      "epoch: 7 trial 3322 training loss: 0.06129617616534233\n",
      "epoch: 7 trial 3323 training loss: 0.018318664748221636\n",
      "epoch: 7 trial 3324 training loss: 0.0035753968404605985\n",
      "epoch: 7 trial 3325 training loss: 0.02385661657899618\n",
      "epoch: 7 trial 3326 training loss: 0.026975389569997787\n",
      "epoch: 7 trial 3327 training loss: 0.05164775159209967\n",
      "epoch: 7 trial 3328 training loss: 0.04066799487918615\n",
      "epoch: 7 trial 3329 training loss: 0.013257126789540052\n",
      "epoch: 7 trial 3330 training loss: 0.051975833252072334\n",
      "epoch: 7 trial 3331 training loss: 0.03321938216686249\n",
      "epoch: 7 trial 3332 training loss: 0.012932368088513613\n",
      "epoch: 7 trial 3333 training loss: 0.02095430064946413\n",
      "epoch: 7 trial 3334 training loss: 0.012351104989647865\n",
      "epoch: 7 trial 3335 training loss: 0.017438848968595266\n",
      "epoch: 7 trial 3336 training loss: 0.035201726481318474\n",
      "epoch: 7 trial 3337 training loss: 0.0335747841745615\n",
      "epoch: 7 trial 3338 training loss: 0.026650981977581978\n",
      "epoch: 7 trial 3339 training loss: 0.018254100810736418\n",
      "epoch: 7 trial 3340 training loss: 0.016376018524169922\n",
      "epoch: 7 trial 3341 training loss: 0.01071723410859704\n",
      "epoch: 7 trial 3342 training loss: 0.016896942164748907\n",
      "epoch: 7 trial 3343 training loss: 0.0207168604247272\n",
      "epoch: 7 trial 3344 training loss: 0.01762152975425124\n",
      "epoch: 7 trial 3345 training loss: 0.006981369573622942\n",
      "epoch: 7 trial 3346 training loss: 0.01585967093706131\n",
      "epoch: 7 trial 3347 training loss: 0.09297914989292622\n",
      "epoch: 7 trial 3348 training loss: 0.014697711914777756\n",
      "epoch: 7 trial 3349 training loss: 0.026572315022349358\n",
      "epoch: 7 trial 3350 training loss: 0.015132108703255653\n",
      "epoch: 7 trial 3351 training loss: 0.04706359002739191\n",
      "epoch: 7 trial 3352 training loss: 0.039532207883894444\n",
      "epoch: 7 trial 3353 training loss: 0.03832046780735254\n",
      "epoch: 7 trial 3354 training loss: 0.02781110256910324\n",
      "epoch: 7 trial 3355 training loss: 0.017903704196214676\n",
      "epoch: 7 trial 3356 training loss: 0.11210986226797104\n",
      "epoch: 7 trial 3357 training loss: 0.006126413238234818\n",
      "epoch: 7 trial 3358 training loss: 0.017654741648584604\n",
      "epoch: 7 trial 3359 training loss: 0.13425633683800697\n",
      "epoch: 7 trial 3360 training loss: 0.02827951591461897\n",
      "epoch: 7 trial 3361 training loss: 0.023225337732583284\n",
      "epoch: 7 trial 3362 training loss: 0.01886668149381876\n",
      "epoch: 7 trial 3363 training loss: 0.01286127744242549\n",
      "epoch: 7 trial 3364 training loss: 0.0237324433401227\n",
      "epoch: 7 trial 3365 training loss: 0.014651278033852577\n",
      "epoch: 7 trial 3366 training loss: 0.04367738030850887\n",
      "epoch: 7 trial 3367 training loss: 0.15641287341713905\n",
      "epoch: 7 trial 3368 training loss: 0.07064546085894108\n",
      "epoch: 7 trial 3369 training loss: 0.02764979749917984\n",
      "epoch: 7 trial 3370 training loss: 0.10355488210916519\n",
      "epoch: 7 trial 3371 training loss: 0.10639077611267567\n",
      "epoch: 7 trial 3372 training loss: 0.02449761237949133\n",
      "epoch: 7 trial 3373 training loss: 0.07736940495669842\n",
      "epoch: 7 trial 3374 training loss: 0.006690525216981769\n",
      "epoch: 7 trial 3375 training loss: 0.057699648663401604\n",
      "epoch: 7 trial 3376 training loss: 0.01381368050351739\n",
      "epoch: 7 trial 3377 training loss: 0.02671730751171708\n",
      "epoch: 7 trial 3378 training loss: 0.10339438542723656\n",
      "epoch: 7 trial 3379 training loss: 0.03825416695326567\n",
      "epoch: 7 trial 3380 training loss: 0.014487935695797205\n",
      "epoch: 7 trial 3381 training loss: 0.026614265516400337\n",
      "epoch: 7 trial 3382 training loss: 0.09216504544019699\n",
      "epoch: 7 trial 3383 training loss: 0.011950026033446193\n",
      "epoch: 7 trial 3384 training loss: 0.04793416615575552\n",
      "epoch: 7 trial 3385 training loss: 0.036405593156814575\n",
      "epoch: 7 trial 3386 training loss: 0.02539745531976223\n",
      "epoch: 7 trial 3387 training loss: 0.013784012291580439\n",
      "epoch: 7 trial 3388 training loss: 0.05302029103040695\n",
      "epoch: 8 trial 3389 training loss: 0.008466122904792428\n",
      "epoch: 8 trial 3390 training loss: 0.013725979719310999\n",
      "epoch: 8 trial 3391 training loss: 0.02013226505368948\n",
      "epoch: 8 trial 3392 training loss: 0.06450203992426395\n",
      "epoch: 8 trial 3393 training loss: 0.004622761276550591\n",
      "epoch: 8 trial 3394 training loss: 0.005493457079865038\n",
      "epoch: 8 trial 3395 training loss: 0.027938939165323973\n",
      "epoch: 8 trial 3396 training loss: 0.01952113164588809\n",
      "epoch: 8 trial 3397 training loss: 0.008617659099400043\n",
      "epoch: 8 trial 3398 training loss: 0.019801963586360216\n",
      "epoch: 8 trial 3399 training loss: 0.04545322246849537\n",
      "epoch: 8 trial 3400 training loss: 0.030018975026905537\n",
      "epoch: 8 trial 3401 training loss: 0.01984936511144042\n",
      "epoch: 8 trial 3402 training loss: 0.0542792733758688\n",
      "epoch: 8 trial 3403 training loss: 0.013185497839003801\n",
      "epoch: 8 trial 3404 training loss: 0.022462825290858746\n",
      "epoch: 8 trial 3405 training loss: 0.1163785569369793\n",
      "epoch: 8 trial 3406 training loss: 0.049575950019061565\n",
      "epoch: 8 trial 3407 training loss: 0.013223795453086495\n",
      "epoch: 8 trial 3408 training loss: 0.017386562656611204\n",
      "epoch: 8 trial 3409 training loss: 0.015489178244024515\n",
      "epoch: 8 trial 3410 training loss: 0.07500473037362099\n",
      "epoch: 8 trial 3411 training loss: 0.009696888038888574\n",
      "epoch: 8 trial 3412 training loss: 0.005675464286468923\n",
      "epoch: 8 trial 3413 training loss: 0.15684080123901367\n",
      "epoch: 8 trial 3414 training loss: 0.023701824713498354\n",
      "epoch: 8 trial 3415 training loss: 0.010639580199494958\n",
      "epoch: 8 trial 3416 training loss: 0.11573827266693115\n",
      "epoch: 8 trial 3417 training loss: 0.024213847238570452\n",
      "epoch: 8 trial 3418 training loss: 0.02046600729227066\n",
      "epoch: 8 trial 3419 training loss: 0.03515467792749405\n",
      "epoch: 8 trial 3420 training loss: 0.013858230784535408\n",
      "epoch: 8 trial 3421 training loss: 0.031279778108000755\n",
      "epoch: 8 trial 3422 training loss: 0.04307306557893753\n",
      "epoch: 8 trial 3423 training loss: 0.02067439630627632\n",
      "epoch: 8 trial 3424 training loss: 0.06507504172623158\n",
      "epoch: 8 trial 3425 training loss: 0.1424579732120037\n",
      "epoch: 8 trial 3426 training loss: 0.31370652467012405\n",
      "epoch: 8 trial 3427 training loss: 0.03618072718381882\n",
      "epoch: 8 trial 3428 training loss: 0.1063938532024622\n",
      "epoch: 8 trial 3429 training loss: 0.11791639775037766\n",
      "epoch: 8 trial 3430 training loss: 0.01393031794577837\n",
      "epoch: 8 trial 3431 training loss: 0.09704398736357689\n",
      "epoch: 8 trial 3432 training loss: 0.03772259596735239\n",
      "epoch: 8 trial 3433 training loss: 0.0312112458050251\n",
      "epoch: 8 trial 3434 training loss: 0.054382127709686756\n",
      "epoch: 8 trial 3435 training loss: 0.016518651507794857\n",
      "epoch: 8 trial 3436 training loss: 0.013429024256765842\n",
      "epoch: 8 trial 3437 training loss: 0.008380066370591521\n",
      "epoch: 8 trial 3438 training loss: 0.0730750747025013\n",
      "epoch: 8 trial 3439 training loss: 0.06922699883580208\n",
      "epoch: 8 trial 3440 training loss: 0.09005912858992815\n",
      "epoch: 8 trial 3441 training loss: 0.06925437599420547\n",
      "epoch: 8 trial 3442 training loss: 0.0766715295612812\n",
      "epoch: 8 trial 3443 training loss: 0.03675804380327463\n",
      "epoch: 8 trial 3444 training loss: 0.07559599354863167\n",
      "epoch: 8 trial 3445 training loss: 0.023022855166345835\n",
      "epoch: 8 trial 3446 training loss: 0.02069709822535515\n",
      "epoch: 8 trial 3447 training loss: 0.018027649261057377\n",
      "epoch: 8 trial 3448 training loss: 0.07626990042626858\n",
      "epoch: 8 trial 3449 training loss: 0.023523567710071802\n",
      "epoch: 8 trial 3450 training loss: 0.1086268313229084\n",
      "epoch: 8 trial 3451 training loss: 0.018123368732631207\n",
      "epoch: 8 trial 3452 training loss: 0.009578806115314364\n",
      "epoch: 8 trial 3453 training loss: 0.03843412082642317\n",
      "epoch: 8 trial 3454 training loss: 0.05405548866838217\n",
      "epoch: 8 trial 3455 training loss: 0.02084975177422166\n",
      "epoch: 8 trial 3456 training loss: 0.022896342910826206\n",
      "epoch: 8 trial 3457 training loss: 0.05526396445930004\n",
      "epoch: 8 trial 3458 training loss: 0.03349904529750347\n",
      "epoch: 8 trial 3459 training loss: 0.046502187848091125\n",
      "epoch: 8 trial 3460 training loss: 0.03480893839150667\n",
      "epoch: 8 trial 3461 training loss: 0.01616027601994574\n",
      "epoch: 8 trial 3462 training loss: 0.06899840012192726\n",
      "epoch: 8 trial 3463 training loss: 0.012800256721675396\n",
      "epoch: 8 trial 3464 training loss: 0.024714371655136347\n",
      "epoch: 8 trial 3465 training loss: 0.027288085781037807\n",
      "epoch: 8 trial 3466 training loss: 0.086361950263381\n",
      "epoch: 8 trial 3467 training loss: 0.020060930866748095\n",
      "epoch: 8 trial 3468 training loss: 0.05552242323756218\n",
      "epoch: 8 trial 3469 training loss: 0.01929838163778186\n",
      "epoch: 8 trial 3470 training loss: 0.04313998203724623\n",
      "epoch: 8 trial 3471 training loss: 0.07619927451014519\n",
      "epoch: 8 trial 3472 training loss: 0.018516818527132273\n",
      "epoch: 8 trial 3473 training loss: 0.06398315913975239\n",
      "epoch: 8 trial 3474 training loss: 0.013191377744078636\n",
      "epoch: 8 trial 3475 training loss: 0.03761095553636551\n",
      "epoch: 8 trial 3476 training loss: 0.0356582710519433\n",
      "epoch: 8 trial 3477 training loss: 0.04602621775120497\n",
      "epoch: 8 trial 3478 training loss: 0.0055414545349776745\n",
      "epoch: 8 trial 3479 training loss: 0.033902390860021114\n",
      "epoch: 8 trial 3480 training loss: 0.0239430358633399\n",
      "epoch: 8 trial 3481 training loss: 0.017836228013038635\n",
      "epoch: 8 trial 3482 training loss: 0.06554826907813549\n",
      "epoch: 8 trial 3483 training loss: 0.04391346964985132\n",
      "epoch: 8 trial 3484 training loss: 0.04941729549318552\n",
      "epoch: 8 trial 3485 training loss: 0.017911746632307768\n",
      "epoch: 8 trial 3486 training loss: 0.09318709932267666\n",
      "epoch: 8 trial 3487 training loss: 0.04487435985356569\n",
      "epoch: 8 trial 3488 training loss: 0.04266241006553173\n",
      "epoch: 8 trial 3489 training loss: 0.04244633484631777\n",
      "epoch: 8 trial 3490 training loss: 0.07403548620641232\n",
      "epoch: 8 trial 3491 training loss: 0.035826160572469234\n",
      "epoch: 8 trial 3492 training loss: 0.04088837746530771\n",
      "epoch: 8 trial 3493 training loss: 0.26698634028434753\n",
      "epoch: 8 trial 3494 training loss: 0.053172528743743896\n",
      "epoch: 8 trial 3495 training loss: 0.036295645870268345\n",
      "epoch: 8 trial 3496 training loss: 0.14151860773563385\n",
      "epoch: 8 trial 3497 training loss: 0.11947769299149513\n",
      "epoch: 8 trial 3498 training loss: 0.018432728946208954\n",
      "epoch: 8 trial 3499 training loss: 0.01293504098430276\n",
      "epoch: 8 trial 3500 training loss: 0.024772026110440493\n",
      "epoch: 8 trial 3501 training loss: 0.012241005897521973\n",
      "epoch: 8 trial 3502 training loss: 0.009850137634202838\n",
      "epoch: 8 trial 3503 training loss: 0.052670721895992756\n",
      "epoch: 8 trial 3504 training loss: 0.01885654265061021\n",
      "epoch: 8 trial 3505 training loss: 0.02949661109596491\n",
      "epoch: 8 trial 3506 training loss: 0.03911922685801983\n",
      "epoch: 8 trial 3507 training loss: 0.029875125735998154\n",
      "epoch: 8 trial 3508 training loss: 0.022644128650426865\n",
      "epoch: 8 trial 3509 training loss: 0.013367129722610116\n",
      "epoch: 8 trial 3510 training loss: 0.049561246298253536\n",
      "epoch: 8 trial 3511 training loss: 0.08765331655740738\n",
      "epoch: 8 trial 3512 training loss: 0.03821024112403393\n",
      "epoch: 8 trial 3513 training loss: 0.008047988405451179\n",
      "epoch: 8 trial 3514 training loss: 0.02788424864411354\n",
      "epoch: 8 trial 3515 training loss: 0.020795862656086683\n",
      "epoch: 8 trial 3516 training loss: 0.02709829667583108\n",
      "epoch: 8 trial 3517 training loss: 0.14916083961725235\n",
      "epoch: 8 trial 3518 training loss: 0.02901069074869156\n",
      "epoch: 8 trial 3519 training loss: 0.08228214643895626\n",
      "epoch: 8 trial 3520 training loss: 0.0317189060151577\n",
      "epoch: 8 trial 3521 training loss: 0.03262521233409643\n",
      "epoch: 8 trial 3522 training loss: 0.054605526849627495\n",
      "epoch: 8 trial 3523 training loss: 0.01015419326722622\n",
      "epoch: 8 trial 3524 training loss: 0.04017476364970207\n",
      "epoch: 8 trial 3525 training loss: 0.01851245667785406\n",
      "epoch: 8 trial 3526 training loss: 0.12149783596396446\n",
      "epoch: 8 trial 3527 training loss: 0.016925394535064697\n",
      "epoch: 8 trial 3528 training loss: 0.05235309060662985\n",
      "epoch: 8 trial 3529 training loss: 0.03979291766881943\n",
      "epoch: 8 trial 3530 training loss: 0.030461715534329414\n",
      "epoch: 8 trial 3531 training loss: 0.031173388473689556\n",
      "epoch: 8 trial 3532 training loss: 0.08597993105649948\n",
      "epoch: 8 trial 3533 training loss: 0.01475869002752006\n",
      "epoch: 8 trial 3534 training loss: 0.01649477705359459\n",
      "epoch: 8 trial 3535 training loss: 0.10727955773472786\n",
      "epoch: 8 trial 3536 training loss: 0.018260375130921602\n",
      "epoch: 8 trial 3537 training loss: 0.0380959203466773\n",
      "epoch: 8 trial 3538 training loss: 0.05935545451939106\n",
      "epoch: 8 trial 3539 training loss: 0.03194813057780266\n",
      "epoch: 8 trial 3540 training loss: 0.029128098860383034\n",
      "epoch: 8 trial 3541 training loss: 0.014652980025857687\n",
      "epoch: 8 trial 3542 training loss: 0.037609503604471684\n",
      "epoch: 8 trial 3543 training loss: 0.00996266771107912\n",
      "epoch: 8 trial 3544 training loss: 0.009679074632003903\n",
      "epoch: 8 trial 3545 training loss: 0.022097788751125336\n",
      "epoch: 8 trial 3546 training loss: 0.0034262409899383783\n",
      "epoch: 8 trial 3547 training loss: 0.013314640615135431\n",
      "epoch: 8 trial 3548 training loss: 0.027004326693713665\n",
      "epoch: 8 trial 3549 training loss: 0.008293929509818554\n",
      "epoch: 8 trial 3550 training loss: 0.055823441594839096\n",
      "epoch: 8 trial 3551 training loss: 0.0038809161633253098\n",
      "epoch: 8 trial 3552 training loss: 0.055404182523489\n",
      "epoch: 8 trial 3553 training loss: 0.008798758266493678\n",
      "epoch: 8 trial 3554 training loss: 0.02389205852523446\n",
      "epoch: 8 trial 3555 training loss: 0.02861066162586212\n",
      "epoch: 8 trial 3556 training loss: 0.012135400203987956\n",
      "epoch: 8 trial 3557 training loss: 0.03552118595689535\n",
      "epoch: 8 trial 3558 training loss: 0.1377733163535595\n",
      "epoch: 8 trial 3559 training loss: 0.04526921920478344\n",
      "epoch: 8 trial 3560 training loss: 0.007418750319629908\n",
      "epoch: 8 trial 3561 training loss: 0.05393678788095713\n",
      "epoch: 8 trial 3562 training loss: 0.01774348458275199\n",
      "epoch: 8 trial 3563 training loss: 0.029123945627361536\n",
      "epoch: 8 trial 3564 training loss: 0.02093267161399126\n",
      "epoch: 8 trial 3565 training loss: 0.01702822372317314\n",
      "epoch: 8 trial 3566 training loss: 0.05075398087501526\n",
      "epoch: 8 trial 3567 training loss: 0.05134336929768324\n",
      "epoch: 8 trial 3568 training loss: 0.03957314230501652\n",
      "epoch: 8 trial 3569 training loss: 0.05229552183300257\n",
      "epoch: 8 trial 3570 training loss: 0.021192085463553667\n",
      "epoch: 8 trial 3571 training loss: 0.03810229059308767\n",
      "epoch: 8 trial 3572 training loss: 0.024433360435068607\n",
      "epoch: 8 trial 3573 training loss: 0.02024006238207221\n",
      "epoch: 8 trial 3574 training loss: 0.05228587705641985\n",
      "epoch: 8 trial 3575 training loss: 0.06123671121895313\n",
      "epoch: 8 trial 3576 training loss: 0.013709074817597866\n",
      "epoch: 8 trial 3577 training loss: 0.05787173472344875\n",
      "epoch: 8 trial 3578 training loss: 0.033025904558598995\n",
      "epoch: 8 trial 3579 training loss: 0.07090405747294426\n",
      "epoch: 8 trial 3580 training loss: 0.09450528584420681\n",
      "epoch: 8 trial 3581 training loss: 0.03914625849574804\n",
      "epoch: 8 trial 3582 training loss: 0.01902243308722973\n",
      "epoch: 8 trial 3583 training loss: 0.0528535321354866\n",
      "epoch: 8 trial 3584 training loss: 0.019524488132447004\n",
      "epoch: 8 trial 3585 training loss: 0.0212067817337811\n",
      "epoch: 8 trial 3586 training loss: 0.060523366555571556\n",
      "epoch: 8 trial 3587 training loss: 0.09478949196636677\n",
      "epoch: 8 trial 3588 training loss: 0.007734186947345734\n",
      "epoch: 8 trial 3589 training loss: 0.006740751443430781\n",
      "epoch: 8 trial 3590 training loss: 0.009676636662334204\n",
      "epoch: 8 trial 3591 training loss: 0.00972919911146164\n",
      "epoch: 8 trial 3592 training loss: 0.015968191903084517\n",
      "epoch: 8 trial 3593 training loss: 0.012100419262424111\n",
      "epoch: 8 trial 3594 training loss: 0.06998606584966183\n",
      "epoch: 8 trial 3595 training loss: 0.039581190794706345\n",
      "epoch: 8 trial 3596 training loss: 0.020916955079883337\n",
      "epoch: 8 trial 3597 training loss: 0.09517756849527359\n",
      "epoch: 8 trial 3598 training loss: 0.044962030835449696\n",
      "epoch: 8 trial 3599 training loss: 0.05955271888524294\n",
      "epoch: 8 trial 3600 training loss: 0.06157999485731125\n",
      "epoch: 8 trial 3601 training loss: 0.13417771458625793\n",
      "epoch: 8 trial 3602 training loss: 0.011509469011798501\n",
      "epoch: 8 trial 3603 training loss: 0.011804946465417743\n",
      "epoch: 8 trial 3604 training loss: 0.02454342506825924\n",
      "epoch: 8 trial 3605 training loss: 0.02128112828359008\n",
      "epoch: 8 trial 3606 training loss: 0.02047723112627864\n",
      "epoch: 8 trial 3607 training loss: 0.04456063359975815\n",
      "epoch: 8 trial 3608 training loss: 0.05437811091542244\n",
      "epoch: 8 trial 3609 training loss: 0.035412770695984364\n",
      "epoch: 8 trial 3610 training loss: 0.06511861830949783\n",
      "epoch: 8 trial 3611 training loss: 0.05252155661582947\n",
      "epoch: 8 trial 3612 training loss: 0.07590719871222973\n",
      "epoch: 8 trial 3613 training loss: 0.009980371687561274\n",
      "epoch: 8 trial 3614 training loss: 0.012407547561451793\n",
      "epoch: 8 trial 3615 training loss: 0.027377446182072163\n",
      "epoch: 8 trial 3616 training loss: 0.018187781795859337\n",
      "epoch: 8 trial 3617 training loss: 0.05160938762128353\n",
      "epoch: 8 trial 3618 training loss: 0.014417474623769522\n",
      "epoch: 8 trial 3619 training loss: 0.1460279170423746\n",
      "epoch: 8 trial 3620 training loss: 0.04468080494552851\n",
      "epoch: 8 trial 3621 training loss: 0.1307452656328678\n",
      "epoch: 8 trial 3622 training loss: 0.09008136205375195\n",
      "epoch: 8 trial 3623 training loss: 0.016633107559755445\n",
      "epoch: 8 trial 3624 training loss: 0.050161621533334255\n",
      "epoch: 8 trial 3625 training loss: 0.020520247984677553\n",
      "epoch: 8 trial 3626 training loss: 0.04255763906985521\n",
      "epoch: 8 trial 3627 training loss: 0.04621019307523966\n",
      "epoch: 8 trial 3628 training loss: 0.10062529146671295\n",
      "epoch: 8 trial 3629 training loss: 0.03328761179000139\n",
      "epoch: 8 trial 3630 training loss: 0.037263622507452965\n",
      "epoch: 8 trial 3631 training loss: 0.04668352659791708\n",
      "epoch: 8 trial 3632 training loss: 0.011082383804023266\n",
      "epoch: 8 trial 3633 training loss: 0.05393657460808754\n",
      "epoch: 8 trial 3634 training loss: 0.018487034365534782\n",
      "epoch: 8 trial 3635 training loss: 0.051710713654756546\n",
      "epoch: 8 trial 3636 training loss: 0.02826035674661398\n",
      "epoch: 8 trial 3637 training loss: 0.025640500709414482\n",
      "epoch: 8 trial 3638 training loss: 0.052416035905480385\n",
      "epoch: 8 trial 3639 training loss: 0.012370414566248655\n",
      "epoch: 8 trial 3640 training loss: 0.005168836796656251\n",
      "epoch: 8 trial 3641 training loss: 0.027571873739361763\n",
      "epoch: 8 trial 3642 training loss: 0.09149294346570969\n",
      "epoch: 8 trial 3643 training loss: 0.01998457033187151\n",
      "epoch: 8 trial 3644 training loss: 0.07951141335070133\n",
      "epoch: 8 trial 3645 training loss: 0.031164526008069515\n",
      "epoch: 8 trial 3646 training loss: 0.022755020763725042\n",
      "epoch: 8 trial 3647 training loss: 0.17562302574515343\n",
      "epoch: 8 trial 3648 training loss: 0.0123706110753119\n",
      "epoch: 8 trial 3649 training loss: 0.011278253747150302\n",
      "epoch: 8 trial 3650 training loss: 0.008304888266138732\n",
      "epoch: 8 trial 3651 training loss: 0.004221984068863094\n",
      "epoch: 8 trial 3652 training loss: 0.017472767271101475\n",
      "epoch: 8 trial 3653 training loss: 0.05600205436348915\n",
      "epoch: 8 trial 3654 training loss: 0.04178353399038315\n",
      "epoch: 8 trial 3655 training loss: 0.03177973907440901\n",
      "epoch: 8 trial 3656 training loss: 0.03272652067244053\n",
      "epoch: 8 trial 3657 training loss: 0.022949571255594492\n",
      "epoch: 8 trial 3658 training loss: 0.07373245246708393\n",
      "epoch: 8 trial 3659 training loss: 0.015418357215821743\n",
      "epoch: 8 trial 3660 training loss: 0.008872203528881073\n",
      "epoch: 8 trial 3661 training loss: 0.02268781140446663\n",
      "epoch: 8 trial 3662 training loss: 0.0065525248646736145\n",
      "epoch: 8 trial 3663 training loss: 0.06832355074584484\n",
      "epoch: 8 trial 3664 training loss: 0.005565782426856458\n",
      "epoch: 8 trial 3665 training loss: 0.02334649581462145\n",
      "epoch: 8 trial 3666 training loss: 0.01318136090412736\n",
      "epoch: 8 trial 3667 training loss: 0.06574395298957825\n",
      "epoch: 8 trial 3668 training loss: 0.020908291917294264\n",
      "epoch: 8 trial 3669 training loss: 0.021045177709311247\n",
      "epoch: 8 trial 3670 training loss: 0.055936725810170174\n",
      "epoch: 8 trial 3671 training loss: 0.08386002108454704\n",
      "epoch: 8 trial 3672 training loss: 0.03107956051826477\n",
      "epoch: 8 trial 3673 training loss: 0.07902594283223152\n",
      "epoch: 8 trial 3674 training loss: 0.021749035455286503\n",
      "epoch: 8 trial 3675 training loss: 0.08076983131468296\n",
      "epoch: 8 trial 3676 training loss: 0.01978504005819559\n",
      "epoch: 8 trial 3677 training loss: 0.013298511505126953\n",
      "epoch: 8 trial 3678 training loss: 0.05081698950380087\n",
      "epoch: 8 trial 3679 training loss: 0.029167989268898964\n",
      "epoch: 8 trial 3680 training loss: 0.017899360042065382\n",
      "epoch: 8 trial 3681 training loss: 0.016308961668983102\n",
      "epoch: 8 trial 3682 training loss: 0.009221514919772744\n",
      "epoch: 8 trial 3683 training loss: 0.025472788140177727\n",
      "epoch: 8 trial 3684 training loss: 0.03133891522884369\n",
      "epoch: 8 trial 3685 training loss: 0.018744695466011763\n",
      "epoch: 8 trial 3686 training loss: 0.03305518953129649\n",
      "epoch: 8 trial 3687 training loss: 0.07020368613302708\n",
      "epoch: 8 trial 3688 training loss: 0.03246291261166334\n",
      "epoch: 8 trial 3689 training loss: 0.0175077011808753\n",
      "epoch: 8 trial 3690 training loss: 0.006759257055819035\n",
      "epoch: 8 trial 3691 training loss: 0.013503438094630837\n",
      "epoch: 8 trial 3692 training loss: 0.03001859411597252\n",
      "epoch: 8 trial 3693 training loss: 0.056385419331490993\n",
      "epoch: 8 trial 3694 training loss: 0.04988196399062872\n",
      "epoch: 8 trial 3695 training loss: 0.002776537381578237\n",
      "epoch: 8 trial 3696 training loss: 0.022669395431876183\n",
      "epoch: 8 trial 3697 training loss: 0.03247232409194112\n",
      "epoch: 8 trial 3698 training loss: 0.021061827894300222\n",
      "epoch: 8 trial 3699 training loss: 0.014456395525485277\n",
      "epoch: 8 trial 3700 training loss: 0.020304568577557802\n",
      "epoch: 8 trial 3701 training loss: 0.041855757124722004\n",
      "epoch: 8 trial 3702 training loss: 0.02141659939661622\n",
      "epoch: 8 trial 3703 training loss: 0.03461324330419302\n",
      "epoch: 8 trial 3704 training loss: 0.05445570778101683\n",
      "epoch: 8 trial 3705 training loss: 0.026286426465958357\n",
      "epoch: 8 trial 3706 training loss: 0.03324499074369669\n",
      "epoch: 8 trial 3707 training loss: 0.06759812124073505\n",
      "epoch: 8 trial 3708 training loss: 0.03282239381223917\n",
      "epoch: 8 trial 3709 training loss: 0.06710236147046089\n",
      "epoch: 8 trial 3710 training loss: 0.017286826856434345\n",
      "epoch: 8 trial 3711 training loss: 0.032051981426775455\n",
      "epoch: 8 trial 3712 training loss: 0.026353861205279827\n",
      "epoch: 8 trial 3713 training loss: 0.02218047296628356\n",
      "epoch: 8 trial 3714 training loss: 0.015486997086554766\n",
      "epoch: 8 trial 3715 training loss: 0.0869486816227436\n",
      "epoch: 8 trial 3716 training loss: 0.012972084106877446\n",
      "epoch: 8 trial 3717 training loss: 0.09064105525612831\n",
      "epoch: 8 trial 3718 training loss: 0.023480544332414865\n",
      "epoch: 8 trial 3719 training loss: 0.007213532226160169\n",
      "epoch: 8 trial 3720 training loss: 0.0582546629011631\n",
      "epoch: 8 trial 3721 training loss: 0.059806566685438156\n",
      "epoch: 8 trial 3722 training loss: 0.010379833169281483\n",
      "epoch: 8 trial 3723 training loss: 0.03185259085148573\n",
      "epoch: 8 trial 3724 training loss: 0.008530548075214028\n",
      "epoch: 8 trial 3725 training loss: 0.033310205675661564\n",
      "epoch: 8 trial 3726 training loss: 0.022857670206576586\n",
      "epoch: 8 trial 3727 training loss: 0.033270999789237976\n",
      "epoch: 8 trial 3728 training loss: 0.2646208629012108\n",
      "epoch: 8 trial 3729 training loss: 0.05311485845595598\n",
      "epoch: 8 trial 3730 training loss: 0.008260068483650684\n",
      "epoch: 8 trial 3731 training loss: 0.02509349398314953\n",
      "epoch: 8 trial 3732 training loss: 0.13332784175872803\n",
      "epoch: 8 trial 3733 training loss: 0.04821986798197031\n",
      "epoch: 8 trial 3734 training loss: 0.06513701006770134\n",
      "epoch: 8 trial 3735 training loss: 0.031095256563276052\n",
      "epoch: 8 trial 3736 training loss: 0.036191340535879135\n",
      "epoch: 8 trial 3737 training loss: 0.015611339593306184\n",
      "epoch: 8 trial 3738 training loss: 0.09457384422421455\n",
      "epoch: 8 trial 3739 training loss: 0.01544177532196045\n",
      "epoch: 8 trial 3740 training loss: 0.07662181928753853\n",
      "epoch: 8 trial 3741 training loss: 0.025863656774163246\n",
      "epoch: 8 trial 3742 training loss: 0.010736538795754313\n",
      "epoch: 8 trial 3743 training loss: 0.04025932960212231\n",
      "epoch: 8 trial 3744 training loss: 0.005699712899513543\n",
      "epoch: 8 trial 3745 training loss: 0.019593023229390383\n",
      "epoch: 8 trial 3746 training loss: 0.09169195592403412\n",
      "epoch: 8 trial 3747 training loss: 0.07374418526887894\n",
      "epoch: 8 trial 3748 training loss: 0.024879938457161188\n",
      "epoch: 8 trial 3749 training loss: 0.1095561720430851\n",
      "epoch: 8 trial 3750 training loss: 0.09660611860454082\n",
      "epoch: 8 trial 3751 training loss: 0.0310342600569129\n",
      "epoch: 8 trial 3752 training loss: 0.06469140760600567\n",
      "epoch: 8 trial 3753 training loss: 0.04240189120173454\n",
      "epoch: 8 trial 3754 training loss: 0.014461661223322153\n",
      "epoch: 8 trial 3755 training loss: 0.005858136573806405\n",
      "epoch: 8 trial 3756 training loss: 0.15087038278579712\n",
      "epoch: 8 trial 3757 training loss: 0.07187929376959801\n",
      "epoch: 8 trial 3758 training loss: 0.05085525941103697\n",
      "epoch: 8 trial 3759 training loss: 0.055647024884819984\n",
      "epoch: 8 trial 3760 training loss: 0.02446984453126788\n",
      "epoch: 8 trial 3761 training loss: 0.029394431971013546\n",
      "epoch: 8 trial 3762 training loss: 0.018485625740140676\n",
      "epoch: 8 trial 3763 training loss: 0.045923659577965736\n",
      "epoch: 8 trial 3764 training loss: 0.03125703288242221\n",
      "epoch: 8 trial 3765 training loss: 0.03996289614588022\n",
      "epoch: 8 trial 3766 training loss: 0.08329707384109497\n",
      "epoch: 8 trial 3767 training loss: 0.0351431705057621\n",
      "epoch: 8 trial 3768 training loss: 0.01270459801889956\n",
      "epoch: 8 trial 3769 training loss: 0.03574735764414072\n",
      "epoch: 8 trial 3770 training loss: 0.004819328780286014\n",
      "epoch: 8 trial 3771 training loss: 0.04778201971203089\n",
      "epoch: 8 trial 3772 training loss: 0.03931472450494766\n",
      "epoch: 8 trial 3773 training loss: 0.015136390458792448\n",
      "epoch: 8 trial 3774 training loss: 0.0030945310136303306\n",
      "epoch: 8 trial 3775 training loss: 0.014439377933740616\n",
      "epoch: 8 trial 3776 training loss: 0.016131451819092035\n",
      "epoch: 8 trial 3777 training loss: 0.04815268889069557\n",
      "epoch: 8 trial 3778 training loss: 0.0070318072102963924\n",
      "epoch: 8 trial 3779 training loss: 0.0033638523309491575\n",
      "epoch: 8 trial 3780 training loss: 0.004804242169484496\n",
      "epoch: 8 trial 3781 training loss: 0.008503811433911324\n",
      "epoch: 8 trial 3782 training loss: 0.01288208831101656\n",
      "epoch: 8 trial 3783 training loss: 0.02992173470556736\n",
      "epoch: 8 trial 3784 training loss: 0.10645574890077114\n",
      "epoch: 8 trial 3785 training loss: 0.024937496054917574\n",
      "epoch: 8 trial 3786 training loss: 0.018956364132463932\n",
      "epoch: 8 trial 3787 training loss: 0.035022493451833725\n",
      "epoch: 8 trial 3788 training loss: 0.0390420276671648\n",
      "epoch: 8 trial 3789 training loss: 0.029847930651158094\n",
      "epoch: 8 trial 3790 training loss: 0.025713490322232246\n",
      "epoch: 8 trial 3791 training loss: 0.04616058338433504\n",
      "epoch: 8 trial 3792 training loss: 0.11454936861991882\n",
      "epoch: 8 trial 3793 training loss: 0.12620994821190834\n",
      "epoch: 8 trial 3794 training loss: 0.025269787292927504\n",
      "epoch: 8 trial 3795 training loss: 0.03303507901728153\n",
      "epoch: 8 trial 3796 training loss: 0.06689463369548321\n",
      "epoch: 8 trial 3797 training loss: 0.021254245191812515\n",
      "epoch: 8 trial 3798 training loss: 0.08841602690517902\n",
      "epoch: 8 trial 3799 training loss: 0.11432860046625137\n",
      "epoch: 8 trial 3800 training loss: 0.09010393545031548\n",
      "epoch: 8 trial 3801 training loss: 0.03086877567693591\n",
      "epoch: 8 trial 3802 training loss: 0.0907418429851532\n",
      "epoch: 8 trial 3803 training loss: 0.03492474928498268\n",
      "epoch: 8 trial 3804 training loss: 0.07361227087676525\n",
      "epoch: 8 trial 3805 training loss: 0.03576063830405474\n",
      "epoch: 8 trial 3806 training loss: 0.08258407935500145\n",
      "epoch: 8 trial 3807 training loss: 0.01282618660479784\n",
      "epoch: 8 trial 3808 training loss: 0.004516190965659916\n",
      "epoch: 8 trial 3809 training loss: 0.02156152483075857\n",
      "epoch: 8 trial 3810 training loss: 0.025054323486983776\n",
      "epoch: 8 trial 3811 training loss: 0.051625773310661316\n",
      "epoch: 8 trial 3812 training loss: 0.04646608140319586\n",
      "epoch: 8 trial 3813 training loss: 0.014847447630017996\n",
      "epoch: 8 trial 3814 training loss: 0.052370065823197365\n",
      "epoch: 8 trial 3815 training loss: 0.025578702799975872\n",
      "epoch: 8 trial 3816 training loss: 0.013571376912295818\n",
      "epoch: 8 trial 3817 training loss: 0.019925847183912992\n",
      "epoch: 8 trial 3818 training loss: 0.011844957014545798\n",
      "epoch: 8 trial 3819 training loss: 0.017507925163954496\n",
      "epoch: 8 trial 3820 training loss: 0.034593588672578335\n",
      "epoch: 8 trial 3821 training loss: 0.03256937023252249\n",
      "epoch: 8 trial 3822 training loss: 0.026060618460178375\n",
      "epoch: 8 trial 3823 training loss: 0.019494376610964537\n",
      "epoch: 8 trial 3824 training loss: 0.01607626397162676\n",
      "epoch: 8 trial 3825 training loss: 0.0099891796708107\n",
      "epoch: 8 trial 3826 training loss: 0.01629278203472495\n",
      "epoch: 8 trial 3827 training loss: 0.014935412211343646\n",
      "epoch: 8 trial 3828 training loss: 0.016516019124537706\n",
      "epoch: 8 trial 3829 training loss: 0.005990738747641444\n",
      "epoch: 8 trial 3830 training loss: 0.014607018791139126\n",
      "epoch: 8 trial 3831 training loss: 0.08812708966434002\n",
      "epoch: 8 trial 3832 training loss: 0.013829125557094812\n",
      "epoch: 8 trial 3833 training loss: 0.0327723752707243\n",
      "epoch: 8 trial 3834 training loss: 0.013251018477603793\n",
      "epoch: 8 trial 3835 training loss: 0.048411025665700436\n",
      "epoch: 8 trial 3836 training loss: 0.03923450130969286\n",
      "epoch: 8 trial 3837 training loss: 0.040241679176688194\n",
      "epoch: 8 trial 3838 training loss: 0.026478286366909742\n",
      "epoch: 8 trial 3839 training loss: 0.017286913469433784\n",
      "epoch: 8 trial 3840 training loss: 0.11106115952134132\n",
      "epoch: 8 trial 3841 training loss: 0.005839626304805279\n",
      "epoch: 8 trial 3842 training loss: 0.017637117765843868\n",
      "epoch: 8 trial 3843 training loss: 0.13053370639681816\n",
      "epoch: 8 trial 3844 training loss: 0.026823941618204117\n",
      "epoch: 8 trial 3845 training loss: 0.022436488419771194\n",
      "epoch: 8 trial 3846 training loss: 0.019168990664184093\n",
      "epoch: 8 trial 3847 training loss: 0.011924360878765583\n",
      "epoch: 8 trial 3848 training loss: 0.024795995093882084\n",
      "epoch: 8 trial 3849 training loss: 0.014248829567804933\n",
      "epoch: 8 trial 3850 training loss: 0.04059955012053251\n",
      "epoch: 8 trial 3851 training loss: 0.16592023149132729\n",
      "epoch: 8 trial 3852 training loss: 0.07171542197465897\n",
      "epoch: 8 trial 3853 training loss: 0.02602746430784464\n",
      "epoch: 8 trial 3854 training loss: 0.0970651376992464\n",
      "epoch: 8 trial 3855 training loss: 0.11212760210037231\n",
      "epoch: 8 trial 3856 training loss: 0.023398011922836304\n",
      "epoch: 8 trial 3857 training loss: 0.07265128288418055\n",
      "epoch: 8 trial 3858 training loss: 0.007035369519144297\n",
      "epoch: 8 trial 3859 training loss: 0.05348740704357624\n",
      "epoch: 8 trial 3860 training loss: 0.0131092369556427\n",
      "epoch: 8 trial 3861 training loss: 0.027067064307630062\n",
      "epoch: 8 trial 3862 training loss: 0.1108376756310463\n",
      "epoch: 8 trial 3863 training loss: 0.03990607336163521\n",
      "epoch: 8 trial 3864 training loss: 0.01566559774801135\n",
      "epoch: 8 trial 3865 training loss: 0.024259100668132305\n",
      "epoch: 8 trial 3866 training loss: 0.08044776879251003\n",
      "epoch: 8 trial 3867 training loss: 0.010289705125615\n",
      "epoch: 8 trial 3868 training loss: 0.05195343308150768\n",
      "epoch: 8 trial 3869 training loss: 0.03533317893743515\n",
      "epoch: 8 trial 3870 training loss: 0.04111192189157009\n",
      "epoch: 8 trial 3871 training loss: 0.01418206188827753\n",
      "epoch: 8 trial 3872 training loss: 0.05053706839680672\n",
      "epoch: 9 trial 3873 training loss: 0.012066296534612775\n",
      "epoch: 9 trial 3874 training loss: 0.014650599099695683\n",
      "epoch: 9 trial 3875 training loss: 0.015186016680672765\n",
      "epoch: 9 trial 3876 training loss: 0.06309442780911922\n",
      "epoch: 9 trial 3877 training loss: 0.004169301828369498\n",
      "epoch: 9 trial 3878 training loss: 0.005441747140139341\n",
      "epoch: 9 trial 3879 training loss: 0.027350139804184437\n",
      "epoch: 9 trial 3880 training loss: 0.017058327328413725\n",
      "epoch: 9 trial 3881 training loss: 0.007022433215752244\n",
      "epoch: 9 trial 3882 training loss: 0.016236307099461555\n",
      "epoch: 9 trial 3883 training loss: 0.0467188423499465\n",
      "epoch: 9 trial 3884 training loss: 0.029055679216980934\n",
      "epoch: 9 trial 3885 training loss: 0.018649477511644363\n",
      "epoch: 9 trial 3886 training loss: 0.06133984215557575\n",
      "epoch: 9 trial 3887 training loss: 0.014748106710612774\n",
      "epoch: 9 trial 3888 training loss: 0.022082017734646797\n",
      "epoch: 9 trial 3889 training loss: 0.13150574639439583\n",
      "epoch: 9 trial 3890 training loss: 0.05149959214031696\n",
      "epoch: 9 trial 3891 training loss: 0.012641276931390166\n",
      "epoch: 9 trial 3892 training loss: 0.01861609797924757\n",
      "epoch: 9 trial 3893 training loss: 0.01721166167408228\n",
      "epoch: 9 trial 3894 training loss: 0.0758877545595169\n",
      "epoch: 9 trial 3895 training loss: 0.009199924301356077\n",
      "epoch: 9 trial 3896 training loss: 0.00551855587400496\n",
      "epoch: 9 trial 3897 training loss: 0.1521463356912136\n",
      "epoch: 9 trial 3898 training loss: 0.02333797561004758\n",
      "epoch: 9 trial 3899 training loss: 0.007617331575602293\n",
      "epoch: 9 trial 3900 training loss: 0.11510948091745377\n",
      "epoch: 9 trial 3901 training loss: 0.028452456928789616\n",
      "epoch: 9 trial 3902 training loss: 0.019825054798275232\n",
      "epoch: 9 trial 3903 training loss: 0.030079664196819067\n",
      "epoch: 9 trial 3904 training loss: 0.014972675126045942\n",
      "epoch: 9 trial 3905 training loss: 0.03325995244085789\n",
      "epoch: 9 trial 3906 training loss: 0.049454870633780956\n",
      "epoch: 9 trial 3907 training loss: 0.01786747993901372\n",
      "epoch: 9 trial 3908 training loss: 0.06990964151918888\n",
      "epoch: 9 trial 3909 training loss: 0.14473828673362732\n",
      "epoch: 9 trial 3910 training loss: 0.3346553146839142\n",
      "epoch: 9 trial 3911 training loss: 0.032546793576329947\n",
      "epoch: 9 trial 3912 training loss: 0.10241127014160156\n",
      "epoch: 9 trial 3913 training loss: 0.12319215014576912\n",
      "epoch: 9 trial 3914 training loss: 0.015079443342983723\n",
      "epoch: 9 trial 3915 training loss: 0.09701439179480076\n",
      "epoch: 9 trial 3916 training loss: 0.04196520894765854\n",
      "epoch: 9 trial 3917 training loss: 0.028094005770981312\n",
      "epoch: 9 trial 3918 training loss: 0.055062842555344105\n",
      "epoch: 9 trial 3919 training loss: 0.014937116764485836\n",
      "epoch: 9 trial 3920 training loss: 0.020672201178967953\n",
      "epoch: 9 trial 3921 training loss: 0.01108985161408782\n",
      "epoch: 9 trial 3922 training loss: 0.08978250063955784\n",
      "epoch: 9 trial 3923 training loss: 0.03436190169304609\n",
      "epoch: 9 trial 3924 training loss: 0.10451298579573631\n",
      "epoch: 9 trial 3925 training loss: 0.04318923503160477\n",
      "epoch: 9 trial 3926 training loss: 0.0407720934599638\n",
      "epoch: 9 trial 3927 training loss: 0.029050719924271107\n",
      "epoch: 9 trial 3928 training loss: 0.06518695689737797\n",
      "epoch: 9 trial 3929 training loss: 0.02407302660867572\n",
      "epoch: 9 trial 3930 training loss: 0.019868182484060526\n",
      "epoch: 9 trial 3931 training loss: 0.024465820752084255\n",
      "epoch: 9 trial 3932 training loss: 0.0884503647685051\n",
      "epoch: 9 trial 3933 training loss: 0.014063707552850246\n",
      "epoch: 9 trial 3934 training loss: 0.10934308357536793\n",
      "epoch: 9 trial 3935 training loss: 0.02399589866399765\n",
      "epoch: 9 trial 3936 training loss: 0.0060393246822059155\n",
      "epoch: 9 trial 3937 training loss: 0.032224259339272976\n",
      "epoch: 9 trial 3938 training loss: 0.04719094559550285\n",
      "epoch: 9 trial 3939 training loss: 0.02036269335076213\n",
      "epoch: 9 trial 3940 training loss: 0.02072589797899127\n",
      "epoch: 9 trial 3941 training loss: 0.04785330034792423\n",
      "epoch: 9 trial 3942 training loss: 0.03335540369153023\n",
      "epoch: 9 trial 3943 training loss: 0.04517385549843311\n",
      "epoch: 9 trial 3944 training loss: 0.031808994710445404\n",
      "epoch: 9 trial 3945 training loss: 0.015332999173551798\n",
      "epoch: 9 trial 3946 training loss: 0.06995409168303013\n",
      "epoch: 9 trial 3947 training loss: 0.013271975796669722\n",
      "epoch: 9 trial 3948 training loss: 0.023695049341768026\n",
      "epoch: 9 trial 3949 training loss: 0.02570775244385004\n",
      "epoch: 9 trial 3950 training loss: 0.08958278223872185\n",
      "epoch: 9 trial 3951 training loss: 0.019905549474060535\n",
      "epoch: 9 trial 3952 training loss: 0.06145052798092365\n",
      "epoch: 9 trial 3953 training loss: 0.027194521855562925\n",
      "epoch: 9 trial 3954 training loss: 0.041857064701616764\n",
      "epoch: 9 trial 3955 training loss: 0.06652768142521381\n",
      "epoch: 9 trial 3956 training loss: 0.01797285722568631\n",
      "epoch: 9 trial 3957 training loss: 0.057504309341311455\n",
      "epoch: 9 trial 3958 training loss: 0.014461683109402657\n",
      "epoch: 9 trial 3959 training loss: 0.03501470107585192\n",
      "epoch: 9 trial 3960 training loss: 0.0830904021859169\n",
      "epoch: 9 trial 3961 training loss: 0.04814810864627361\n",
      "epoch: 9 trial 3962 training loss: 0.005897094262763858\n",
      "epoch: 9 trial 3963 training loss: 0.0373866343870759\n",
      "epoch: 9 trial 3964 training loss: 0.026701223105192184\n",
      "epoch: 9 trial 3965 training loss: 0.02007888862863183\n",
      "epoch: 9 trial 3966 training loss: 0.06323430128395557\n",
      "epoch: 9 trial 3967 training loss: 0.04515455476939678\n",
      "epoch: 9 trial 3968 training loss: 0.053109556436538696\n",
      "epoch: 9 trial 3969 training loss: 0.035877387039363384\n",
      "epoch: 9 trial 3970 training loss: 0.09019165858626366\n",
      "epoch: 9 trial 3971 training loss: 0.042796255089342594\n",
      "epoch: 9 trial 3972 training loss: 0.036785040982067585\n",
      "epoch: 9 trial 3973 training loss: 0.04293662868440151\n",
      "epoch: 9 trial 3974 training loss: 0.0945473276078701\n",
      "epoch: 9 trial 3975 training loss: 0.045594122260808945\n",
      "epoch: 9 trial 3976 training loss: 0.03587158117443323\n",
      "epoch: 9 trial 3977 training loss: 0.2763008177280426\n",
      "epoch: 9 trial 3978 training loss: 0.04105433635413647\n",
      "epoch: 9 trial 3979 training loss: 0.02983298897743225\n",
      "epoch: 9 trial 3980 training loss: 0.14971103891730309\n",
      "epoch: 9 trial 3981 training loss: 0.09644724056124687\n",
      "epoch: 9 trial 3982 training loss: 0.025567370001226664\n",
      "epoch: 9 trial 3983 training loss: 0.01914922147989273\n",
      "epoch: 9 trial 3984 training loss: 0.02044819388538599\n",
      "epoch: 9 trial 3985 training loss: 0.011302255792543292\n",
      "epoch: 9 trial 3986 training loss: 0.008273093611933291\n",
      "epoch: 9 trial 3987 training loss: 0.060347966849803925\n",
      "epoch: 9 trial 3988 training loss: 0.017388762906193733\n",
      "epoch: 9 trial 3989 training loss: 0.02643228881061077\n",
      "epoch: 9 trial 3990 training loss: 0.04222292359918356\n",
      "epoch: 9 trial 3991 training loss: 0.03249604720622301\n",
      "epoch: 9 trial 3992 training loss: 0.02695731446146965\n",
      "epoch: 9 trial 3993 training loss: 0.016780156642198563\n",
      "epoch: 9 trial 3994 training loss: 0.0488637275993824\n",
      "epoch: 9 trial 3995 training loss: 0.07923739217221737\n",
      "epoch: 9 trial 3996 training loss: 0.040835339576005936\n",
      "epoch: 9 trial 3997 training loss: 0.009798881597816944\n",
      "epoch: 9 trial 3998 training loss: 0.021903808694332838\n",
      "epoch: 9 trial 3999 training loss: 0.02022740012034774\n",
      "epoch: 9 trial 4000 training loss: 0.026149705983698368\n",
      "epoch: 9 trial 4001 training loss: 0.15776252001523972\n",
      "epoch: 9 trial 4002 training loss: 0.03220484033226967\n",
      "epoch: 9 trial 4003 training loss: 0.069403026252985\n",
      "epoch: 9 trial 4004 training loss: 0.03086330695077777\n",
      "epoch: 9 trial 4005 training loss: 0.033759474754333496\n",
      "epoch: 9 trial 4006 training loss: 0.049959901720285416\n",
      "epoch: 9 trial 4007 training loss: 0.008488403633236885\n",
      "epoch: 9 trial 4008 training loss: 0.04170831572264433\n",
      "epoch: 9 trial 4009 training loss: 0.01789089571684599\n",
      "epoch: 9 trial 4010 training loss: 0.11666598916053772\n",
      "epoch: 9 trial 4011 training loss: 0.015878597274422646\n",
      "epoch: 9 trial 4012 training loss: 0.0688224658370018\n",
      "epoch: 9 trial 4013 training loss: 0.03571720514446497\n",
      "epoch: 9 trial 4014 training loss: 0.030075049959123135\n",
      "epoch: 9 trial 4015 training loss: 0.029924745205789804\n",
      "epoch: 9 trial 4016 training loss: 0.09980768710374832\n",
      "epoch: 9 trial 4017 training loss: 0.013464351650327444\n",
      "epoch: 9 trial 4018 training loss: 0.017182929906994104\n",
      "epoch: 9 trial 4019 training loss: 0.1104087233543396\n",
      "epoch: 9 trial 4020 training loss: 0.018027833197265863\n",
      "epoch: 9 trial 4021 training loss: 0.03770911134779453\n",
      "epoch: 9 trial 4022 training loss: 0.060265250504016876\n",
      "epoch: 9 trial 4023 training loss: 0.030060661025345325\n",
      "epoch: 9 trial 4024 training loss: 0.025316317100077868\n",
      "epoch: 9 trial 4025 training loss: 0.016912461258471012\n",
      "epoch: 9 trial 4026 training loss: 0.03554735891520977\n",
      "epoch: 9 trial 4027 training loss: 0.010751441586762667\n",
      "epoch: 9 trial 4028 training loss: 0.009800741914659739\n",
      "epoch: 9 trial 4029 training loss: 0.02628077520057559\n",
      "epoch: 9 trial 4030 training loss: 0.003947564284317195\n",
      "epoch: 9 trial 4031 training loss: 0.01122259022668004\n",
      "epoch: 9 trial 4032 training loss: 0.027247854508459568\n",
      "epoch: 9 trial 4033 training loss: 0.00885222153738141\n",
      "epoch: 9 trial 4034 training loss: 0.057594865560531616\n",
      "epoch: 9 trial 4035 training loss: 0.0033276055473834276\n",
      "epoch: 9 trial 4036 training loss: 0.05618789978325367\n",
      "epoch: 9 trial 4037 training loss: 0.006534127285704017\n",
      "epoch: 9 trial 4038 training loss: 0.02418043091893196\n",
      "epoch: 9 trial 4039 training loss: 0.02687431126832962\n",
      "epoch: 9 trial 4040 training loss: 0.010385380359366536\n",
      "epoch: 9 trial 4041 training loss: 0.03788070008158684\n",
      "epoch: 9 trial 4042 training loss: 0.13713634759187698\n",
      "epoch: 9 trial 4043 training loss: 0.04877448081970215\n",
      "epoch: 9 trial 4044 training loss: 0.006825618795119226\n",
      "epoch: 9 trial 4045 training loss: 0.05327608995139599\n",
      "epoch: 9 trial 4046 training loss: 0.020116910338401794\n",
      "epoch: 9 trial 4047 training loss: 0.021458688657730818\n",
      "epoch: 9 trial 4048 training loss: 0.019742604810744524\n",
      "epoch: 9 trial 4049 training loss: 0.017822462134063244\n",
      "epoch: 9 trial 4050 training loss: 0.05998799949884415\n",
      "epoch: 9 trial 4051 training loss: 0.0562190106138587\n",
      "epoch: 9 trial 4052 training loss: 0.038683442398905754\n",
      "epoch: 9 trial 4053 training loss: 0.05927950795739889\n",
      "epoch: 9 trial 4054 training loss: 0.022709687240421772\n",
      "epoch: 9 trial 4055 training loss: 0.040418749675154686\n",
      "epoch: 9 trial 4056 training loss: 0.02493255166336894\n",
      "epoch: 9 trial 4057 training loss: 0.018282342236489058\n",
      "epoch: 9 trial 4058 training loss: 0.04595115873962641\n",
      "epoch: 9 trial 4059 training loss: 0.07134775072336197\n",
      "epoch: 9 trial 4060 training loss: 0.015514649450778961\n",
      "epoch: 9 trial 4061 training loss: 0.04545667767524719\n",
      "epoch: 9 trial 4062 training loss: 0.04141146130859852\n",
      "epoch: 9 trial 4063 training loss: 0.06306794285774231\n",
      "epoch: 9 trial 4064 training loss: 0.07008260115981102\n",
      "epoch: 9 trial 4065 training loss: 0.03531034290790558\n",
      "epoch: 9 trial 4066 training loss: 0.02220090664923191\n",
      "epoch: 9 trial 4067 training loss: 0.04834612365812063\n",
      "epoch: 9 trial 4068 training loss: 0.023206281010061502\n",
      "epoch: 9 trial 4069 training loss: 0.02223648549988866\n",
      "epoch: 9 trial 4070 training loss: 0.059378392063081264\n",
      "epoch: 9 trial 4071 training loss: 0.0956245418637991\n",
      "epoch: 9 trial 4072 training loss: 0.011207102565094829\n",
      "epoch: 9 trial 4073 training loss: 0.006504786433652043\n",
      "epoch: 9 trial 4074 training loss: 0.009553631767630577\n",
      "epoch: 9 trial 4075 training loss: 0.009396835463121533\n",
      "epoch: 9 trial 4076 training loss: 0.01697368733584881\n",
      "epoch: 9 trial 4077 training loss: 0.014049363089725375\n",
      "epoch: 9 trial 4078 training loss: 0.06700966879725456\n",
      "epoch: 9 trial 4079 training loss: 0.0369234848767519\n",
      "epoch: 9 trial 4080 training loss: 0.01823936216533184\n",
      "epoch: 9 trial 4081 training loss: 0.10158855840563774\n",
      "epoch: 9 trial 4082 training loss: 0.0502079539000988\n",
      "epoch: 9 trial 4083 training loss: 0.06292680464684963\n",
      "epoch: 9 trial 4084 training loss: 0.06151381693780422\n",
      "epoch: 9 trial 4085 training loss: 0.11636249907314777\n",
      "epoch: 9 trial 4086 training loss: 0.012671603122726083\n",
      "epoch: 9 trial 4087 training loss: 0.008217802969738841\n",
      "epoch: 9 trial 4088 training loss: 0.0286624813452363\n",
      "epoch: 9 trial 4089 training loss: 0.022709710989147425\n",
      "epoch: 9 trial 4090 training loss: 0.0201130174100399\n",
      "epoch: 9 trial 4091 training loss: 0.05293600633740425\n",
      "epoch: 9 trial 4092 training loss: 0.06115489453077316\n",
      "epoch: 9 trial 4093 training loss: 0.03509226627647877\n",
      "epoch: 9 trial 4094 training loss: 0.06720215640962124\n",
      "epoch: 9 trial 4095 training loss: 0.04593952465802431\n",
      "epoch: 9 trial 4096 training loss: 0.08760974369943142\n",
      "epoch: 9 trial 4097 training loss: 0.011622858699411154\n",
      "epoch: 9 trial 4098 training loss: 0.011065938277170062\n",
      "epoch: 9 trial 4099 training loss: 0.025962473824620247\n",
      "epoch: 9 trial 4100 training loss: 0.021187073551118374\n",
      "epoch: 9 trial 4101 training loss: 0.048435348086059093\n",
      "epoch: 9 trial 4102 training loss: 0.015623604413121939\n",
      "epoch: 9 trial 4103 training loss: 0.16051235795021057\n",
      "epoch: 9 trial 4104 training loss: 0.03957945667207241\n",
      "epoch: 9 trial 4105 training loss: 0.12836069613695145\n",
      "epoch: 9 trial 4106 training loss: 0.09859990328550339\n",
      "epoch: 9 trial 4107 training loss: 0.018723064102232456\n",
      "epoch: 9 trial 4108 training loss: 0.05783442594110966\n",
      "epoch: 9 trial 4109 training loss: 0.017930381931364536\n",
      "epoch: 9 trial 4110 training loss: 0.04489438980817795\n",
      "epoch: 9 trial 4111 training loss: 0.04954726994037628\n",
      "epoch: 9 trial 4112 training loss: 0.10331908985972404\n",
      "epoch: 9 trial 4113 training loss: 0.026605278719216585\n",
      "epoch: 9 trial 4114 training loss: 0.03591259801760316\n",
      "epoch: 9 trial 4115 training loss: 0.039723269641399384\n",
      "epoch: 9 trial 4116 training loss: 0.016020855866372585\n",
      "epoch: 9 trial 4117 training loss: 0.05972248315811157\n",
      "epoch: 9 trial 4118 training loss: 0.01966469967737794\n",
      "epoch: 9 trial 4119 training loss: 0.05220417212694883\n",
      "epoch: 9 trial 4120 training loss: 0.02661366295069456\n",
      "epoch: 9 trial 4121 training loss: 0.026360925752669573\n",
      "epoch: 9 trial 4122 training loss: 0.05074104852974415\n",
      "epoch: 9 trial 4123 training loss: 0.012638806365430355\n",
      "epoch: 9 trial 4124 training loss: 0.004852607729844749\n",
      "epoch: 9 trial 4125 training loss: 0.027413057163357735\n",
      "epoch: 9 trial 4126 training loss: 0.08858256414532661\n",
      "epoch: 9 trial 4127 training loss: 0.018477005884051323\n",
      "epoch: 9 trial 4128 training loss: 0.08220811747014523\n",
      "epoch: 9 trial 4129 training loss: 0.03673534747213125\n",
      "epoch: 9 trial 4130 training loss: 0.021351155824959278\n",
      "epoch: 9 trial 4131 training loss: 0.1730276569724083\n",
      "epoch: 9 trial 4132 training loss: 0.014143592212349176\n",
      "epoch: 9 trial 4133 training loss: 0.01352602243423462\n",
      "epoch: 9 trial 4134 training loss: 0.009463621769100428\n",
      "epoch: 9 trial 4135 training loss: 0.003509117872454226\n",
      "epoch: 9 trial 4136 training loss: 0.024090837687253952\n",
      "epoch: 9 trial 4137 training loss: 0.05114810913801193\n",
      "epoch: 9 trial 4138 training loss: 0.04005041532218456\n",
      "epoch: 9 trial 4139 training loss: 0.03314989525824785\n",
      "epoch: 9 trial 4140 training loss: 0.03440302051603794\n",
      "epoch: 9 trial 4141 training loss: 0.023638134822249413\n",
      "epoch: 9 trial 4142 training loss: 0.07620939519256353\n",
      "epoch: 9 trial 4143 training loss: 0.01633150177076459\n",
      "epoch: 9 trial 4144 training loss: 0.008147941436618567\n",
      "epoch: 9 trial 4145 training loss: 0.023176119197160006\n",
      "epoch: 9 trial 4146 training loss: 0.006822199793532491\n",
      "epoch: 9 trial 4147 training loss: 0.06992398761212826\n",
      "epoch: 9 trial 4148 training loss: 0.004371924442239106\n",
      "epoch: 9 trial 4149 training loss: 0.02461784752085805\n",
      "epoch: 9 trial 4150 training loss: 0.012448523892089725\n",
      "epoch: 9 trial 4151 training loss: 0.06077805534005165\n",
      "epoch: 9 trial 4152 training loss: 0.023341642692685127\n",
      "epoch: 9 trial 4153 training loss: 0.02426488045603037\n",
      "epoch: 9 trial 4154 training loss: 0.05264048743993044\n",
      "epoch: 9 trial 4155 training loss: 0.06827596016228199\n",
      "epoch: 9 trial 4156 training loss: 0.03216215316206217\n",
      "epoch: 9 trial 4157 training loss: 0.06750557944178581\n",
      "epoch: 9 trial 4158 training loss: 0.021001663990318775\n",
      "epoch: 9 trial 4159 training loss: 0.08393682539463043\n",
      "epoch: 9 trial 4160 training loss: 0.01974799670279026\n",
      "epoch: 9 trial 4161 training loss: 0.009877400239929557\n",
      "epoch: 9 trial 4162 training loss: 0.05112431012094021\n",
      "epoch: 9 trial 4163 training loss: 0.030713730491697788\n",
      "epoch: 9 trial 4164 training loss: 0.017305871937423944\n",
      "epoch: 9 trial 4165 training loss: 0.01694748387672007\n",
      "epoch: 9 trial 4166 training loss: 0.009463280905038118\n",
      "epoch: 9 trial 4167 training loss: 0.02198731666430831\n",
      "epoch: 9 trial 4168 training loss: 0.031169289723038673\n",
      "epoch: 9 trial 4169 training loss: 0.01693340903148055\n",
      "epoch: 9 trial 4170 training loss: 0.03311212453991175\n",
      "epoch: 9 trial 4171 training loss: 0.0928436852991581\n",
      "epoch: 9 trial 4172 training loss: 0.033847990445792675\n",
      "epoch: 9 trial 4173 training loss: 0.0161647223867476\n",
      "epoch: 9 trial 4174 training loss: 0.006180041120387614\n",
      "epoch: 9 trial 4175 training loss: 0.009304673410952091\n",
      "epoch: 9 trial 4176 training loss: 0.02727009542286396\n",
      "epoch: 9 trial 4177 training loss: 0.05379518959671259\n",
      "epoch: 9 trial 4178 training loss: 0.04807771369814873\n",
      "epoch: 9 trial 4179 training loss: 0.002798325615003705\n",
      "epoch: 9 trial 4180 training loss: 0.032825419679284096\n",
      "epoch: 9 trial 4181 training loss: 0.04641474038362503\n",
      "epoch: 9 trial 4182 training loss: 0.022035899106413126\n",
      "epoch: 9 trial 4183 training loss: 0.012427713023498654\n",
      "epoch: 9 trial 4184 training loss: 0.018137493170797825\n",
      "epoch: 9 trial 4185 training loss: 0.03796607907861471\n",
      "epoch: 9 trial 4186 training loss: 0.02182404650375247\n",
      "epoch: 9 trial 4187 training loss: 0.03419200424104929\n",
      "epoch: 9 trial 4188 training loss: 0.06269006058573723\n",
      "epoch: 9 trial 4189 training loss: 0.027977060060948133\n",
      "epoch: 9 trial 4190 training loss: 0.03587855584919453\n",
      "epoch: 9 trial 4191 training loss: 0.0700997095555067\n",
      "epoch: 9 trial 4192 training loss: 0.03449153155088425\n",
      "epoch: 9 trial 4193 training loss: 0.06690051220357418\n",
      "epoch: 9 trial 4194 training loss: 0.01830597035586834\n",
      "epoch: 9 trial 4195 training loss: 0.0298507921397686\n",
      "epoch: 9 trial 4196 training loss: 0.02640688791871071\n",
      "epoch: 9 trial 4197 training loss: 0.022683636751025915\n",
      "epoch: 9 trial 4198 training loss: 0.01764304470270872\n",
      "epoch: 9 trial 4199 training loss: 0.0880410224199295\n",
      "epoch: 9 trial 4200 training loss: 0.012583899311721325\n",
      "epoch: 9 trial 4201 training loss: 0.08419311419129372\n",
      "epoch: 9 trial 4202 training loss: 0.023552363738417625\n",
      "epoch: 9 trial 4203 training loss: 0.007132583297789097\n",
      "epoch: 9 trial 4204 training loss: 0.058011436834931374\n",
      "epoch: 9 trial 4205 training loss: 0.06458068452775478\n",
      "epoch: 9 trial 4206 training loss: 0.009513208642601967\n",
      "epoch: 9 trial 4207 training loss: 0.03283785469830036\n",
      "epoch: 9 trial 4208 training loss: 0.008148953784257174\n",
      "epoch: 9 trial 4209 training loss: 0.03333393391221762\n",
      "epoch: 9 trial 4210 training loss: 0.02088194712996483\n",
      "epoch: 9 trial 4211 training loss: 0.03420297708362341\n",
      "epoch: 9 trial 4212 training loss: 0.26412204653024673\n",
      "epoch: 9 trial 4213 training loss: 0.050097743049263954\n",
      "epoch: 9 trial 4214 training loss: 0.007860582787543535\n",
      "epoch: 9 trial 4215 training loss: 0.022165100555866957\n",
      "epoch: 9 trial 4216 training loss: 0.15382355079054832\n",
      "epoch: 9 trial 4217 training loss: 0.04934728145599365\n",
      "epoch: 9 trial 4218 training loss: 0.06826789490878582\n",
      "epoch: 9 trial 4219 training loss: 0.029360486660152674\n",
      "epoch: 9 trial 4220 training loss: 0.0399428540840745\n",
      "epoch: 9 trial 4221 training loss: 0.015976468566805124\n",
      "epoch: 9 trial 4222 training loss: 0.07984587363898754\n",
      "epoch: 9 trial 4223 training loss: 0.016386539209634066\n",
      "epoch: 9 trial 4224 training loss: 0.07533365860581398\n",
      "epoch: 9 trial 4225 training loss: 0.020174693316221237\n",
      "epoch: 9 trial 4226 training loss: 0.00877693179063499\n",
      "epoch: 9 trial 4227 training loss: 0.032155536115169525\n",
      "epoch: 9 trial 4228 training loss: 0.005155403981916606\n",
      "epoch: 9 trial 4229 training loss: 0.0174650140106678\n",
      "epoch: 9 trial 4230 training loss: 0.08559691719710827\n",
      "epoch: 9 trial 4231 training loss: 0.07295345701277256\n",
      "epoch: 9 trial 4232 training loss: 0.02519919816404581\n",
      "epoch: 9 trial 4233 training loss: 0.11235302686691284\n",
      "epoch: 9 trial 4234 training loss: 0.09171012789011002\n",
      "epoch: 9 trial 4235 training loss: 0.030619055032730103\n",
      "epoch: 9 trial 4236 training loss: 0.0640358105301857\n",
      "epoch: 9 trial 4237 training loss: 0.04315312020480633\n",
      "epoch: 9 trial 4238 training loss: 0.012690042611211538\n",
      "epoch: 9 trial 4239 training loss: 0.007356340065598488\n",
      "epoch: 9 trial 4240 training loss: 0.15957067161798477\n",
      "epoch: 9 trial 4241 training loss: 0.06133396737277508\n",
      "epoch: 9 trial 4242 training loss: 0.04277642071247101\n",
      "epoch: 9 trial 4243 training loss: 0.05447826907038689\n",
      "epoch: 9 trial 4244 training loss: 0.025279204826802015\n",
      "epoch: 9 trial 4245 training loss: 0.030645436141639948\n",
      "epoch: 9 trial 4246 training loss: 0.0178629863075912\n",
      "epoch: 9 trial 4247 training loss: 0.045412507839500904\n",
      "epoch: 9 trial 4248 training loss: 0.03458098974078894\n",
      "epoch: 9 trial 4249 training loss: 0.04483342356979847\n",
      "epoch: 9 trial 4250 training loss: 0.08600549586117268\n",
      "epoch: 9 trial 4251 training loss: 0.03209124831482768\n",
      "epoch: 9 trial 4252 training loss: 0.012719412799924612\n",
      "epoch: 9 trial 4253 training loss: 0.03471582289785147\n",
      "epoch: 9 trial 4254 training loss: 0.003408908611163497\n",
      "epoch: 9 trial 4255 training loss: 0.04917972907423973\n",
      "epoch: 9 trial 4256 training loss: 0.0414703581482172\n",
      "epoch: 9 trial 4257 training loss: 0.015946047380566597\n",
      "epoch: 9 trial 4258 training loss: 0.003359089372679591\n",
      "epoch: 9 trial 4259 training loss: 0.01395335327833891\n",
      "epoch: 9 trial 4260 training loss: 0.016467958223074675\n",
      "epoch: 9 trial 4261 training loss: 0.04201431758701801\n",
      "epoch: 9 trial 4262 training loss: 0.006185665959492326\n",
      "epoch: 9 trial 4263 training loss: 0.003453417040873319\n",
      "epoch: 9 trial 4264 training loss: 0.005094248219393194\n",
      "epoch: 9 trial 4265 training loss: 0.008773160399869084\n",
      "epoch: 9 trial 4266 training loss: 0.012629378819838166\n",
      "epoch: 9 trial 4267 training loss: 0.02885723114013672\n",
      "epoch: 9 trial 4268 training loss: 0.11287238821387291\n",
      "epoch: 9 trial 4269 training loss: 0.02447082195430994\n",
      "epoch: 9 trial 4270 training loss: 0.022627517115324736\n",
      "epoch: 9 trial 4271 training loss: 0.032688490115106106\n",
      "epoch: 9 trial 4272 training loss: 0.040181475691497326\n",
      "epoch: 9 trial 4273 training loss: 0.02941917907446623\n",
      "epoch: 9 trial 4274 training loss: 0.030719521455466747\n",
      "epoch: 9 trial 4275 training loss: 0.03572278283536434\n",
      "epoch: 9 trial 4276 training loss: 0.11091224662959576\n",
      "epoch: 9 trial 4277 training loss: 0.13003294169902802\n",
      "epoch: 9 trial 4278 training loss: 0.023822032380849123\n",
      "epoch: 9 trial 4279 training loss: 0.029036634135991335\n",
      "epoch: 9 trial 4280 training loss: 0.06866035237908363\n",
      "epoch: 9 trial 4281 training loss: 0.01702038710936904\n",
      "epoch: 9 trial 4282 training loss: 0.07491281256079674\n",
      "epoch: 9 trial 4283 training loss: 0.10582979954779148\n",
      "epoch: 9 trial 4284 training loss: 0.10711967200040817\n",
      "epoch: 9 trial 4285 training loss: 0.036624476313591\n",
      "epoch: 9 trial 4286 training loss: 0.09862544946372509\n",
      "epoch: 9 trial 4287 training loss: 0.032536416314542294\n",
      "epoch: 9 trial 4288 training loss: 0.07617148011922836\n",
      "epoch: 9 trial 4289 training loss: 0.03943292610347271\n",
      "epoch: 9 trial 4290 training loss: 0.09381976909935474\n",
      "epoch: 9 trial 4291 training loss: 0.010462263599038124\n",
      "epoch: 9 trial 4292 training loss: 0.005832731025293469\n",
      "epoch: 9 trial 4293 training loss: 0.023462005890905857\n",
      "epoch: 9 trial 4294 training loss: 0.025420038495212793\n",
      "epoch: 9 trial 4295 training loss: 0.05523895751684904\n",
      "epoch: 9 trial 4296 training loss: 0.05188230983912945\n",
      "epoch: 9 trial 4297 training loss: 0.015046192798763514\n",
      "epoch: 9 trial 4298 training loss: 0.051914749667048454\n",
      "epoch: 9 trial 4299 training loss: 0.021109523251652718\n",
      "epoch: 9 trial 4300 training loss: 0.012651590863242745\n",
      "epoch: 9 trial 4301 training loss: 0.019787308294326067\n",
      "epoch: 9 trial 4302 training loss: 0.014089679345488548\n",
      "epoch: 9 trial 4303 training loss: 0.019577710423618555\n",
      "epoch: 9 trial 4304 training loss: 0.038884169422090054\n",
      "epoch: 9 trial 4305 training loss: 0.02796095423400402\n",
      "epoch: 9 trial 4306 training loss: 0.02649943297728896\n",
      "epoch: 9 trial 4307 training loss: 0.020303215365856886\n",
      "epoch: 9 trial 4308 training loss: 0.0188174438662827\n",
      "epoch: 9 trial 4309 training loss: 0.007658954244107008\n",
      "epoch: 9 trial 4310 training loss: 0.013866751454770565\n",
      "epoch: 9 trial 4311 training loss: 0.01600327016785741\n",
      "epoch: 9 trial 4312 training loss: 0.018010191153734922\n",
      "epoch: 9 trial 4313 training loss: 0.005789404618553817\n",
      "epoch: 9 trial 4314 training loss: 0.012852310203015804\n",
      "epoch: 9 trial 4315 training loss: 0.08798230811953545\n",
      "epoch: 9 trial 4316 training loss: 0.014390286523848772\n",
      "epoch: 9 trial 4317 training loss: 0.03693035524338484\n",
      "epoch: 9 trial 4318 training loss: 0.017108857398852706\n",
      "epoch: 9 trial 4319 training loss: 0.04786831513047218\n",
      "epoch: 9 trial 4320 training loss: 0.03604534175246954\n",
      "epoch: 9 trial 4321 training loss: 0.03848966211080551\n",
      "epoch: 9 trial 4322 training loss: 0.030111503787338734\n",
      "epoch: 9 trial 4323 training loss: 0.01817134954035282\n",
      "epoch: 9 trial 4324 training loss: 0.09583980776369572\n",
      "epoch: 9 trial 4325 training loss: 0.007587968022562563\n",
      "epoch: 9 trial 4326 training loss: 0.015889635309576988\n",
      "epoch: 9 trial 4327 training loss: 0.12929247319698334\n",
      "epoch: 9 trial 4328 training loss: 0.02513631246984005\n",
      "epoch: 9 trial 4329 training loss: 0.020487665198743343\n",
      "epoch: 9 trial 4330 training loss: 0.015249037416651845\n",
      "epoch: 9 trial 4331 training loss: 0.0123591935262084\n",
      "epoch: 9 trial 4332 training loss: 0.0259847454726696\n",
      "epoch: 9 trial 4333 training loss: 0.013512603007256985\n",
      "epoch: 9 trial 4334 training loss: 0.048770470544695854\n",
      "epoch: 9 trial 4335 training loss: 0.14687665179371834\n",
      "epoch: 9 trial 4336 training loss: 0.07806086167693138\n",
      "epoch: 9 trial 4337 training loss: 0.02582011464983225\n",
      "epoch: 9 trial 4338 training loss: 0.09564996510744095\n",
      "epoch: 9 trial 4339 training loss: 0.10994663462042809\n",
      "epoch: 9 trial 4340 training loss: 0.02411580551415682\n",
      "epoch: 9 trial 4341 training loss: 0.07507638819515705\n",
      "epoch: 9 trial 4342 training loss: 0.005933115957304835\n",
      "epoch: 9 trial 4343 training loss: 0.05351349897682667\n",
      "epoch: 9 trial 4344 training loss: 0.012616278370842338\n",
      "epoch: 9 trial 4345 training loss: 0.02732457173988223\n",
      "epoch: 9 trial 4346 training loss: 0.10957730188965797\n",
      "epoch: 9 trial 4347 training loss: 0.04230230115354061\n",
      "epoch: 9 trial 4348 training loss: 0.016094318591058254\n",
      "epoch: 9 trial 4349 training loss: 0.022992725018411875\n",
      "epoch: 9 trial 4350 training loss: 0.07785548456013203\n",
      "epoch: 9 trial 4351 training loss: 0.011202780995517969\n",
      "epoch: 9 trial 4352 training loss: 0.054432112723588943\n",
      "epoch: 9 trial 4353 training loss: 0.03795203845947981\n",
      "epoch: 9 trial 4354 training loss: 0.033521574921905994\n",
      "epoch: 9 trial 4355 training loss: 0.011453255545347929\n",
      "epoch: 9 trial 4356 training loss: 0.04822566080838442\n",
      "epoch: 10 trial 4357 training loss: 0.011361675336956978\n",
      "epoch: 10 trial 4358 training loss: 0.012773027177900076\n",
      "epoch: 10 trial 4359 training loss: 0.01731001352891326\n",
      "epoch: 10 trial 4360 training loss: 0.06206533499062061\n",
      "epoch: 10 trial 4361 training loss: 0.004189659492112696\n",
      "epoch: 10 trial 4362 training loss: 0.00567489885725081\n",
      "epoch: 10 trial 4363 training loss: 0.02950304001569748\n",
      "epoch: 10 trial 4364 training loss: 0.019910331815481186\n",
      "epoch: 10 trial 4365 training loss: 0.00658933212980628\n",
      "epoch: 10 trial 4366 training loss: 0.013389716390520334\n",
      "epoch: 10 trial 4367 training loss: 0.05026568938046694\n",
      "epoch: 10 trial 4368 training loss: 0.029492168687283993\n",
      "epoch: 10 trial 4369 training loss: 0.01945527456700802\n",
      "epoch: 10 trial 4370 training loss: 0.04426947142928839\n",
      "epoch: 10 trial 4371 training loss: 0.013862553052604198\n",
      "epoch: 10 trial 4372 training loss: 0.02150167664512992\n",
      "epoch: 10 trial 4373 training loss: 0.11793654412031174\n",
      "epoch: 10 trial 4374 training loss: 0.04894660692662001\n",
      "epoch: 10 trial 4375 training loss: 0.012274037348106503\n",
      "epoch: 10 trial 4376 training loss: 0.019400616642087698\n",
      "epoch: 10 trial 4377 training loss: 0.01606293162330985\n",
      "epoch: 10 trial 4378 training loss: 0.07722119987010956\n",
      "epoch: 10 trial 4379 training loss: 0.009795061545446515\n",
      "epoch: 10 trial 4380 training loss: 0.006292963866144419\n",
      "epoch: 10 trial 4381 training loss: 0.1618189439177513\n",
      "epoch: 10 trial 4382 training loss: 0.02511441335082054\n",
      "epoch: 10 trial 4383 training loss: 0.010379748418927193\n",
      "epoch: 10 trial 4384 training loss: 0.11174069717526436\n",
      "epoch: 10 trial 4385 training loss: 0.02615003613755107\n",
      "epoch: 10 trial 4386 training loss: 0.01778685674071312\n",
      "epoch: 10 trial 4387 training loss: 0.02824059035629034\n",
      "epoch: 10 trial 4388 training loss: 0.013630928238853812\n",
      "epoch: 10 trial 4389 training loss: 0.032664816826581955\n",
      "epoch: 10 trial 4390 training loss: 0.04866941645741463\n",
      "epoch: 10 trial 4391 training loss: 0.02050379989668727\n",
      "epoch: 10 trial 4392 training loss: 0.0675736591219902\n",
      "epoch: 10 trial 4393 training loss: 0.15678077936172485\n",
      "epoch: 10 trial 4394 training loss: 0.3026356026530266\n",
      "epoch: 10 trial 4395 training loss: 0.03389465902000666\n",
      "epoch: 10 trial 4396 training loss: 0.09161707013845444\n",
      "epoch: 10 trial 4397 training loss: 0.10524747520685196\n",
      "epoch: 10 trial 4398 training loss: 0.013089381624013186\n",
      "epoch: 10 trial 4399 training loss: 0.09175573661923409\n",
      "epoch: 10 trial 4400 training loss: 0.04813004657626152\n",
      "epoch: 10 trial 4401 training loss: 0.02740579377859831\n",
      "epoch: 10 trial 4402 training loss: 0.06093102041631937\n",
      "epoch: 10 trial 4403 training loss: 0.00794960802886635\n",
      "epoch: 10 trial 4404 training loss: 0.01701647974550724\n",
      "epoch: 10 trial 4405 training loss: 0.0109285325743258\n",
      "epoch: 10 trial 4406 training loss: 0.07749724015593529\n",
      "epoch: 10 trial 4407 training loss: 0.050183048471808434\n",
      "epoch: 10 trial 4408 training loss: 0.0733466949313879\n",
      "epoch: 10 trial 4409 training loss: 0.1056323554366827\n",
      "epoch: 10 trial 4410 training loss: 0.11799884960055351\n",
      "epoch: 10 trial 4411 training loss: 0.05237128399312496\n",
      "epoch: 10 trial 4412 training loss: 0.07956393249332905\n",
      "epoch: 10 trial 4413 training loss: 0.025941526517271996\n",
      "epoch: 10 trial 4414 training loss: 0.014293619897216558\n",
      "epoch: 10 trial 4415 training loss: 0.014168059919029474\n",
      "epoch: 10 trial 4416 training loss: 0.0831002164632082\n",
      "epoch: 10 trial 4417 training loss: 0.017154830507934093\n",
      "epoch: 10 trial 4418 training loss: 0.09376751631498337\n",
      "epoch: 10 trial 4419 training loss: 0.037393905222415924\n",
      "epoch: 10 trial 4420 training loss: 0.002765777288004756\n",
      "epoch: 10 trial 4421 training loss: 0.04297976475208998\n",
      "epoch: 10 trial 4422 training loss: 0.04516002908349037\n",
      "epoch: 10 trial 4423 training loss: 0.020774221513420343\n",
      "epoch: 10 trial 4424 training loss: 0.013858920196071267\n",
      "epoch: 10 trial 4425 training loss: 0.052219751290977\n",
      "epoch: 10 trial 4426 training loss: 0.06317956000566483\n",
      "epoch: 10 trial 4427 training loss: 0.04465766157954931\n",
      "epoch: 10 trial 4428 training loss: 0.026373122818768024\n",
      "epoch: 10 trial 4429 training loss: 0.014503291342407465\n",
      "epoch: 10 trial 4430 training loss: 0.06715665385127068\n",
      "epoch: 10 trial 4431 training loss: 0.01553590502589941\n",
      "epoch: 10 trial 4432 training loss: 0.02456699125468731\n",
      "epoch: 10 trial 4433 training loss: 0.02541918260976672\n",
      "epoch: 10 trial 4434 training loss: 0.09436474740505219\n",
      "epoch: 10 trial 4435 training loss: 0.01681577041745186\n",
      "epoch: 10 trial 4436 training loss: 0.06777081079781055\n",
      "epoch: 10 trial 4437 training loss: 0.04342077439650893\n",
      "epoch: 10 trial 4438 training loss: 0.04360937234014273\n",
      "epoch: 10 trial 4439 training loss: 0.0572440791875124\n",
      "epoch: 10 trial 4440 training loss: 0.021027824841439724\n",
      "epoch: 10 trial 4441 training loss: 0.058091698214411736\n",
      "epoch: 10 trial 4442 training loss: 0.014817794784903526\n",
      "epoch: 10 trial 4443 training loss: 0.03308393619954586\n",
      "epoch: 10 trial 4444 training loss: 0.10610413923859596\n",
      "epoch: 10 trial 4445 training loss: 0.05162948928773403\n",
      "epoch: 10 trial 4446 training loss: 0.0061217412585392594\n",
      "epoch: 10 trial 4447 training loss: 0.03878450579941273\n",
      "epoch: 10 trial 4448 training loss: 0.02376321656629443\n",
      "epoch: 10 trial 4449 training loss: 0.01723572053015232\n",
      "epoch: 10 trial 4450 training loss: 0.06790474615991116\n",
      "epoch: 10 trial 4451 training loss: 0.04342740960419178\n",
      "epoch: 10 trial 4452 training loss: 0.05337077006697655\n",
      "epoch: 10 trial 4453 training loss: 0.044967164285480976\n",
      "epoch: 10 trial 4454 training loss: 0.08391566015779972\n",
      "epoch: 10 trial 4455 training loss: 0.043879494071006775\n",
      "epoch: 10 trial 4456 training loss: 0.0399667602032423\n",
      "epoch: 10 trial 4457 training loss: 0.05154294893145561\n",
      "epoch: 10 trial 4458 training loss: 0.10934421420097351\n",
      "epoch: 10 trial 4459 training loss: 0.050975216552615166\n",
      "epoch: 10 trial 4460 training loss: 0.03427246771752834\n",
      "epoch: 10 trial 4461 training loss: 0.2708752676844597\n",
      "epoch: 10 trial 4462 training loss: 0.04214424639940262\n",
      "epoch: 10 trial 4463 training loss: 0.030935218557715416\n",
      "epoch: 10 trial 4464 training loss: 0.165896974503994\n",
      "epoch: 10 trial 4465 training loss: 0.09395154193043709\n",
      "epoch: 10 trial 4466 training loss: 0.02705961186438799\n",
      "epoch: 10 trial 4467 training loss: 0.016799163073301315\n",
      "epoch: 10 trial 4468 training loss: 0.019277261570096016\n",
      "epoch: 10 trial 4469 training loss: 0.011239694198593497\n",
      "epoch: 10 trial 4470 training loss: 0.008890093071386218\n",
      "epoch: 10 trial 4471 training loss: 0.0582868829369545\n",
      "epoch: 10 trial 4472 training loss: 0.018090500961989164\n",
      "epoch: 10 trial 4473 training loss: 0.025150667410343885\n",
      "epoch: 10 trial 4474 training loss: 0.03204511012881994\n",
      "epoch: 10 trial 4475 training loss: 0.03305419813841581\n",
      "epoch: 10 trial 4476 training loss: 0.028209920041263103\n",
      "epoch: 10 trial 4477 training loss: 0.017694656737148762\n",
      "epoch: 10 trial 4478 training loss: 0.041192952543497086\n",
      "epoch: 10 trial 4479 training loss: 0.0825545247644186\n",
      "epoch: 10 trial 4480 training loss: 0.03727751970291138\n",
      "epoch: 10 trial 4481 training loss: 0.009710176149383187\n",
      "epoch: 10 trial 4482 training loss: 0.021815985906869173\n",
      "epoch: 10 trial 4483 training loss: 0.02071588346734643\n",
      "epoch: 10 trial 4484 training loss: 0.024978902656584978\n",
      "epoch: 10 trial 4485 training loss: 0.15109671279788017\n",
      "epoch: 10 trial 4486 training loss: 0.030903263948857784\n",
      "epoch: 10 trial 4487 training loss: 0.06054118648171425\n",
      "epoch: 10 trial 4488 training loss: 0.03122432203963399\n",
      "epoch: 10 trial 4489 training loss: 0.03371700458228588\n",
      "epoch: 10 trial 4490 training loss: 0.05200879368931055\n",
      "epoch: 10 trial 4491 training loss: 0.009223054628819227\n",
      "epoch: 10 trial 4492 training loss: 0.04470549896359444\n",
      "epoch: 10 trial 4493 training loss: 0.01785012800246477\n",
      "epoch: 10 trial 4494 training loss: 0.117479233071208\n",
      "epoch: 10 trial 4495 training loss: 0.016934516839683056\n",
      "epoch: 10 trial 4496 training loss: 0.06469215452671051\n",
      "epoch: 10 trial 4497 training loss: 0.03589381091296673\n",
      "epoch: 10 trial 4498 training loss: 0.031014162115752697\n",
      "epoch: 10 trial 4499 training loss: 0.02973309252411127\n",
      "epoch: 10 trial 4500 training loss: 0.08529534377157688\n",
      "epoch: 10 trial 4501 training loss: 0.015123397577553988\n",
      "epoch: 10 trial 4502 training loss: 0.017105091828852892\n",
      "epoch: 10 trial 4503 training loss: 0.10529160127043724\n",
      "epoch: 10 trial 4504 training loss: 0.01837376831099391\n",
      "epoch: 10 trial 4505 training loss: 0.04120111186057329\n",
      "epoch: 10 trial 4506 training loss: 0.06121739372611046\n",
      "epoch: 10 trial 4507 training loss: 0.02868196088820696\n",
      "epoch: 10 trial 4508 training loss: 0.02513735741376877\n",
      "epoch: 10 trial 4509 training loss: 0.013788198819383979\n",
      "epoch: 10 trial 4510 training loss: 0.035743855871260166\n",
      "epoch: 10 trial 4511 training loss: 0.011064119869843125\n",
      "epoch: 10 trial 4512 training loss: 0.01079834857955575\n",
      "epoch: 10 trial 4513 training loss: 0.024121980648487806\n",
      "epoch: 10 trial 4514 training loss: 0.004015544080175459\n",
      "epoch: 10 trial 4515 training loss: 0.01217768806964159\n",
      "epoch: 10 trial 4516 training loss: 0.02440556650981307\n",
      "epoch: 10 trial 4517 training loss: 0.009463550057262182\n",
      "epoch: 10 trial 4518 training loss: 0.054287075996398926\n",
      "epoch: 10 trial 4519 training loss: 0.0038033067830838263\n",
      "epoch: 10 trial 4520 training loss: 0.056177874095737934\n",
      "epoch: 10 trial 4521 training loss: 0.0071454718708992004\n",
      "epoch: 10 trial 4522 training loss: 0.023459010757505894\n",
      "epoch: 10 trial 4523 training loss: 0.027289998717606068\n",
      "epoch: 10 trial 4524 training loss: 0.0103193165268749\n",
      "epoch: 10 trial 4525 training loss: 0.037705534137785435\n",
      "epoch: 10 trial 4526 training loss: 0.1275448203086853\n",
      "epoch: 10 trial 4527 training loss: 0.046464988961815834\n",
      "epoch: 10 trial 4528 training loss: 0.007678505498915911\n",
      "epoch: 10 trial 4529 training loss: 0.05447857268154621\n",
      "epoch: 10 trial 4530 training loss: 0.0215400536544621\n",
      "epoch: 10 trial 4531 training loss: 0.024703326169401407\n",
      "epoch: 10 trial 4532 training loss: 0.02105971472337842\n",
      "epoch: 10 trial 4533 training loss: 0.016941202338784933\n",
      "epoch: 10 trial 4534 training loss: 0.05076151341199875\n",
      "epoch: 10 trial 4535 training loss: 0.050693255849182606\n",
      "epoch: 10 trial 4536 training loss: 0.03946031164377928\n",
      "epoch: 10 trial 4537 training loss: 0.06525607593357563\n",
      "epoch: 10 trial 4538 training loss: 0.016212727408856153\n",
      "epoch: 10 trial 4539 training loss: 0.03979108948260546\n",
      "epoch: 10 trial 4540 training loss: 0.020329362247139215\n",
      "epoch: 10 trial 4541 training loss: 0.019429221283644438\n",
      "epoch: 10 trial 4542 training loss: 0.044022164307534695\n",
      "epoch: 10 trial 4543 training loss: 0.059754347428679466\n",
      "epoch: 10 trial 4544 training loss: 0.016055476386100054\n",
      "epoch: 10 trial 4545 training loss: 0.048830886371433735\n",
      "epoch: 10 trial 4546 training loss: 0.04799020290374756\n",
      "epoch: 10 trial 4547 training loss: 0.059510840103030205\n",
      "epoch: 10 trial 4548 training loss: 0.05395644158124924\n",
      "epoch: 10 trial 4549 training loss: 0.029387447517365217\n",
      "epoch: 10 trial 4550 training loss: 0.017437986563891172\n",
      "epoch: 10 trial 4551 training loss: 0.037536321207880974\n",
      "epoch: 10 trial 4552 training loss: 0.02201119950041175\n",
      "epoch: 10 trial 4553 training loss: 0.022534824442118406\n",
      "epoch: 10 trial 4554 training loss: 0.06130824517458677\n",
      "epoch: 10 trial 4555 training loss: 0.09505788423120975\n",
      "epoch: 10 trial 4556 training loss: 0.010155266150832176\n",
      "epoch: 10 trial 4557 training loss: 0.00608543970156461\n",
      "epoch: 10 trial 4558 training loss: 0.009080696851015091\n",
      "epoch: 10 trial 4559 training loss: 0.00972268870100379\n",
      "epoch: 10 trial 4560 training loss: 0.016244178172200918\n",
      "epoch: 10 trial 4561 training loss: 0.014719281811267138\n",
      "epoch: 10 trial 4562 training loss: 0.053914789110422134\n",
      "epoch: 10 trial 4563 training loss: 0.042524865828454494\n",
      "epoch: 10 trial 4564 training loss: 0.01680643390864134\n",
      "epoch: 10 trial 4565 training loss: 0.09983807429671288\n",
      "epoch: 10 trial 4566 training loss: 0.04268732760101557\n",
      "epoch: 10 trial 4567 training loss: 0.060017336159944534\n",
      "epoch: 10 trial 4568 training loss: 0.06064346432685852\n",
      "epoch: 10 trial 4569 training loss: 0.12200239300727844\n",
      "epoch: 10 trial 4570 training loss: 0.012761153746396303\n",
      "epoch: 10 trial 4571 training loss: 0.012065809918567538\n",
      "epoch: 10 trial 4572 training loss: 0.024709076154977083\n",
      "epoch: 10 trial 4573 training loss: 0.022473966237157583\n",
      "epoch: 10 trial 4574 training loss: 0.01965146092697978\n",
      "epoch: 10 trial 4575 training loss: 0.042190718464553356\n",
      "epoch: 10 trial 4576 training loss: 0.05166259687393904\n",
      "epoch: 10 trial 4577 training loss: 0.03765068203210831\n",
      "epoch: 10 trial 4578 training loss: 0.06619713269174099\n",
      "epoch: 10 trial 4579 training loss: 0.053174165077507496\n",
      "epoch: 10 trial 4580 training loss: 0.08379729837179184\n",
      "epoch: 10 trial 4581 training loss: 0.011632974725216627\n",
      "epoch: 10 trial 4582 training loss: 0.009881040779873729\n",
      "epoch: 10 trial 4583 training loss: 0.02528304187580943\n",
      "epoch: 10 trial 4584 training loss: 0.025774340145289898\n",
      "epoch: 10 trial 4585 training loss: 0.047701235860586166\n",
      "epoch: 10 trial 4586 training loss: 0.015254229307174683\n",
      "epoch: 10 trial 4587 training loss: 0.15368308499455452\n",
      "epoch: 10 trial 4588 training loss: 0.03863919898867607\n",
      "epoch: 10 trial 4589 training loss: 0.12371627241373062\n",
      "epoch: 10 trial 4590 training loss: 0.09489892795681953\n",
      "epoch: 10 trial 4591 training loss: 0.016662983922287822\n",
      "epoch: 10 trial 4592 training loss: 0.05437355022877455\n",
      "epoch: 10 trial 4593 training loss: 0.020535182673484087\n",
      "epoch: 10 trial 4594 training loss: 0.048896146938204765\n",
      "epoch: 10 trial 4595 training loss: 0.046792494133114815\n",
      "epoch: 10 trial 4596 training loss: 0.09582580998539925\n",
      "epoch: 10 trial 4597 training loss: 0.02216725191101432\n",
      "epoch: 10 trial 4598 training loss: 0.03555190470069647\n",
      "epoch: 10 trial 4599 training loss: 0.03984569385647774\n",
      "epoch: 10 trial 4600 training loss: 0.020506448578089476\n",
      "epoch: 10 trial 4601 training loss: 0.058301083743572235\n",
      "epoch: 10 trial 4602 training loss: 0.018682078924030066\n",
      "epoch: 10 trial 4603 training loss: 0.05246284790337086\n",
      "epoch: 10 trial 4604 training loss: 0.02451863931491971\n",
      "epoch: 10 trial 4605 training loss: 0.02821008302271366\n",
      "epoch: 10 trial 4606 training loss: 0.04638868849724531\n",
      "epoch: 10 trial 4607 training loss: 0.011614691466093063\n",
      "epoch: 10 trial 4608 training loss: 0.005128390737809241\n",
      "epoch: 10 trial 4609 training loss: 0.027207566425204277\n",
      "epoch: 10 trial 4610 training loss: 0.0946572981774807\n",
      "epoch: 10 trial 4611 training loss: 0.019141624681651592\n",
      "epoch: 10 trial 4612 training loss: 0.08124711364507675\n",
      "epoch: 10 trial 4613 training loss: 0.03661241289228201\n",
      "epoch: 10 trial 4614 training loss: 0.022383091039955616\n",
      "epoch: 10 trial 4615 training loss: 0.17050569504499435\n",
      "epoch: 10 trial 4616 training loss: 0.013253461569547653\n",
      "epoch: 10 trial 4617 training loss: 0.015017467085272074\n",
      "epoch: 10 trial 4618 training loss: 0.010293188970535994\n",
      "epoch: 10 trial 4619 training loss: 0.003667529672384262\n",
      "epoch: 10 trial 4620 training loss: 0.024186928756535053\n",
      "epoch: 10 trial 4621 training loss: 0.05027494952082634\n",
      "epoch: 10 trial 4622 training loss: 0.039993010461330414\n",
      "epoch: 10 trial 4623 training loss: 0.035261557437479496\n",
      "epoch: 10 trial 4624 training loss: 0.032464299350976944\n",
      "epoch: 10 trial 4625 training loss: 0.026297712698578835\n",
      "epoch: 10 trial 4626 training loss: 0.08016976714134216\n",
      "epoch: 10 trial 4627 training loss: 0.015478645917028189\n",
      "epoch: 10 trial 4628 training loss: 0.008359479368664324\n",
      "epoch: 10 trial 4629 training loss: 0.021885167807340622\n",
      "epoch: 10 trial 4630 training loss: 0.006365049513988197\n",
      "epoch: 10 trial 4631 training loss: 0.06682991050183773\n",
      "epoch: 10 trial 4632 training loss: 0.003894043737091124\n",
      "epoch: 10 trial 4633 training loss: 0.021755377762019634\n",
      "epoch: 10 trial 4634 training loss: 0.014614820946007967\n",
      "epoch: 10 trial 4635 training loss: 0.06293128430843353\n",
      "epoch: 10 trial 4636 training loss: 0.02635813457891345\n",
      "epoch: 10 trial 4637 training loss: 0.022583579644560814\n",
      "epoch: 10 trial 4638 training loss: 0.05150603223592043\n",
      "epoch: 10 trial 4639 training loss: 0.07442005909979343\n",
      "epoch: 10 trial 4640 training loss: 0.034076498821377754\n",
      "epoch: 10 trial 4641 training loss: 0.06982249207794666\n",
      "epoch: 10 trial 4642 training loss: 0.020309755112975836\n",
      "epoch: 10 trial 4643 training loss: 0.08019031397998333\n",
      "epoch: 10 trial 4644 training loss: 0.021551684010773897\n",
      "epoch: 10 trial 4645 training loss: 0.009765938622877002\n",
      "epoch: 10 trial 4646 training loss: 0.04856958892196417\n",
      "epoch: 10 trial 4647 training loss: 0.02933696284890175\n",
      "epoch: 10 trial 4648 training loss: 0.017389096785336733\n",
      "epoch: 10 trial 4649 training loss: 0.015967557672411203\n",
      "epoch: 10 trial 4650 training loss: 0.009038476506248116\n",
      "epoch: 10 trial 4651 training loss: 0.022388015408068895\n",
      "epoch: 10 trial 4652 training loss: 0.03057057410478592\n",
      "epoch: 10 trial 4653 training loss: 0.01701101241633296\n",
      "epoch: 10 trial 4654 training loss: 0.0335810761898756\n",
      "epoch: 10 trial 4655 training loss: 0.07629221491515636\n",
      "epoch: 10 trial 4656 training loss: 0.035397738218307495\n",
      "epoch: 10 trial 4657 training loss: 0.017187512014061213\n",
      "epoch: 10 trial 4658 training loss: 0.006514152744784951\n",
      "epoch: 10 trial 4659 training loss: 0.009685060707852244\n",
      "epoch: 10 trial 4660 training loss: 0.028654959052801132\n",
      "epoch: 10 trial 4661 training loss: 0.057145402766764164\n",
      "epoch: 10 trial 4662 training loss: 0.046502322889864445\n",
      "epoch: 10 trial 4663 training loss: 0.0026893546455539763\n",
      "epoch: 10 trial 4664 training loss: 0.03014205861836672\n",
      "epoch: 10 trial 4665 training loss: 0.037519222125411034\n",
      "epoch: 10 trial 4666 training loss: 0.020030778367072344\n",
      "epoch: 10 trial 4667 training loss: 0.0156451016664505\n",
      "epoch: 10 trial 4668 training loss: 0.019749903585761786\n",
      "epoch: 10 trial 4669 training loss: 0.04144933260977268\n",
      "epoch: 10 trial 4670 training loss: 0.021132192574441433\n",
      "epoch: 10 trial 4671 training loss: 0.03636153507977724\n",
      "epoch: 10 trial 4672 training loss: 0.06282375007867813\n",
      "epoch: 10 trial 4673 training loss: 0.03139367140829563\n",
      "epoch: 10 trial 4674 training loss: 0.031394774094223976\n",
      "epoch: 10 trial 4675 training loss: 0.061721271835267544\n",
      "epoch: 10 trial 4676 training loss: 0.0336952768266201\n",
      "epoch: 10 trial 4677 training loss: 0.06453151442110538\n",
      "epoch: 10 trial 4678 training loss: 0.01836402155458927\n",
      "epoch: 10 trial 4679 training loss: 0.030040156561881304\n",
      "epoch: 10 trial 4680 training loss: 0.02450361754745245\n",
      "epoch: 10 trial 4681 training loss: 0.021692531649023294\n",
      "epoch: 10 trial 4682 training loss: 0.02231475617736578\n",
      "epoch: 10 trial 4683 training loss: 0.08841415122151375\n",
      "epoch: 10 trial 4684 training loss: 0.012558226007968187\n",
      "epoch: 10 trial 4685 training loss: 0.08580570667982101\n",
      "epoch: 10 trial 4686 training loss: 0.023313894402235746\n",
      "epoch: 10 trial 4687 training loss: 0.006591013865545392\n",
      "epoch: 10 trial 4688 training loss: 0.05499867722392082\n",
      "epoch: 10 trial 4689 training loss: 0.06042412109673023\n",
      "epoch: 10 trial 4690 training loss: 0.010297315660864115\n",
      "epoch: 10 trial 4691 training loss: 0.03364207874983549\n",
      "epoch: 10 trial 4692 training loss: 0.008568743709474802\n",
      "epoch: 10 trial 4693 training loss: 0.03289680741727352\n",
      "epoch: 10 trial 4694 training loss: 0.020370101556181908\n",
      "epoch: 10 trial 4695 training loss: 0.037902277894318104\n",
      "epoch: 10 trial 4696 training loss: 0.25431327521800995\n",
      "epoch: 10 trial 4697 training loss: 0.050539394840598106\n",
      "epoch: 10 trial 4698 training loss: 0.00867973710410297\n",
      "epoch: 10 trial 4699 training loss: 0.021497641690075397\n",
      "epoch: 10 trial 4700 training loss: 0.13223270699381828\n",
      "epoch: 10 trial 4701 training loss: 0.045349800027906895\n",
      "epoch: 10 trial 4702 training loss: 0.0720970369875431\n",
      "epoch: 10 trial 4703 training loss: 0.03562110150232911\n",
      "epoch: 10 trial 4704 training loss: 0.031766525469720364\n",
      "epoch: 10 trial 4705 training loss: 0.016314438777044415\n",
      "epoch: 10 trial 4706 training loss: 0.09120984748005867\n",
      "epoch: 10 trial 4707 training loss: 0.019558623433113098\n",
      "epoch: 10 trial 4708 training loss: 0.08067089132964611\n",
      "epoch: 10 trial 4709 training loss: 0.0241047372110188\n",
      "epoch: 10 trial 4710 training loss: 0.009188073454424739\n",
      "epoch: 10 trial 4711 training loss: 0.03929328639060259\n",
      "epoch: 10 trial 4712 training loss: 0.004322155786212534\n",
      "epoch: 10 trial 4713 training loss: 0.01540874456986785\n",
      "epoch: 10 trial 4714 training loss: 0.08731481619179249\n",
      "epoch: 10 trial 4715 training loss: 0.07638188265264034\n",
      "epoch: 10 trial 4716 training loss: 0.023693987634032965\n",
      "epoch: 10 trial 4717 training loss: 0.10309004597365856\n",
      "epoch: 10 trial 4718 training loss: 0.11102748289704323\n",
      "epoch: 10 trial 4719 training loss: 0.028011870570480824\n",
      "epoch: 10 trial 4720 training loss: 0.06199053302407265\n",
      "epoch: 10 trial 4721 training loss: 0.0392789077013731\n",
      "epoch: 10 trial 4722 training loss: 0.0136302022729069\n",
      "epoch: 10 trial 4723 training loss: 0.00812411680817604\n",
      "epoch: 10 trial 4724 training loss: 0.15151463821530342\n",
      "epoch: 10 trial 4725 training loss: 0.06878893822431564\n",
      "epoch: 10 trial 4726 training loss: 0.043937923386693\n",
      "epoch: 10 trial 4727 training loss: 0.054147059097886086\n",
      "epoch: 10 trial 4728 training loss: 0.026070774532854557\n",
      "epoch: 10 trial 4729 training loss: 0.032008095644414425\n",
      "epoch: 10 trial 4730 training loss: 0.018731657415628433\n",
      "epoch: 10 trial 4731 training loss: 0.046328979544341564\n",
      "epoch: 10 trial 4732 training loss: 0.03518187627196312\n",
      "epoch: 10 trial 4733 training loss: 0.04627566784620285\n",
      "epoch: 10 trial 4734 training loss: 0.09132692031562328\n",
      "epoch: 10 trial 4735 training loss: 0.030621574260294437\n",
      "epoch: 10 trial 4736 training loss: 0.013639767654240131\n",
      "epoch: 10 trial 4737 training loss: 0.031206130050122738\n",
      "epoch: 10 trial 4738 training loss: 0.003494638134725392\n",
      "epoch: 10 trial 4739 training loss: 0.05014754645526409\n",
      "epoch: 10 trial 4740 training loss: 0.04081704467535019\n",
      "epoch: 10 trial 4741 training loss: 0.019029867835342884\n",
      "epoch: 10 trial 4742 training loss: 0.004316602658946067\n",
      "epoch: 10 trial 4743 training loss: 0.012892794096842408\n",
      "epoch: 10 trial 4744 training loss: 0.017372515983879566\n",
      "epoch: 10 trial 4745 training loss: 0.038048988208174706\n",
      "epoch: 10 trial 4746 training loss: 0.006199379567988217\n",
      "epoch: 10 trial 4747 training loss: 0.0036306462716311216\n",
      "epoch: 10 trial 4748 training loss: 0.0055189793929457664\n",
      "epoch: 10 trial 4749 training loss: 0.008856595726683736\n",
      "epoch: 10 trial 4750 training loss: 0.013017123099416494\n",
      "epoch: 10 trial 4751 training loss: 0.025359136518090963\n",
      "epoch: 10 trial 4752 training loss: 0.10851963795721531\n",
      "epoch: 10 trial 4753 training loss: 0.021885540336370468\n",
      "epoch: 10 trial 4754 training loss: 0.02390865096822381\n",
      "epoch: 10 trial 4755 training loss: 0.025483347941190004\n",
      "epoch: 10 trial 4756 training loss: 0.028560790698975325\n",
      "epoch: 10 trial 4757 training loss: 0.023720343597233295\n",
      "epoch: 10 trial 4758 training loss: 0.02769283391535282\n",
      "epoch: 10 trial 4759 training loss: 0.03989744931459427\n",
      "epoch: 10 trial 4760 training loss: 0.016153252217918634\n",
      "epoch: 10 trial 4761 training loss: 0.05723775178194046\n",
      "epoch: 10 trial 4762 training loss: 0.03032416570931673\n",
      "epoch: 10 trial 4763 training loss: 0.06676982389762998\n",
      "epoch: 10 trial 4764 training loss: 0.19349663332104683\n",
      "epoch: 10 trial 4765 training loss: 0.007114270236343145\n",
      "epoch: 10 trial 4766 training loss: 0.04196589067578316\n",
      "epoch: 10 trial 4767 training loss: 0.08263429254293442\n",
      "epoch: 10 trial 4768 training loss: 0.053554121404886246\n",
      "epoch: 10 trial 4769 training loss: 0.025053564459085464\n",
      "epoch: 10 trial 4770 training loss: 0.07401496544480324\n",
      "epoch: 10 trial 4771 training loss: 0.03872631769627333\n",
      "epoch: 10 trial 4772 training loss: 0.05446786247193813\n",
      "epoch: 10 trial 4773 training loss: 0.022071226965636015\n",
      "epoch: 10 trial 4774 training loss: 0.037264617159962654\n",
      "epoch: 10 trial 4775 training loss: 0.028900430537760258\n",
      "epoch: 10 trial 4776 training loss: 0.005913458066061139\n",
      "epoch: 10 trial 4777 training loss: 0.018290136009454727\n",
      "epoch: 10 trial 4778 training loss: 0.035155028104782104\n",
      "epoch: 10 trial 4779 training loss: 0.047597166150808334\n",
      "epoch: 10 trial 4780 training loss: 0.03121332824230194\n",
      "epoch: 10 trial 4781 training loss: 0.012866088887676597\n",
      "epoch: 10 trial 4782 training loss: 0.05114174261689186\n",
      "epoch: 10 trial 4783 training loss: 0.03464049752801657\n",
      "epoch: 10 trial 4784 training loss: 0.011343583231791854\n",
      "epoch: 10 trial 4785 training loss: 0.019602397456765175\n",
      "epoch: 10 trial 4786 training loss: 0.010015645530074835\n",
      "epoch: 10 trial 4787 training loss: 0.01900024153292179\n",
      "epoch: 10 trial 4788 training loss: 0.03726792801171541\n",
      "epoch: 10 trial 4789 training loss: 0.03183192480355501\n",
      "epoch: 10 trial 4790 training loss: 0.019112813752144575\n",
      "epoch: 10 trial 4791 training loss: 0.0182450613938272\n",
      "epoch: 10 trial 4792 training loss: 0.016900261864066124\n",
      "epoch: 10 trial 4793 training loss: 0.008323420537635684\n",
      "epoch: 10 trial 4794 training loss: 0.03513513132929802\n",
      "epoch: 10 trial 4795 training loss: 0.013284832006320357\n",
      "epoch: 10 trial 4796 training loss: 0.014662506058812141\n",
      "epoch: 10 trial 4797 training loss: 0.0077781653963029385\n",
      "epoch: 10 trial 4798 training loss: 0.013397289207205176\n",
      "epoch: 10 trial 4799 training loss: 0.08068897388875484\n",
      "epoch: 10 trial 4800 training loss: 0.01417314144782722\n",
      "epoch: 10 trial 4801 training loss: 0.0351066654548049\n",
      "epoch: 10 trial 4802 training loss: 0.01570895127952099\n",
      "epoch: 10 trial 4803 training loss: 0.048162514343857765\n",
      "epoch: 10 trial 4804 training loss: 0.047020590864121914\n",
      "epoch: 10 trial 4805 training loss: 0.039583658799529076\n",
      "epoch: 10 trial 4806 training loss: 0.02787287626415491\n",
      "epoch: 10 trial 4807 training loss: 0.019476655405014753\n",
      "epoch: 10 trial 4808 training loss: 0.0982561744749546\n",
      "epoch: 10 trial 4809 training loss: 0.0067257320042699575\n",
      "epoch: 10 trial 4810 training loss: 0.01617016550153494\n",
      "epoch: 10 trial 4811 training loss: 0.13387346267700195\n",
      "epoch: 10 trial 4812 training loss: 0.024270816706120968\n",
      "epoch: 10 trial 4813 training loss: 0.021936712320894003\n",
      "epoch: 10 trial 4814 training loss: 0.012476461939513683\n",
      "epoch: 10 trial 4815 training loss: 0.01324183284305036\n",
      "epoch: 10 trial 4816 training loss: 0.0262317918241024\n",
      "epoch: 10 trial 4817 training loss: 0.011947500985115767\n",
      "epoch: 10 trial 4818 training loss: 0.0426755715161562\n",
      "epoch: 10 trial 4819 training loss: 0.1448935605585575\n",
      "epoch: 10 trial 4820 training loss: 0.0673339981585741\n",
      "epoch: 10 trial 4821 training loss: 0.02486363984644413\n",
      "epoch: 10 trial 4822 training loss: 0.10206472501158714\n",
      "epoch: 10 trial 4823 training loss: 0.10532137006521225\n",
      "epoch: 10 trial 4824 training loss: 0.025632296223193407\n",
      "epoch: 10 trial 4825 training loss: 0.07053646445274353\n",
      "epoch: 10 trial 4826 training loss: 0.007579405792057514\n",
      "epoch: 10 trial 4827 training loss: 0.0567192267626524\n",
      "epoch: 10 trial 4828 training loss: 0.013214321807026863\n",
      "epoch: 10 trial 4829 training loss: 0.026689114049077034\n",
      "epoch: 10 trial 4830 training loss: 0.1092216819524765\n",
      "epoch: 10 trial 4831 training loss: 0.03987874649465084\n",
      "epoch: 10 trial 4832 training loss: 0.014696921687573195\n",
      "epoch: 10 trial 4833 training loss: 0.019137583673000336\n",
      "epoch: 10 trial 4834 training loss: 0.08340850658714771\n",
      "epoch: 10 trial 4835 training loss: 0.012251960346475244\n",
      "epoch: 10 trial 4836 training loss: 0.045650722458958626\n",
      "epoch: 10 trial 4837 training loss: 0.036315467208623886\n",
      "epoch: 10 trial 4838 training loss: 0.033282579854130745\n",
      "epoch: 10 trial 4839 training loss: 0.011457875370979309\n",
      "epoch: 10 trial 4840 training loss: 0.05272437632083893\n",
      "epoch: 11 trial 4841 training loss: 0.00956589775159955\n",
      "epoch: 11 trial 4842 training loss: 0.013188454322516918\n",
      "epoch: 11 trial 4843 training loss: 0.01780676795169711\n",
      "epoch: 11 trial 4844 training loss: 0.06106714345514774\n",
      "epoch: 11 trial 4845 training loss: 0.004308103700168431\n",
      "epoch: 11 trial 4846 training loss: 0.005819736630655825\n",
      "epoch: 11 trial 4847 training loss: 0.028201100416481495\n",
      "epoch: 11 trial 4848 training loss: 0.017308115493506193\n",
      "epoch: 11 trial 4849 training loss: 0.008005784591659904\n",
      "epoch: 11 trial 4850 training loss: 0.01469750190153718\n",
      "epoch: 11 trial 4851 training loss: 0.05313653778284788\n",
      "epoch: 11 trial 4852 training loss: 0.03015702310949564\n",
      "epoch: 11 trial 4853 training loss: 0.01770106516778469\n",
      "epoch: 11 trial 4854 training loss: 0.044384790584445\n",
      "epoch: 11 trial 4855 training loss: 0.01446731062605977\n",
      "epoch: 11 trial 4856 training loss: 0.022297342773526907\n",
      "epoch: 11 trial 4857 training loss: 0.11827955022454262\n",
      "epoch: 11 trial 4858 training loss: 0.047304592095315456\n",
      "epoch: 11 trial 4859 training loss: 0.014463795349001884\n",
      "epoch: 11 trial 4860 training loss: 0.01868318486958742\n",
      "epoch: 11 trial 4861 training loss: 0.017492416780442\n",
      "epoch: 11 trial 4862 training loss: 0.07088229060173035\n",
      "epoch: 11 trial 4863 training loss: 0.009457027073949575\n",
      "epoch: 11 trial 4864 training loss: 0.005887140869162977\n",
      "epoch: 11 trial 4865 training loss: 0.1420620195567608\n",
      "epoch: 11 trial 4866 training loss: 0.024431410245597363\n",
      "epoch: 11 trial 4867 training loss: 0.009934634435921907\n",
      "epoch: 11 trial 4868 training loss: 0.10695753991603851\n",
      "epoch: 11 trial 4869 training loss: 0.023508913815021515\n",
      "epoch: 11 trial 4870 training loss: 0.018766718450933695\n",
      "epoch: 11 trial 4871 training loss: 0.031921593472361565\n",
      "epoch: 11 trial 4872 training loss: 0.014003097545355558\n",
      "epoch: 11 trial 4873 training loss: 0.030190564692020416\n",
      "epoch: 11 trial 4874 training loss: 0.04471710883080959\n",
      "epoch: 11 trial 4875 training loss: 0.019335136748850346\n",
      "epoch: 11 trial 4876 training loss: 0.06342321168631315\n",
      "epoch: 11 trial 4877 training loss: 0.1457947976887226\n",
      "epoch: 11 trial 4878 training loss: 0.2693566009402275\n",
      "epoch: 11 trial 4879 training loss: 0.03459770046174526\n",
      "epoch: 11 trial 4880 training loss: 0.0966406874358654\n",
      "epoch: 11 trial 4881 training loss: 0.10304582305252552\n",
      "epoch: 11 trial 4882 training loss: 0.013026895001530647\n",
      "epoch: 11 trial 4883 training loss: 0.09958312660455704\n",
      "epoch: 11 trial 4884 training loss: 0.045049906708300114\n",
      "epoch: 11 trial 4885 training loss: 0.023807324469089508\n",
      "epoch: 11 trial 4886 training loss: 0.06690831296145916\n",
      "epoch: 11 trial 4887 training loss: 0.013827264308929443\n",
      "epoch: 11 trial 4888 training loss: 0.023693092167377472\n",
      "epoch: 11 trial 4889 training loss: 0.013170801568776369\n",
      "epoch: 11 trial 4890 training loss: 0.09334186650812626\n",
      "epoch: 11 trial 4891 training loss: 0.03575882222503424\n",
      "epoch: 11 trial 4892 training loss: 0.11914672702550888\n",
      "epoch: 11 trial 4893 training loss: 0.04097475949674845\n",
      "epoch: 11 trial 4894 training loss: 0.04429769329726696\n",
      "epoch: 11 trial 4895 training loss: 0.031442697159945965\n",
      "epoch: 11 trial 4896 training loss: 0.06491866707801819\n",
      "epoch: 11 trial 4897 training loss: 0.024996359832584858\n",
      "epoch: 11 trial 4898 training loss: 0.017208673525601625\n",
      "epoch: 11 trial 4899 training loss: 0.022482887841761112\n",
      "epoch: 11 trial 4900 training loss: 0.08343400433659554\n",
      "epoch: 11 trial 4901 training loss: 0.033285570330917835\n",
      "epoch: 11 trial 4902 training loss: 0.11137142777442932\n",
      "epoch: 11 trial 4903 training loss: 0.02537443209439516\n",
      "epoch: 11 trial 4904 training loss: 0.0028815974364988506\n",
      "epoch: 11 trial 4905 training loss: 0.047412000596523285\n",
      "epoch: 11 trial 4906 training loss: 0.05704883672297001\n",
      "epoch: 11 trial 4907 training loss: 0.021632696501910686\n",
      "epoch: 11 trial 4908 training loss: 0.018513841554522514\n",
      "epoch: 11 trial 4909 training loss: 0.060042787343263626\n",
      "epoch: 11 trial 4910 training loss: 0.04507396649569273\n",
      "epoch: 11 trial 4911 training loss: 0.0439502838999033\n",
      "epoch: 11 trial 4912 training loss: 0.0355041828006506\n",
      "epoch: 11 trial 4913 training loss: 0.014365827199071646\n",
      "epoch: 11 trial 4914 training loss: 0.06903235241770744\n",
      "epoch: 11 trial 4915 training loss: 0.015192400896921754\n",
      "epoch: 11 trial 4916 training loss: 0.024498707614839077\n",
      "epoch: 11 trial 4917 training loss: 0.02849869802594185\n",
      "epoch: 11 trial 4918 training loss: 0.08861321769654751\n",
      "epoch: 11 trial 4919 training loss: 0.017557870130985975\n",
      "epoch: 11 trial 4920 training loss: 0.05765485018491745\n",
      "epoch: 11 trial 4921 training loss: 0.03871015924960375\n",
      "epoch: 11 trial 4922 training loss: 0.045637049712240696\n",
      "epoch: 11 trial 4923 training loss: 0.05391516163945198\n",
      "epoch: 11 trial 4924 training loss: 0.020494678523391485\n",
      "epoch: 11 trial 4925 training loss: 0.06074769049882889\n",
      "epoch: 11 trial 4926 training loss: 0.015306681860238314\n",
      "epoch: 11 trial 4927 training loss: 0.03347255755215883\n",
      "epoch: 11 trial 4928 training loss: 0.1078513115644455\n",
      "epoch: 11 trial 4929 training loss: 0.05194560065865517\n",
      "epoch: 11 trial 4930 training loss: 0.005721058812923729\n",
      "epoch: 11 trial 4931 training loss: 0.03932833392173052\n",
      "epoch: 11 trial 4932 training loss: 0.02545366995036602\n",
      "epoch: 11 trial 4933 training loss: 0.016250870190560818\n",
      "epoch: 11 trial 4934 training loss: 0.07015264220535755\n",
      "epoch: 11 trial 4935 training loss: 0.04279061686247587\n",
      "epoch: 11 trial 4936 training loss: 0.05040351394563913\n",
      "epoch: 11 trial 4937 training loss: 0.040796264074742794\n",
      "epoch: 11 trial 4938 training loss: 0.08766674995422363\n",
      "epoch: 11 trial 4939 training loss: 0.04164655692875385\n",
      "epoch: 11 trial 4940 training loss: 0.03215488139539957\n",
      "epoch: 11 trial 4941 training loss: 0.036787756718695164\n",
      "epoch: 11 trial 4942 training loss: 0.10999239981174469\n",
      "epoch: 11 trial 4943 training loss: 0.050561859272420406\n",
      "epoch: 11 trial 4944 training loss: 0.03641879837960005\n",
      "epoch: 11 trial 4945 training loss: 0.260947085916996\n",
      "epoch: 11 trial 4946 training loss: 0.04739861376583576\n",
      "epoch: 11 trial 4947 training loss: 0.028893338050693274\n",
      "epoch: 11 trial 4948 training loss: 0.15532567352056503\n",
      "epoch: 11 trial 4949 training loss: 0.09569987840950489\n",
      "epoch: 11 trial 4950 training loss: 0.02707693539559841\n",
      "epoch: 11 trial 4951 training loss: 0.01822594366967678\n",
      "epoch: 11 trial 4952 training loss: 0.02118479972705245\n",
      "epoch: 11 trial 4953 training loss: 0.011848811991512775\n",
      "epoch: 11 trial 4954 training loss: 0.008248911472037435\n",
      "epoch: 11 trial 4955 training loss: 0.06028295960277319\n",
      "epoch: 11 trial 4956 training loss: 0.018593968357890844\n",
      "epoch: 11 trial 4957 training loss: 0.02534301206469536\n",
      "epoch: 11 trial 4958 training loss: 0.03587101586163044\n",
      "epoch: 11 trial 4959 training loss: 0.03191596921533346\n",
      "epoch: 11 trial 4960 training loss: 0.028130880557000637\n",
      "epoch: 11 trial 4961 training loss: 0.01796484040096402\n",
      "epoch: 11 trial 4962 training loss: 0.04042463656514883\n",
      "epoch: 11 trial 4963 training loss: 0.08154185116291046\n",
      "epoch: 11 trial 4964 training loss: 0.03875654935836792\n",
      "epoch: 11 trial 4965 training loss: 0.010090441908687353\n",
      "epoch: 11 trial 4966 training loss: 0.02321472531184554\n",
      "epoch: 11 trial 4967 training loss: 0.02087930217385292\n",
      "epoch: 11 trial 4968 training loss: 0.024579209741204977\n",
      "epoch: 11 trial 4969 training loss: 0.14449408277869225\n",
      "epoch: 11 trial 4970 training loss: 0.031173035502433777\n",
      "epoch: 11 trial 4971 training loss: 0.060787444934248924\n",
      "epoch: 11 trial 4972 training loss: 0.03304657153785229\n",
      "epoch: 11 trial 4973 training loss: 0.036413236521184444\n",
      "epoch: 11 trial 4974 training loss: 0.04674214869737625\n",
      "epoch: 11 trial 4975 training loss: 0.009647747734561563\n",
      "epoch: 11 trial 4976 training loss: 0.045086116530001163\n",
      "epoch: 11 trial 4977 training loss: 0.017586045200005174\n",
      "epoch: 11 trial 4978 training loss: 0.12325098365545273\n",
      "epoch: 11 trial 4979 training loss: 0.01594874309375882\n",
      "epoch: 11 trial 4980 training loss: 0.05971124954521656\n",
      "epoch: 11 trial 4981 training loss: 0.040762778371572495\n",
      "epoch: 11 trial 4982 training loss: 0.031170994974672794\n",
      "epoch: 11 trial 4983 training loss: 0.028926994651556015\n",
      "epoch: 11 trial 4984 training loss: 0.08331681415438652\n",
      "epoch: 11 trial 4985 training loss: 0.015845719259232283\n",
      "epoch: 11 trial 4986 training loss: 0.017296651378273964\n",
      "epoch: 11 trial 4987 training loss: 0.10898451507091522\n",
      "epoch: 11 trial 4988 training loss: 0.018574214540421963\n",
      "epoch: 11 trial 4989 training loss: 0.04160369001328945\n",
      "epoch: 11 trial 4990 training loss: 0.06716697104275227\n",
      "epoch: 11 trial 4991 training loss: 0.02933456376194954\n",
      "epoch: 11 trial 4992 training loss: 0.02628265507519245\n",
      "epoch: 11 trial 4993 training loss: 0.015254236292093992\n",
      "epoch: 11 trial 4994 training loss: 0.03788334783166647\n",
      "epoch: 11 trial 4995 training loss: 0.011101259617134929\n",
      "epoch: 11 trial 4996 training loss: 0.011702535673975945\n",
      "epoch: 11 trial 4997 training loss: 0.02574738161638379\n",
      "epoch: 11 trial 4998 training loss: 0.003833562368527055\n",
      "epoch: 11 trial 4999 training loss: 0.012467247666791081\n",
      "epoch: 11 trial 5000 training loss: 0.02213336480781436\n",
      "epoch: 11 trial 5001 training loss: 0.009374806191772223\n",
      "epoch: 11 trial 5002 training loss: 0.05901489406824112\n",
      "epoch: 11 trial 5003 training loss: 0.0035898469504900277\n",
      "epoch: 11 trial 5004 training loss: 0.054083624854683876\n",
      "epoch: 11 trial 5005 training loss: 0.007211429649032652\n",
      "epoch: 11 trial 5006 training loss: 0.021901129744946957\n",
      "epoch: 11 trial 5007 training loss: 0.027233746368438005\n",
      "epoch: 11 trial 5008 training loss: 0.010735077550634742\n",
      "epoch: 11 trial 5009 training loss: 0.03962368331849575\n",
      "epoch: 11 trial 5010 training loss: 0.13005616888403893\n",
      "epoch: 11 trial 5011 training loss: 0.04960650950670242\n",
      "epoch: 11 trial 5012 training loss: 0.007285066414624453\n",
      "epoch: 11 trial 5013 training loss: 0.05473215878009796\n",
      "epoch: 11 trial 5014 training loss: 0.020309007726609707\n",
      "epoch: 11 trial 5015 training loss: 0.02214043401181698\n",
      "epoch: 11 trial 5016 training loss: 0.020898508839309216\n",
      "epoch: 11 trial 5017 training loss: 0.018526483327150345\n",
      "epoch: 11 trial 5018 training loss: 0.05310498643666506\n",
      "epoch: 11 trial 5019 training loss: 0.05151614546775818\n",
      "epoch: 11 trial 5020 training loss: 0.042405955493450165\n",
      "epoch: 11 trial 5021 training loss: 0.0636154804378748\n",
      "epoch: 11 trial 5022 training loss: 0.018284275196492672\n",
      "epoch: 11 trial 5023 training loss: 0.040080017410218716\n",
      "epoch: 11 trial 5024 training loss: 0.022188961505889893\n",
      "epoch: 11 trial 5025 training loss: 0.01870705932378769\n",
      "epoch: 11 trial 5026 training loss: 0.04659999720752239\n",
      "epoch: 11 trial 5027 training loss: 0.06156757287681103\n",
      "epoch: 11 trial 5028 training loss: 0.017344127874821424\n",
      "epoch: 11 trial 5029 training loss: 0.0456975856795907\n",
      "epoch: 11 trial 5030 training loss: 0.049709899351000786\n",
      "epoch: 11 trial 5031 training loss: 0.06145278923213482\n",
      "epoch: 11 trial 5032 training loss: 0.05047469865530729\n",
      "epoch: 11 trial 5033 training loss: 0.0312729412689805\n",
      "epoch: 11 trial 5034 training loss: 0.018839295022189617\n",
      "epoch: 11 trial 5035 training loss: 0.03628949448466301\n",
      "epoch: 11 trial 5036 training loss: 0.025029003620147705\n",
      "epoch: 11 trial 5037 training loss: 0.026449790224432945\n",
      "epoch: 11 trial 5038 training loss: 0.057419100776314735\n",
      "epoch: 11 trial 5039 training loss: 0.08993902802467346\n",
      "epoch: 11 trial 5040 training loss: 0.012334541417658329\n",
      "epoch: 11 trial 5041 training loss: 0.00602069974411279\n",
      "epoch: 11 trial 5042 training loss: 0.009742093738168478\n",
      "epoch: 11 trial 5043 training loss: 0.010253943037241697\n",
      "epoch: 11 trial 5044 training loss: 0.015561929903924465\n",
      "epoch: 11 trial 5045 training loss: 0.015635374234989285\n",
      "epoch: 11 trial 5046 training loss: 0.04585645440965891\n",
      "epoch: 11 trial 5047 training loss: 0.04595483746379614\n",
      "epoch: 11 trial 5048 training loss: 0.015330001711845398\n",
      "epoch: 11 trial 5049 training loss: 0.10146403685212135\n",
      "epoch: 11 trial 5050 training loss: 0.03602783381938934\n",
      "epoch: 11 trial 5051 training loss: 0.05315951909869909\n",
      "epoch: 11 trial 5052 training loss: 0.06152368523180485\n",
      "epoch: 11 trial 5053 training loss: 0.11674218252301216\n",
      "epoch: 11 trial 5054 training loss: 0.014237486058846116\n",
      "epoch: 11 trial 5055 training loss: 0.012766515370458364\n",
      "epoch: 11 trial 5056 training loss: 0.02196537284180522\n",
      "epoch: 11 trial 5057 training loss: 0.02163003198802471\n",
      "epoch: 11 trial 5058 training loss: 0.019004605244845152\n",
      "epoch: 11 trial 5059 training loss: 0.04248376563191414\n",
      "epoch: 11 trial 5060 training loss: 0.05319786071777344\n",
      "epoch: 11 trial 5061 training loss: 0.036745570600032806\n",
      "epoch: 11 trial 5062 training loss: 0.06090380810201168\n",
      "epoch: 11 trial 5063 training loss: 0.04679108504205942\n",
      "epoch: 11 trial 5064 training loss: 0.07619836926460266\n",
      "epoch: 11 trial 5065 training loss: 0.010179917560890317\n",
      "epoch: 11 trial 5066 training loss: 0.010878333356231451\n",
      "epoch: 11 trial 5067 training loss: 0.02662429492920637\n",
      "epoch: 11 trial 5068 training loss: 0.02712969295680523\n",
      "epoch: 11 trial 5069 training loss: 0.04635648149996996\n",
      "epoch: 11 trial 5070 training loss: 0.014019627124071121\n",
      "epoch: 11 trial 5071 training loss: 0.1517677642405033\n",
      "epoch: 11 trial 5072 training loss: 0.04003685340285301\n",
      "epoch: 11 trial 5073 training loss: 0.13176823034882545\n",
      "epoch: 11 trial 5074 training loss: 0.09915247187018394\n",
      "epoch: 11 trial 5075 training loss: 0.017322387546300888\n",
      "epoch: 11 trial 5076 training loss: 0.052775370888412\n",
      "epoch: 11 trial 5077 training loss: 0.02183340582996607\n",
      "epoch: 11 trial 5078 training loss: 0.05297518242150545\n",
      "epoch: 11 trial 5079 training loss: 0.04874922335147858\n",
      "epoch: 11 trial 5080 training loss: 0.09722569771111012\n",
      "epoch: 11 trial 5081 training loss: 0.02207250241190195\n",
      "epoch: 11 trial 5082 training loss: 0.03576668631285429\n",
      "epoch: 11 trial 5083 training loss: 0.04020348936319351\n",
      "epoch: 11 trial 5084 training loss: 0.02281143795698881\n",
      "epoch: 11 trial 5085 training loss: 0.0548100546002388\n",
      "epoch: 11 trial 5086 training loss: 0.01915240753442049\n",
      "epoch: 11 trial 5087 training loss: 0.05136995483189821\n",
      "epoch: 11 trial 5088 training loss: 0.02486393228173256\n",
      "epoch: 11 trial 5089 training loss: 0.025747152045369148\n",
      "epoch: 11 trial 5090 training loss: 0.04675064887851477\n",
      "epoch: 11 trial 5091 training loss: 0.011573099298402667\n",
      "epoch: 11 trial 5092 training loss: 0.004875791375525296\n",
      "epoch: 11 trial 5093 training loss: 0.027095451019704342\n",
      "epoch: 11 trial 5094 training loss: 0.09427683427929878\n",
      "epoch: 11 trial 5095 training loss: 0.019534670747816563\n",
      "epoch: 11 trial 5096 training loss: 0.07882067654281855\n",
      "epoch: 11 trial 5097 training loss: 0.037530320696532726\n",
      "epoch: 11 trial 5098 training loss: 0.021815645042806864\n",
      "epoch: 11 trial 5099 training loss: 0.17121177539229393\n",
      "epoch: 11 trial 5100 training loss: 0.015376647934317589\n",
      "epoch: 11 trial 5101 training loss: 0.015114102745428681\n",
      "epoch: 11 trial 5102 training loss: 0.011555427452549338\n",
      "epoch: 11 trial 5103 training loss: 0.0036725911777466536\n",
      "epoch: 11 trial 5104 training loss: 0.024479299318045378\n",
      "epoch: 11 trial 5105 training loss: 0.045052734203636646\n",
      "epoch: 11 trial 5106 training loss: 0.040553271770477295\n",
      "epoch: 11 trial 5107 training loss: 0.03599351458251476\n",
      "epoch: 11 trial 5108 training loss: 0.03404489532113075\n",
      "epoch: 11 trial 5109 training loss: 0.02506049396470189\n",
      "epoch: 11 trial 5110 training loss: 0.07410219125449657\n",
      "epoch: 11 trial 5111 training loss: 0.015503205824643373\n",
      "epoch: 11 trial 5112 training loss: 0.007484895875677466\n",
      "epoch: 11 trial 5113 training loss: 0.022447763476520777\n",
      "epoch: 11 trial 5114 training loss: 0.005849760607816279\n",
      "epoch: 11 trial 5115 training loss: 0.06606626883149147\n",
      "epoch: 11 trial 5116 training loss: 0.005012799287214875\n",
      "epoch: 11 trial 5117 training loss: 0.023108302149921656\n",
      "epoch: 11 trial 5118 training loss: 0.014087793417274952\n",
      "epoch: 11 trial 5119 training loss: 0.06348962895572186\n",
      "epoch: 11 trial 5120 training loss: 0.03044545277953148\n",
      "epoch: 11 trial 5121 training loss: 0.02216357970610261\n",
      "epoch: 11 trial 5122 training loss: 0.05170348100364208\n",
      "epoch: 11 trial 5123 training loss: 0.07176224887371063\n",
      "epoch: 11 trial 5124 training loss: 0.030933452770113945\n",
      "epoch: 11 trial 5125 training loss: 0.07190787605941296\n",
      "epoch: 11 trial 5126 training loss: 0.02053010230883956\n",
      "epoch: 11 trial 5127 training loss: 0.08015768229961395\n",
      "epoch: 11 trial 5128 training loss: 0.01859972905367613\n",
      "epoch: 11 trial 5129 training loss: 0.010222771437838674\n",
      "epoch: 11 trial 5130 training loss: 0.053282071836292744\n",
      "epoch: 11 trial 5131 training loss: 0.0280762892216444\n",
      "epoch: 11 trial 5132 training loss: 0.017092346446588635\n",
      "epoch: 11 trial 5133 training loss: 0.017370134592056274\n",
      "epoch: 11 trial 5134 training loss: 0.009273598669096828\n",
      "epoch: 11 trial 5135 training loss: 0.022773911245167255\n",
      "epoch: 11 trial 5136 training loss: 0.029955831356346607\n",
      "epoch: 11 trial 5137 training loss: 0.017750509548932314\n",
      "epoch: 11 trial 5138 training loss: 0.03423260059207678\n",
      "epoch: 11 trial 5139 training loss: 0.09033040888607502\n",
      "epoch: 11 trial 5140 training loss: 0.0353956138715148\n",
      "epoch: 11 trial 5141 training loss: 0.016307800076901913\n",
      "epoch: 11 trial 5142 training loss: 0.005844469298608601\n",
      "epoch: 11 trial 5143 training loss: 0.00883967848494649\n",
      "epoch: 11 trial 5144 training loss: 0.026808183174580336\n",
      "epoch: 11 trial 5145 training loss: 0.0520313149318099\n",
      "epoch: 11 trial 5146 training loss: 0.046115160919725895\n",
      "epoch: 11 trial 5147 training loss: 0.002819910296238959\n",
      "epoch: 11 trial 5148 training loss: 0.03177788760513067\n",
      "epoch: 11 trial 5149 training loss: 0.041385550051927567\n",
      "epoch: 11 trial 5150 training loss: 0.022266339976340532\n",
      "epoch: 11 trial 5151 training loss: 0.014106053160503507\n",
      "epoch: 11 trial 5152 training loss: 0.01869130600243807\n",
      "epoch: 11 trial 5153 training loss: 0.03897483740001917\n",
      "epoch: 11 trial 5154 training loss: 0.01994341635145247\n",
      "epoch: 11 trial 5155 training loss: 0.0360316289588809\n",
      "epoch: 11 trial 5156 training loss: 0.06389749236404896\n",
      "epoch: 11 trial 5157 training loss: 0.02888704091310501\n",
      "epoch: 11 trial 5158 training loss: 0.034965259954333305\n",
      "epoch: 11 trial 5159 training loss: 0.07039090804755688\n",
      "epoch: 11 trial 5160 training loss: 0.03301526606082916\n",
      "epoch: 11 trial 5161 training loss: 0.06733266077935696\n",
      "epoch: 11 trial 5162 training loss: 0.018011301290243864\n",
      "epoch: 11 trial 5163 training loss: 0.03190462198108435\n",
      "epoch: 11 trial 5164 training loss: 0.025472577661275864\n",
      "epoch: 11 trial 5165 training loss: 0.021638725884258747\n",
      "epoch: 11 trial 5166 training loss: 0.022458523977547884\n",
      "epoch: 11 trial 5167 training loss: 0.09286821633577347\n",
      "epoch: 11 trial 5168 training loss: 0.012639148160815239\n",
      "epoch: 11 trial 5169 training loss: 0.08616337366402149\n",
      "epoch: 11 trial 5170 training loss: 0.024607899598777294\n",
      "epoch: 11 trial 5171 training loss: 0.006875397637486458\n",
      "epoch: 11 trial 5172 training loss: 0.05463410262018442\n",
      "epoch: 11 trial 5173 training loss: 0.06085457280278206\n",
      "epoch: 11 trial 5174 training loss: 0.009757025865837932\n",
      "epoch: 11 trial 5175 training loss: 0.033595441840589046\n",
      "epoch: 11 trial 5176 training loss: 0.007896477822214365\n",
      "epoch: 11 trial 5177 training loss: 0.030554034747183323\n",
      "epoch: 11 trial 5178 training loss: 0.020420029293745756\n",
      "epoch: 11 trial 5179 training loss: 0.035988643765449524\n",
      "epoch: 11 trial 5180 training loss: 0.2589634284377098\n",
      "epoch: 11 trial 5181 training loss: 0.051640136167407036\n",
      "epoch: 11 trial 5182 training loss: 0.008530986728146672\n",
      "epoch: 11 trial 5183 training loss: 0.0215824986808002\n",
      "epoch: 11 trial 5184 training loss: 0.144791379570961\n",
      "epoch: 11 trial 5185 training loss: 0.049963103607296944\n",
      "epoch: 11 trial 5186 training loss: 0.07495284453034401\n",
      "epoch: 11 trial 5187 training loss: 0.03628484671935439\n",
      "epoch: 11 trial 5188 training loss: 0.03624145593494177\n",
      "epoch: 11 trial 5189 training loss: 0.015625304076820612\n",
      "epoch: 11 trial 5190 training loss: 0.08127606846392155\n",
      "epoch: 11 trial 5191 training loss: 0.01916299620643258\n",
      "epoch: 11 trial 5192 training loss: 0.07118359580636024\n",
      "epoch: 11 trial 5193 training loss: 0.023097821045666933\n",
      "epoch: 11 trial 5194 training loss: 0.010103740030899644\n",
      "epoch: 11 trial 5195 training loss: 0.033807859756052494\n",
      "epoch: 11 trial 5196 training loss: 0.005204828106798232\n",
      "epoch: 11 trial 5197 training loss: 0.015618600882589817\n",
      "epoch: 11 trial 5198 training loss: 0.0819342527538538\n",
      "epoch: 11 trial 5199 training loss: 0.07947931066155434\n",
      "epoch: 11 trial 5200 training loss: 0.024022412486374378\n",
      "epoch: 11 trial 5201 training loss: 0.10842405632138252\n",
      "epoch: 11 trial 5202 training loss: 0.10356733947992325\n",
      "epoch: 11 trial 5203 training loss: 0.030866198241710663\n",
      "epoch: 11 trial 5204 training loss: 0.06129076890647411\n",
      "epoch: 11 trial 5205 training loss: 0.0399660337716341\n",
      "epoch: 11 trial 5206 training loss: 0.011547713074833155\n",
      "epoch: 11 trial 5207 training loss: 0.00810185493901372\n",
      "epoch: 11 trial 5208 training loss: 0.14925634115934372\n",
      "epoch: 11 trial 5209 training loss: 0.06408408097922802\n",
      "epoch: 11 trial 5210 training loss: 0.04470672830939293\n",
      "epoch: 11 trial 5211 training loss: 0.05565957352519035\n",
      "epoch: 11 trial 5212 training loss: 0.025300548411905766\n",
      "epoch: 11 trial 5213 training loss: 0.0316206244751811\n",
      "epoch: 11 trial 5214 training loss: 0.017986455466598272\n",
      "epoch: 11 trial 5215 training loss: 0.04421130754053593\n",
      "epoch: 11 trial 5216 training loss: 0.0350243765860796\n",
      "epoch: 11 trial 5217 training loss: 0.041813112795352936\n",
      "epoch: 11 trial 5218 training loss: 0.08384127728641033\n",
      "epoch: 11 trial 5219 training loss: 0.03183853207156062\n",
      "epoch: 11 trial 5220 training loss: 0.01402108813636005\n",
      "epoch: 11 trial 5221 training loss: 0.030664322897791862\n",
      "epoch: 11 trial 5222 training loss: 0.0035369517281651497\n",
      "epoch: 11 trial 5223 training loss: 0.041999924927949905\n",
      "epoch: 11 trial 5224 training loss: 0.0378972589969635\n",
      "epoch: 11 trial 5225 training loss: 0.019245754927396774\n",
      "epoch: 11 trial 5226 training loss: 0.003555554256308824\n",
      "epoch: 11 trial 5227 training loss: 0.011391013162210584\n",
      "epoch: 11 trial 5228 training loss: 0.01685113739222288\n",
      "epoch: 11 trial 5229 training loss: 0.039509222842752934\n",
      "epoch: 11 trial 5230 training loss: 0.0058562608901411295\n",
      "epoch: 11 trial 5231 training loss: 0.0036161885946057737\n",
      "epoch: 11 trial 5232 training loss: 0.005393672385253012\n",
      "epoch: 11 trial 5233 training loss: 0.009258006932213902\n",
      "epoch: 11 trial 5234 training loss: 0.013263057451695204\n",
      "epoch: 11 trial 5235 training loss: 0.023906919173896313\n",
      "epoch: 11 trial 5236 training loss: 0.10606991872191429\n",
      "epoch: 11 trial 5237 training loss: 0.026640173513442278\n",
      "epoch: 11 trial 5238 training loss: 0.023801286704838276\n",
      "epoch: 11 trial 5239 training loss: 0.020686420146375895\n",
      "epoch: 11 trial 5240 training loss: 0.026783255860209465\n",
      "epoch: 11 trial 5241 training loss: 0.025605048518627882\n",
      "epoch: 11 trial 5242 training loss: 0.02987239882349968\n",
      "epoch: 11 trial 5243 training loss: 0.0336112966760993\n",
      "epoch: 11 trial 5244 training loss: 0.0840931348502636\n",
      "epoch: 11 trial 5245 training loss: 0.10898537188768387\n",
      "epoch: 11 trial 5246 training loss: 0.02312805224210024\n",
      "epoch: 11 trial 5247 training loss: 0.026308687403798103\n",
      "epoch: 11 trial 5248 training loss: 0.07438988238573074\n",
      "epoch: 11 trial 5249 training loss: 0.015298459213227034\n",
      "epoch: 11 trial 5250 training loss: 0.06533386558294296\n",
      "epoch: 11 trial 5251 training loss: 0.10240902565419674\n",
      "epoch: 11 trial 5252 training loss: 0.09859847091138363\n",
      "epoch: 11 trial 5253 training loss: 0.03685883339494467\n",
      "epoch: 11 trial 5254 training loss: 0.09589118510484695\n",
      "epoch: 11 trial 5255 training loss: 0.0324352728202939\n",
      "epoch: 11 trial 5256 training loss: 0.06704131327569485\n",
      "epoch: 11 trial 5257 training loss: 0.037538292817771435\n",
      "epoch: 11 trial 5258 training loss: 0.08140416257083416\n",
      "epoch: 11 trial 5259 training loss: 0.010265838587656617\n",
      "epoch: 11 trial 5260 training loss: 0.005736097926273942\n",
      "epoch: 11 trial 5261 training loss: 0.022750284522771835\n",
      "epoch: 11 trial 5262 training loss: 0.027136393822729588\n",
      "epoch: 11 trial 5263 training loss: 0.05625765211880207\n",
      "epoch: 11 trial 5264 training loss: 0.04842790309339762\n",
      "epoch: 11 trial 5265 training loss: 0.01467343932017684\n",
      "epoch: 11 trial 5266 training loss: 0.05330410599708557\n",
      "epoch: 11 trial 5267 training loss: 0.02269432693719864\n",
      "epoch: 11 trial 5268 training loss: 0.013606857042759657\n",
      "epoch: 11 trial 5269 training loss: 0.02040994819253683\n",
      "epoch: 11 trial 5270 training loss: 0.01440957235172391\n",
      "epoch: 11 trial 5271 training loss: 0.01934225345030427\n",
      "epoch: 11 trial 5272 training loss: 0.040957643650472164\n",
      "epoch: 11 trial 5273 training loss: 0.03170868847519159\n",
      "epoch: 11 trial 5274 training loss: 0.02572030620649457\n",
      "epoch: 11 trial 5275 training loss: 0.020636040717363358\n",
      "epoch: 11 trial 5276 training loss: 0.016401262022554874\n",
      "epoch: 11 trial 5277 training loss: 0.006914586992934346\n",
      "epoch: 11 trial 5278 training loss: 0.015755809377878904\n",
      "epoch: 11 trial 5279 training loss: 0.01517782686278224\n",
      "epoch: 11 trial 5280 training loss: 0.016245105769485235\n",
      "epoch: 11 trial 5281 training loss: 0.00575800915248692\n",
      "epoch: 11 trial 5282 training loss: 0.012774298898875713\n",
      "epoch: 11 trial 5283 training loss: 0.0808253102004528\n",
      "epoch: 11 trial 5284 training loss: 0.013584844768047333\n",
      "epoch: 11 trial 5285 training loss: 0.036316460929811\n",
      "epoch: 11 trial 5286 training loss: 0.01565250102430582\n",
      "epoch: 11 trial 5287 training loss: 0.05077546648681164\n",
      "epoch: 11 trial 5288 training loss: 0.040016770362854004\n",
      "epoch: 11 trial 5289 training loss: 0.038743771612644196\n",
      "epoch: 11 trial 5290 training loss: 0.029234616085886955\n",
      "epoch: 11 trial 5291 training loss: 0.016682925634086132\n",
      "epoch: 11 trial 5292 training loss: 0.10389138013124466\n",
      "epoch: 11 trial 5293 training loss: 0.006232136161997914\n",
      "epoch: 11 trial 5294 training loss: 0.01683498825877905\n",
      "epoch: 11 trial 5295 training loss: 0.12666526809334755\n",
      "epoch: 11 trial 5296 training loss: 0.02492780890315771\n",
      "epoch: 11 trial 5297 training loss: 0.021572626195847988\n",
      "epoch: 11 trial 5298 training loss: 0.019077940843999386\n",
      "epoch: 11 trial 5299 training loss: 0.012438280507922173\n",
      "epoch: 11 trial 5300 training loss: 0.0261765718460083\n",
      "epoch: 11 trial 5301 training loss: 0.012783702928572893\n",
      "epoch: 11 trial 5302 training loss: 0.05123593285679817\n",
      "epoch: 11 trial 5303 training loss: 0.15256869047880173\n",
      "epoch: 11 trial 5304 training loss: 0.07509163580834866\n",
      "epoch: 11 trial 5305 training loss: 0.02729772310703993\n",
      "epoch: 11 trial 5306 training loss: 0.09167008846998215\n",
      "epoch: 11 trial 5307 training loss: 0.106377724558115\n",
      "epoch: 11 trial 5308 training loss: 0.0245840554125607\n",
      "epoch: 11 trial 5309 training loss: 0.07572621293365955\n",
      "epoch: 11 trial 5310 training loss: 0.007050132029689848\n",
      "epoch: 11 trial 5311 training loss: 0.05045196507126093\n",
      "epoch: 11 trial 5312 training loss: 0.012991435825824738\n",
      "epoch: 11 trial 5313 training loss: 0.027687507681548595\n",
      "epoch: 11 trial 5314 training loss: 0.10516661778092384\n",
      "epoch: 11 trial 5315 training loss: 0.043121554888784885\n",
      "epoch: 11 trial 5316 training loss: 0.015456224326044321\n",
      "epoch: 11 trial 5317 training loss: 0.022902030497789383\n",
      "epoch: 11 trial 5318 training loss: 0.07116175070405006\n",
      "epoch: 11 trial 5319 training loss: 0.012497029500082135\n",
      "epoch: 11 trial 5320 training loss: 0.047469266690313816\n",
      "epoch: 11 trial 5321 training loss: 0.03992533218115568\n",
      "epoch: 11 trial 5322 training loss: 0.032737718895077705\n",
      "epoch: 11 trial 5323 training loss: 0.011146763106808066\n",
      "epoch: 11 trial 5324 training loss: 0.054740218445658684\n",
      "epoch: 12 trial 5325 training loss: 0.010464324615895748\n",
      "epoch: 12 trial 5326 training loss: 0.013398746959865093\n",
      "epoch: 12 trial 5327 training loss: 0.014192616799846292\n",
      "epoch: 12 trial 5328 training loss: 0.06025356613099575\n",
      "epoch: 12 trial 5329 training loss: 0.004152381210587919\n",
      "epoch: 12 trial 5330 training loss: 0.005506530636921525\n",
      "epoch: 12 trial 5331 training loss: 0.027200180105865\n",
      "epoch: 12 trial 5332 training loss: 0.02377892890945077\n",
      "epoch: 12 trial 5333 training loss: 0.006062150117941201\n",
      "epoch: 12 trial 5334 training loss: 0.013882149010896683\n",
      "epoch: 12 trial 5335 training loss: 0.053985923528671265\n",
      "epoch: 12 trial 5336 training loss: 0.03000617679208517\n",
      "epoch: 12 trial 5337 training loss: 0.019540302455425262\n",
      "epoch: 12 trial 5338 training loss: 0.03457118384540081\n",
      "epoch: 12 trial 5339 training loss: 0.015103631187230349\n",
      "epoch: 12 trial 5340 training loss: 0.02312237210571766\n",
      "epoch: 12 trial 5341 training loss: 0.11910467222332954\n",
      "epoch: 12 trial 5342 training loss: 0.05102996062487364\n",
      "epoch: 12 trial 5343 training loss: 0.012764960527420044\n",
      "epoch: 12 trial 5344 training loss: 0.019662872422486544\n",
      "epoch: 12 trial 5345 training loss: 0.01719959732145071\n",
      "epoch: 12 trial 5346 training loss: 0.0762582365423441\n",
      "epoch: 12 trial 5347 training loss: 0.009311905363574624\n",
      "epoch: 12 trial 5348 training loss: 0.00499217351898551\n",
      "epoch: 12 trial 5349 training loss: 0.14397742971777916\n",
      "epoch: 12 trial 5350 training loss: 0.0257736649364233\n",
      "epoch: 12 trial 5351 training loss: 0.01012899843044579\n",
      "epoch: 12 trial 5352 training loss: 0.10201732069253922\n",
      "epoch: 12 trial 5353 training loss: 0.01921833772212267\n",
      "epoch: 12 trial 5354 training loss: 0.023600909393280745\n",
      "epoch: 12 trial 5355 training loss: 0.023202547803521156\n",
      "epoch: 12 trial 5356 training loss: 0.014247844461351633\n",
      "epoch: 12 trial 5357 training loss: 0.0314013734459877\n",
      "epoch: 12 trial 5358 training loss: 0.04694115184247494\n",
      "epoch: 12 trial 5359 training loss: 0.019220544956624508\n",
      "epoch: 12 trial 5360 training loss: 0.06792961433529854\n",
      "epoch: 12 trial 5361 training loss: 0.1585889458656311\n",
      "epoch: 12 trial 5362 training loss: 0.2985125929117203\n",
      "epoch: 12 trial 5363 training loss: 0.03711901884526014\n",
      "epoch: 12 trial 5364 training loss: 0.09759818948805332\n",
      "epoch: 12 trial 5365 training loss: 0.10606570541858673\n",
      "epoch: 12 trial 5366 training loss: 0.011811485281214118\n",
      "epoch: 12 trial 5367 training loss: 0.09880102425813675\n",
      "epoch: 12 trial 5368 training loss: 0.04643503203988075\n",
      "epoch: 12 trial 5369 training loss: 0.027093456126749516\n",
      "epoch: 12 trial 5370 training loss: 0.05403120443224907\n",
      "epoch: 12 trial 5371 training loss: 0.014035695232450962\n",
      "epoch: 12 trial 5372 training loss: 0.024206953588873148\n",
      "epoch: 12 trial 5373 training loss: 0.01224971143528819\n",
      "epoch: 12 trial 5374 training loss: 0.09094082936644554\n",
      "epoch: 12 trial 5375 training loss: 0.03648770321160555\n",
      "epoch: 12 trial 5376 training loss: 0.12577789276838303\n",
      "epoch: 12 trial 5377 training loss: 0.04110966436564922\n",
      "epoch: 12 trial 5378 training loss: 0.0427596028894186\n",
      "epoch: 12 trial 5379 training loss: 0.033284434117376804\n",
      "epoch: 12 trial 5380 training loss: 0.06298146769404411\n",
      "epoch: 12 trial 5381 training loss: 0.027843937277793884\n",
      "epoch: 12 trial 5382 training loss: 0.018021087162196636\n",
      "epoch: 12 trial 5383 training loss: 0.025624281261116266\n",
      "epoch: 12 trial 5384 training loss: 0.08783860690891743\n",
      "epoch: 12 trial 5385 training loss: 0.020692740101367235\n",
      "epoch: 12 trial 5386 training loss: 0.10454344563186169\n",
      "epoch: 12 trial 5387 training loss: 0.02160367462784052\n",
      "epoch: 12 trial 5388 training loss: 0.00478125037625432\n",
      "epoch: 12 trial 5389 training loss: 0.034515016712248325\n",
      "epoch: 12 trial 5390 training loss: 0.04660162515938282\n",
      "epoch: 12 trial 5391 training loss: 0.021958591882139444\n",
      "epoch: 12 trial 5392 training loss: 0.02282871538773179\n",
      "epoch: 12 trial 5393 training loss: 0.046084373258054256\n",
      "epoch: 12 trial 5394 training loss: 0.03105857502669096\n",
      "epoch: 12 trial 5395 training loss: 0.044549438171088696\n",
      "epoch: 12 trial 5396 training loss: 0.03156371833756566\n",
      "epoch: 12 trial 5397 training loss: 0.015570915769785643\n",
      "epoch: 12 trial 5398 training loss: 0.07210488617420197\n",
      "epoch: 12 trial 5399 training loss: 0.011744669172912836\n",
      "epoch: 12 trial 5400 training loss: 0.02380648022517562\n",
      "epoch: 12 trial 5401 training loss: 0.028999012894928455\n",
      "epoch: 12 trial 5402 training loss: 0.08412086218595505\n",
      "epoch: 12 trial 5403 training loss: 0.01920892857015133\n",
      "epoch: 12 trial 5404 training loss: 0.05550750996917486\n",
      "epoch: 12 trial 5405 training loss: 0.024578153621405363\n",
      "epoch: 12 trial 5406 training loss: 0.04549744538962841\n",
      "epoch: 12 trial 5407 training loss: 0.06477232277393341\n",
      "epoch: 12 trial 5408 training loss: 0.020099062472581863\n",
      "epoch: 12 trial 5409 training loss: 0.05872428975999355\n",
      "epoch: 12 trial 5410 training loss: 0.014966475311666727\n",
      "epoch: 12 trial 5411 training loss: 0.030449202749878168\n",
      "epoch: 12 trial 5412 training loss: 0.0846901647746563\n",
      "epoch: 12 trial 5413 training loss: 0.04912564158439636\n",
      "epoch: 12 trial 5414 training loss: 0.0054242657497525215\n",
      "epoch: 12 trial 5415 training loss: 0.038569699972867966\n",
      "epoch: 12 trial 5416 training loss: 0.023755181580781937\n",
      "epoch: 12 trial 5417 training loss: 0.02228682115674019\n",
      "epoch: 12 trial 5418 training loss: 0.06381011381745338\n",
      "epoch: 12 trial 5419 training loss: 0.04669190850108862\n",
      "epoch: 12 trial 5420 training loss: 0.05040053930133581\n",
      "epoch: 12 trial 5421 training loss: 0.015267377719283104\n",
      "epoch: 12 trial 5422 training loss: 0.08577539213001728\n",
      "epoch: 12 trial 5423 training loss: 0.032123155891895294\n",
      "epoch: 12 trial 5424 training loss: 0.036252325400710106\n",
      "epoch: 12 trial 5425 training loss: 0.021356951911002398\n",
      "epoch: 12 trial 5426 training loss: 0.08853244595229626\n",
      "epoch: 12 trial 5427 training loss: 0.04334832541644573\n",
      "epoch: 12 trial 5428 training loss: 0.03363243490457535\n",
      "epoch: 12 trial 5429 training loss: 0.23172594606876373\n",
      "epoch: 12 trial 5430 training loss: 0.05934854969382286\n",
      "epoch: 12 trial 5431 training loss: 0.025432765018194914\n",
      "epoch: 12 trial 5432 training loss: 0.16730932518839836\n",
      "epoch: 12 trial 5433 training loss: 0.0857725739479065\n",
      "epoch: 12 trial 5434 training loss: 0.024698708206415176\n",
      "epoch: 12 trial 5435 training loss: 0.019101555924862623\n",
      "epoch: 12 trial 5436 training loss: 0.02497824188321829\n",
      "epoch: 12 trial 5437 training loss: 0.010269787395372987\n",
      "epoch: 12 trial 5438 training loss: 0.008034965489059687\n",
      "epoch: 12 trial 5439 training loss: 0.05744568072259426\n",
      "epoch: 12 trial 5440 training loss: 0.01660917908884585\n",
      "epoch: 12 trial 5441 training loss: 0.02457442879676819\n",
      "epoch: 12 trial 5442 training loss: 0.036060700193047523\n",
      "epoch: 12 trial 5443 training loss: 0.031662361696362495\n",
      "epoch: 12 trial 5444 training loss: 0.030833506025373936\n",
      "epoch: 12 trial 5445 training loss: 0.02090935641899705\n",
      "epoch: 12 trial 5446 training loss: 0.043460967019200325\n",
      "epoch: 12 trial 5447 training loss: 0.07549157366156578\n",
      "epoch: 12 trial 5448 training loss: 0.043050168082118034\n",
      "epoch: 12 trial 5449 training loss: 0.009298649150878191\n",
      "epoch: 12 trial 5450 training loss: 0.022769110277295113\n",
      "epoch: 12 trial 5451 training loss: 0.021075789351016283\n",
      "epoch: 12 trial 5452 training loss: 0.027797299902886152\n",
      "epoch: 12 trial 5453 training loss: 0.15637065842747688\n",
      "epoch: 12 trial 5454 training loss: 0.03226111922413111\n",
      "epoch: 12 trial 5455 training loss: 0.05787576362490654\n",
      "epoch: 12 trial 5456 training loss: 0.025367032969370484\n",
      "epoch: 12 trial 5457 training loss: 0.03314715065062046\n",
      "epoch: 12 trial 5458 training loss: 0.05198316927999258\n",
      "epoch: 12 trial 5459 training loss: 0.009905119892209768\n",
      "epoch: 12 trial 5460 training loss: 0.04309285990893841\n",
      "epoch: 12 trial 5461 training loss: 0.01710646692663431\n",
      "epoch: 12 trial 5462 training loss: 0.10745477676391602\n",
      "epoch: 12 trial 5463 training loss: 0.016909488011151552\n",
      "epoch: 12 trial 5464 training loss: 0.0631274338811636\n",
      "epoch: 12 trial 5465 training loss: 0.03463962487876415\n",
      "epoch: 12 trial 5466 training loss: 0.029613674618303776\n",
      "epoch: 12 trial 5467 training loss: 0.02474565338343382\n",
      "epoch: 12 trial 5468 training loss: 0.09251641295850277\n",
      "epoch: 12 trial 5469 training loss: 0.014886352932080626\n",
      "epoch: 12 trial 5470 training loss: 0.014987415866926312\n",
      "epoch: 12 trial 5471 training loss: 0.10383832827210426\n",
      "epoch: 12 trial 5472 training loss: 0.015656461473554373\n",
      "epoch: 12 trial 5473 training loss: 0.03748986404389143\n",
      "epoch: 12 trial 5474 training loss: 0.05754889082163572\n",
      "epoch: 12 trial 5475 training loss: 0.0297562088817358\n",
      "epoch: 12 trial 5476 training loss: 0.023082245141267776\n",
      "epoch: 12 trial 5477 training loss: 0.016951943282037973\n",
      "epoch: 12 trial 5478 training loss: 0.03607117664068937\n",
      "epoch: 12 trial 5479 training loss: 0.011549016460776329\n",
      "epoch: 12 trial 5480 training loss: 0.01277377363294363\n",
      "epoch: 12 trial 5481 training loss: 0.027524298056960106\n",
      "epoch: 12 trial 5482 training loss: 0.003993956372141838\n",
      "epoch: 12 trial 5483 training loss: 0.012861247407272458\n",
      "epoch: 12 trial 5484 training loss: 0.0202605570666492\n",
      "epoch: 12 trial 5485 training loss: 0.010250263847410679\n",
      "epoch: 12 trial 5486 training loss: 0.055309997871518135\n",
      "epoch: 12 trial 5487 training loss: 0.003814792027696967\n",
      "epoch: 12 trial 5488 training loss: 0.058362171053886414\n",
      "epoch: 12 trial 5489 training loss: 0.007834936957806349\n",
      "epoch: 12 trial 5490 training loss: 0.02189155761152506\n",
      "epoch: 12 trial 5491 training loss: 0.026571672409772873\n",
      "epoch: 12 trial 5492 training loss: 0.011090102139860392\n",
      "epoch: 12 trial 5493 training loss: 0.0366583401337266\n",
      "epoch: 12 trial 5494 training loss: 0.12001955509185791\n",
      "epoch: 12 trial 5495 training loss: 0.04618302546441555\n",
      "epoch: 12 trial 5496 training loss: 0.007758133579045534\n",
      "epoch: 12 trial 5497 training loss: 0.05851332098245621\n",
      "epoch: 12 trial 5498 training loss: 0.02715971227735281\n",
      "epoch: 12 trial 5499 training loss: 0.025375787168741226\n",
      "epoch: 12 trial 5500 training loss: 0.023522009141743183\n",
      "epoch: 12 trial 5501 training loss: 0.021209324710071087\n",
      "epoch: 12 trial 5502 training loss: 0.04800868406891823\n",
      "epoch: 12 trial 5503 training loss: 0.045727163553237915\n",
      "epoch: 12 trial 5504 training loss: 0.04043669905513525\n",
      "epoch: 12 trial 5505 training loss: 0.07110071741044521\n",
      "epoch: 12 trial 5506 training loss: 0.015613134484738111\n",
      "epoch: 12 trial 5507 training loss: 0.04291709512472153\n",
      "epoch: 12 trial 5508 training loss: 0.021840648725628853\n",
      "epoch: 12 trial 5509 training loss: 0.017744028009474277\n",
      "epoch: 12 trial 5510 training loss: 0.042491731233894825\n",
      "epoch: 12 trial 5511 training loss: 0.06300108693540096\n",
      "epoch: 12 trial 5512 training loss: 0.019938011188060045\n",
      "epoch: 12 trial 5513 training loss: 0.04264149349182844\n",
      "epoch: 12 trial 5514 training loss: 0.057029896415770054\n",
      "epoch: 12 trial 5515 training loss: 0.062292614951729774\n",
      "epoch: 12 trial 5516 training loss: 0.04698531795293093\n",
      "epoch: 12 trial 5517 training loss: 0.029903382994234562\n",
      "epoch: 12 trial 5518 training loss: 0.01698083570227027\n",
      "epoch: 12 trial 5519 training loss: 0.0339795108884573\n",
      "epoch: 12 trial 5520 training loss: 0.025653631426393986\n",
      "epoch: 12 trial 5521 training loss: 0.025256041903048754\n",
      "epoch: 12 trial 5522 training loss: 0.061905913054943085\n",
      "epoch: 12 trial 5523 training loss: 0.09879809431731701\n",
      "epoch: 12 trial 5524 training loss: 0.011936675058677793\n",
      "epoch: 12 trial 5525 training loss: 0.0059587855357676744\n",
      "epoch: 12 trial 5526 training loss: 0.01002181926742196\n",
      "epoch: 12 trial 5527 training loss: 0.009223785484209657\n",
      "epoch: 12 trial 5528 training loss: 0.015472839120775461\n",
      "epoch: 12 trial 5529 training loss: 0.015538511332124472\n",
      "epoch: 12 trial 5530 training loss: 0.06196509674191475\n",
      "epoch: 12 trial 5531 training loss: 0.03839779272675514\n",
      "epoch: 12 trial 5532 training loss: 0.016988458577543497\n",
      "epoch: 12 trial 5533 training loss: 0.09043268486857414\n",
      "epoch: 12 trial 5534 training loss: 0.04476173222064972\n",
      "epoch: 12 trial 5535 training loss: 0.06185338273644447\n",
      "epoch: 12 trial 5536 training loss: 0.06537780724465847\n",
      "epoch: 12 trial 5537 training loss: 0.10801179148256779\n",
      "epoch: 12 trial 5538 training loss: 0.012485786573961377\n",
      "epoch: 12 trial 5539 training loss: 0.00911637395620346\n",
      "epoch: 12 trial 5540 training loss: 0.027701059356331825\n",
      "epoch: 12 trial 5541 training loss: 0.023915654979646206\n",
      "epoch: 12 trial 5542 training loss: 0.019720575772225857\n",
      "epoch: 12 trial 5543 training loss: 0.045009221881628036\n",
      "epoch: 12 trial 5544 training loss: 0.05683481879532337\n",
      "epoch: 12 trial 5545 training loss: 0.03357424773275852\n",
      "epoch: 12 trial 5546 training loss: 0.060794681310653687\n",
      "epoch: 12 trial 5547 training loss: 0.04585671424865723\n",
      "epoch: 12 trial 5548 training loss: 0.08822166919708252\n",
      "epoch: 12 trial 5549 training loss: 0.012569167651236057\n",
      "epoch: 12 trial 5550 training loss: 0.009537520119920373\n",
      "epoch: 12 trial 5551 training loss: 0.025250278413295746\n",
      "epoch: 12 trial 5552 training loss: 0.028689871076494455\n",
      "epoch: 12 trial 5553 training loss: 0.04938604962080717\n",
      "epoch: 12 trial 5554 training loss: 0.014907208736985922\n",
      "epoch: 12 trial 5555 training loss: 0.1546596996486187\n",
      "epoch: 12 trial 5556 training loss: 0.03260216303169727\n",
      "epoch: 12 trial 5557 training loss: 0.12823380529880524\n",
      "epoch: 12 trial 5558 training loss: 0.1022073607891798\n",
      "epoch: 12 trial 5559 training loss: 0.015322863822802901\n",
      "epoch: 12 trial 5560 training loss: 0.056307731196284294\n",
      "epoch: 12 trial 5561 training loss: 0.018746694549918175\n",
      "epoch: 12 trial 5562 training loss: 0.05275449901819229\n",
      "epoch: 12 trial 5563 training loss: 0.04770306684076786\n",
      "epoch: 12 trial 5564 training loss: 0.10014586709439754\n",
      "epoch: 12 trial 5565 training loss: 0.02187982387840748\n",
      "epoch: 12 trial 5566 training loss: 0.03539455961436033\n",
      "epoch: 12 trial 5567 training loss: 0.041518403217196465\n",
      "epoch: 12 trial 5568 training loss: 0.0245205108076334\n",
      "epoch: 12 trial 5569 training loss: 0.056496236473321915\n",
      "epoch: 12 trial 5570 training loss: 0.019309180323034525\n",
      "epoch: 12 trial 5571 training loss: 0.05254152603447437\n",
      "epoch: 12 trial 5572 training loss: 0.023267886601388454\n",
      "epoch: 12 trial 5573 training loss: 0.027966567315161228\n",
      "epoch: 12 trial 5574 training loss: 0.04804027732461691\n",
      "epoch: 12 trial 5575 training loss: 0.011851240647956729\n",
      "epoch: 12 trial 5576 training loss: 0.0052681578090414405\n",
      "epoch: 12 trial 5577 training loss: 0.027418942656368017\n",
      "epoch: 12 trial 5578 training loss: 0.09856958501040936\n",
      "epoch: 12 trial 5579 training loss: 0.018425364512950182\n",
      "epoch: 12 trial 5580 training loss: 0.0815650187432766\n",
      "epoch: 12 trial 5581 training loss: 0.038436850532889366\n",
      "epoch: 12 trial 5582 training loss: 0.021680264733731747\n",
      "epoch: 12 trial 5583 training loss: 0.17457466199994087\n",
      "epoch: 12 trial 5584 training loss: 0.014420883264392614\n",
      "epoch: 12 trial 5585 training loss: 0.013431372120976448\n",
      "epoch: 12 trial 5586 training loss: 0.011099289869889617\n",
      "epoch: 12 trial 5587 training loss: 0.003877638722769916\n",
      "epoch: 12 trial 5588 training loss: 0.024273826740682125\n",
      "epoch: 12 trial 5589 training loss: 0.040415589697659016\n",
      "epoch: 12 trial 5590 training loss: 0.04329932387918234\n",
      "epoch: 12 trial 5591 training loss: 0.033325064927339554\n",
      "epoch: 12 trial 5592 training loss: 0.03360163699835539\n",
      "epoch: 12 trial 5593 training loss: 0.023700847290456295\n",
      "epoch: 12 trial 5594 training loss: 0.07386377453804016\n",
      "epoch: 12 trial 5595 training loss: 0.015303636901080608\n",
      "epoch: 12 trial 5596 training loss: 0.007162332069128752\n",
      "epoch: 12 trial 5597 training loss: 0.02256394922733307\n",
      "epoch: 12 trial 5598 training loss: 0.006734318565577269\n",
      "epoch: 12 trial 5599 training loss: 0.06652946770191193\n",
      "epoch: 12 trial 5600 training loss: 0.005358953028917313\n",
      "epoch: 12 trial 5601 training loss: 0.022260327823460102\n",
      "epoch: 12 trial 5602 training loss: 0.013557785889133811\n",
      "epoch: 12 trial 5603 training loss: 0.06416225992143154\n",
      "epoch: 12 trial 5604 training loss: 0.03020673617720604\n",
      "epoch: 12 trial 5605 training loss: 0.02532381657510996\n",
      "epoch: 12 trial 5606 training loss: 0.04593760333955288\n",
      "epoch: 12 trial 5607 training loss: 0.06512610428035259\n",
      "epoch: 12 trial 5608 training loss: 0.03011750429868698\n",
      "epoch: 12 trial 5609 training loss: 0.07084674946963787\n",
      "epoch: 12 trial 5610 training loss: 0.021168642211705446\n",
      "epoch: 12 trial 5611 training loss: 0.07152116484940052\n",
      "epoch: 12 trial 5612 training loss: 0.020870785228908062\n",
      "epoch: 12 trial 5613 training loss: 0.009427906479686499\n",
      "epoch: 12 trial 5614 training loss: 0.04961317591369152\n",
      "epoch: 12 trial 5615 training loss: 0.02956494875252247\n",
      "epoch: 12 trial 5616 training loss: 0.016446438618004322\n",
      "epoch: 12 trial 5617 training loss: 0.015589606249704957\n",
      "epoch: 12 trial 5618 training loss: 0.0092068153899163\n",
      "epoch: 12 trial 5619 training loss: 0.021571874152868986\n",
      "epoch: 12 trial 5620 training loss: 0.02948659984394908\n",
      "epoch: 12 trial 5621 training loss: 0.016668816097080708\n",
      "epoch: 12 trial 5622 training loss: 0.03158100740984082\n",
      "epoch: 12 trial 5623 training loss: 0.07968419045209885\n",
      "epoch: 12 trial 5624 training loss: 0.03433317132294178\n",
      "epoch: 12 trial 5625 training loss: 0.01708461530506611\n",
      "epoch: 12 trial 5626 training loss: 0.005717465188354254\n",
      "epoch: 12 trial 5627 training loss: 0.00870802509598434\n",
      "epoch: 12 trial 5628 training loss: 0.027176604140549898\n",
      "epoch: 12 trial 5629 training loss: 0.055258793756365776\n",
      "epoch: 12 trial 5630 training loss: 0.04644263070076704\n",
      "epoch: 12 trial 5631 training loss: 0.002907456480897963\n",
      "epoch: 12 trial 5632 training loss: 0.030925882048904896\n",
      "epoch: 12 trial 5633 training loss: 0.03511940874159336\n",
      "epoch: 12 trial 5634 training loss: 0.019708608277142048\n",
      "epoch: 12 trial 5635 training loss: 0.015201271511614323\n",
      "epoch: 12 trial 5636 training loss: 0.017808240372687578\n",
      "epoch: 12 trial 5637 training loss: 0.04367642477154732\n",
      "epoch: 12 trial 5638 training loss: 0.02083172556012869\n",
      "epoch: 12 trial 5639 training loss: 0.034253811463713646\n",
      "epoch: 12 trial 5640 training loss: 0.06346273608505726\n",
      "epoch: 12 trial 5641 training loss: 0.02777529344893992\n",
      "epoch: 12 trial 5642 training loss: 0.03586098551750183\n",
      "epoch: 12 trial 5643 training loss: 0.0656751673668623\n",
      "epoch: 12 trial 5644 training loss: 0.03313918877393007\n",
      "epoch: 12 trial 5645 training loss: 0.07164201140403748\n",
      "epoch: 12 trial 5646 training loss: 0.01873519131913781\n",
      "epoch: 12 trial 5647 training loss: 0.03239105176180601\n",
      "epoch: 12 trial 5648 training loss: 0.025319970212876797\n",
      "epoch: 12 trial 5649 training loss: 0.02188335219398141\n",
      "epoch: 12 trial 5650 training loss: 0.024100748356431723\n",
      "epoch: 12 trial 5651 training loss: 0.08527904376387596\n",
      "epoch: 12 trial 5652 training loss: 0.01267359871417284\n",
      "epoch: 12 trial 5653 training loss: 0.08744647540152073\n",
      "epoch: 12 trial 5654 training loss: 0.023255276959389448\n",
      "epoch: 12 trial 5655 training loss: 0.007763738511130214\n",
      "epoch: 12 trial 5656 training loss: 0.062326837331056595\n",
      "epoch: 12 trial 5657 training loss: 0.057429565116763115\n",
      "epoch: 12 trial 5658 training loss: 0.009352550143375993\n",
      "epoch: 12 trial 5659 training loss: 0.03418128564953804\n",
      "epoch: 12 trial 5660 training loss: 0.009180072927847505\n",
      "epoch: 12 trial 5661 training loss: 0.03291172720491886\n",
      "epoch: 12 trial 5662 training loss: 0.020291629247367382\n",
      "epoch: 12 trial 5663 training loss: 0.03653744515031576\n",
      "epoch: 12 trial 5664 training loss: 0.2536851838231087\n",
      "epoch: 12 trial 5665 training loss: 0.0559832863509655\n",
      "epoch: 12 trial 5666 training loss: 0.007214049226604402\n",
      "epoch: 12 trial 5667 training loss: 0.020132775884121656\n",
      "epoch: 12 trial 5668 training loss: 0.13085510954260826\n",
      "epoch: 12 trial 5669 training loss: 0.04803258739411831\n",
      "epoch: 12 trial 5670 training loss: 0.06796116009354591\n",
      "epoch: 12 trial 5671 training loss: 0.03511037025600672\n",
      "epoch: 12 trial 5672 training loss: 0.03385414369404316\n",
      "epoch: 12 trial 5673 training loss: 0.014079260639846325\n",
      "epoch: 12 trial 5674 training loss: 0.07542290538549423\n",
      "epoch: 12 trial 5675 training loss: 0.0187541376799345\n",
      "epoch: 12 trial 5676 training loss: 0.08381113968789577\n",
      "epoch: 12 trial 5677 training loss: 0.019410809502005577\n",
      "epoch: 12 trial 5678 training loss: 0.007691131555475295\n",
      "epoch: 12 trial 5679 training loss: 0.03814078588038683\n",
      "epoch: 12 trial 5680 training loss: 0.005220379214733839\n",
      "epoch: 12 trial 5681 training loss: 0.015845927875488997\n",
      "epoch: 12 trial 5682 training loss: 0.08743610419332981\n",
      "epoch: 12 trial 5683 training loss: 0.06858815252780914\n",
      "epoch: 12 trial 5684 training loss: 0.02407730557024479\n",
      "epoch: 12 trial 5685 training loss: 0.10733488574624062\n",
      "epoch: 12 trial 5686 training loss: 0.10671169683337212\n",
      "epoch: 12 trial 5687 training loss: 0.03294456470757723\n",
      "epoch: 12 trial 5688 training loss: 0.06451576389372349\n",
      "epoch: 12 trial 5689 training loss: 0.03915225621312857\n",
      "epoch: 12 trial 5690 training loss: 0.010710868751630187\n",
      "epoch: 12 trial 5691 training loss: 0.007369745755568147\n",
      "epoch: 12 trial 5692 training loss: 0.15062982961535454\n",
      "epoch: 12 trial 5693 training loss: 0.05770999379456043\n",
      "epoch: 12 trial 5694 training loss: 0.042423710227012634\n",
      "epoch: 12 trial 5695 training loss: 0.05302240699529648\n",
      "epoch: 12 trial 5696 training loss: 0.02638756763190031\n",
      "epoch: 12 trial 5697 training loss: 0.03305576741695404\n",
      "epoch: 12 trial 5698 training loss: 0.01882863510400057\n",
      "epoch: 12 trial 5699 training loss: 0.04770115576684475\n",
      "epoch: 12 trial 5700 training loss: 0.03381325025111437\n",
      "epoch: 12 trial 5701 training loss: 0.041869026608765125\n",
      "epoch: 12 trial 5702 training loss: 0.08390375413000584\n",
      "epoch: 12 trial 5703 training loss: 0.023733629379421473\n",
      "epoch: 12 trial 5704 training loss: 0.01123863342218101\n",
      "epoch: 12 trial 5705 training loss: 0.02942311018705368\n",
      "epoch: 12 trial 5706 training loss: 0.003803565283305943\n",
      "epoch: 12 trial 5707 training loss: 0.041519694961607456\n",
      "epoch: 12 trial 5708 training loss: 0.04253736138343811\n",
      "epoch: 12 trial 5709 training loss: 0.021724415011703968\n",
      "epoch: 12 trial 5710 training loss: 0.004054143239045516\n",
      "epoch: 12 trial 5711 training loss: 0.004966904758475721\n",
      "epoch: 12 trial 5712 training loss: 0.04254798777401447\n",
      "epoch: 12 trial 5713 training loss: 0.04287415090948343\n",
      "epoch: 12 trial 5714 training loss: 0.005907472805120051\n",
      "epoch: 12 trial 5715 training loss: 0.00582655833568424\n",
      "epoch: 12 trial 5716 training loss: 0.006570149213075638\n",
      "epoch: 12 trial 5717 training loss: 0.011038952507078648\n",
      "epoch: 12 trial 5718 training loss: 0.013522117398679256\n",
      "epoch: 12 trial 5719 training loss: 0.031227867119014263\n",
      "epoch: 12 trial 5720 training loss: 0.11140001192688942\n",
      "epoch: 12 trial 5721 training loss: 0.03241956979036331\n",
      "epoch: 12 trial 5722 training loss: 0.01948387548327446\n",
      "epoch: 12 trial 5723 training loss: 0.04156874492764473\n",
      "epoch: 12 trial 5724 training loss: 0.039810278452932835\n",
      "epoch: 12 trial 5725 training loss: 0.028081921860575676\n",
      "epoch: 12 trial 5726 training loss: 0.03065517731010914\n",
      "epoch: 12 trial 5727 training loss: 0.047334928065538406\n",
      "epoch: 12 trial 5728 training loss: 0.10882480442523956\n",
      "epoch: 12 trial 5729 training loss: 0.12197351269423962\n",
      "epoch: 12 trial 5730 training loss: 0.02533558290451765\n",
      "epoch: 12 trial 5731 training loss: 0.029708865098655224\n",
      "epoch: 12 trial 5732 training loss: 0.07048964686691761\n",
      "epoch: 12 trial 5733 training loss: 0.01868213666602969\n",
      "epoch: 12 trial 5734 training loss: 0.07199489325284958\n",
      "epoch: 12 trial 5735 training loss: 0.10307355225086212\n",
      "epoch: 12 trial 5736 training loss: 0.10053964890539646\n",
      "epoch: 12 trial 5737 training loss: 0.039502707310020924\n",
      "epoch: 12 trial 5738 training loss: 0.10075203701853752\n",
      "epoch: 12 trial 5739 training loss: 0.024067554157227278\n",
      "epoch: 12 trial 5740 training loss: 0.06363807991147041\n",
      "epoch: 12 trial 5741 training loss: 0.037061840295791626\n",
      "epoch: 12 trial 5742 training loss: 0.09012785367667675\n",
      "epoch: 12 trial 5743 training loss: 0.007779254578053951\n",
      "epoch: 12 trial 5744 training loss: 0.005594503483735025\n",
      "epoch: 12 trial 5745 training loss: 0.01961422059684992\n",
      "epoch: 12 trial 5746 training loss: 0.03003290481865406\n",
      "epoch: 12 trial 5747 training loss: 0.05605437234044075\n",
      "epoch: 12 trial 5748 training loss: 0.050381376408040524\n",
      "epoch: 12 trial 5749 training loss: 0.014576088637113571\n",
      "epoch: 12 trial 5750 training loss: 0.05502863600850105\n",
      "epoch: 12 trial 5751 training loss: 0.019823662471026182\n",
      "epoch: 12 trial 5752 training loss: 0.015833185520023108\n",
      "epoch: 12 trial 5753 training loss: 0.020469497423619032\n",
      "epoch: 12 trial 5754 training loss: 0.01295048464089632\n",
      "epoch: 12 trial 5755 training loss: 0.019099597819149494\n",
      "epoch: 12 trial 5756 training loss: 0.04096609074622393\n",
      "epoch: 12 trial 5757 training loss: 0.029616146348416805\n",
      "epoch: 12 trial 5758 training loss: 0.026463383808732033\n",
      "epoch: 12 trial 5759 training loss: 0.02254529297351837\n",
      "epoch: 12 trial 5760 training loss: 0.017276575788855553\n",
      "epoch: 12 trial 5761 training loss: 0.005459957174025476\n",
      "epoch: 12 trial 5762 training loss: 0.013337972341105342\n",
      "epoch: 12 trial 5763 training loss: 0.015222262125462294\n",
      "epoch: 12 trial 5764 training loss: 0.015590372262522578\n",
      "epoch: 12 trial 5765 training loss: 0.0055307026486843824\n",
      "epoch: 12 trial 5766 training loss: 0.01300720777362585\n",
      "epoch: 12 trial 5767 training loss: 0.08719710074365139\n",
      "epoch: 12 trial 5768 training loss: 0.012908759526908398\n",
      "epoch: 12 trial 5769 training loss: 0.0350693603977561\n",
      "epoch: 12 trial 5770 training loss: 0.01406882656738162\n",
      "epoch: 12 trial 5771 training loss: 0.05064913537353277\n",
      "epoch: 12 trial 5772 training loss: 0.03688195813447237\n",
      "epoch: 12 trial 5773 training loss: 0.03895149379968643\n",
      "epoch: 12 trial 5774 training loss: 0.028892993927001953\n",
      "epoch: 12 trial 5775 training loss: 0.01716959150508046\n",
      "epoch: 12 trial 5776 training loss: 0.11589990183711052\n",
      "epoch: 12 trial 5777 training loss: 0.005795273697003722\n",
      "epoch: 12 trial 5778 training loss: 0.01696690684184432\n",
      "epoch: 12 trial 5779 training loss: 0.1303916648030281\n",
      "epoch: 12 trial 5780 training loss: 0.02900076936930418\n",
      "epoch: 12 trial 5781 training loss: 0.022793260402977467\n",
      "epoch: 12 trial 5782 training loss: 0.024424167815595865\n",
      "epoch: 12 trial 5783 training loss: 0.01242692326195538\n",
      "epoch: 12 trial 5784 training loss: 0.025892519392073154\n",
      "epoch: 12 trial 5785 training loss: 0.013026280561462045\n",
      "epoch: 12 trial 5786 training loss: 0.062486110255122185\n",
      "epoch: 12 trial 5787 training loss: 0.1711701825261116\n",
      "epoch: 12 trial 5788 training loss: 0.07745391502976418\n",
      "epoch: 12 trial 5789 training loss: 0.028360050171613693\n",
      "epoch: 12 trial 5790 training loss: 0.0861719362437725\n",
      "epoch: 12 trial 5791 training loss: 0.10458197817206383\n",
      "epoch: 12 trial 5792 training loss: 0.021357716526836157\n",
      "epoch: 12 trial 5793 training loss: 0.08217440359294415\n",
      "epoch: 12 trial 5794 training loss: 0.007252336188685149\n",
      "epoch: 12 trial 5795 training loss: 0.043998110108077526\n",
      "epoch: 12 trial 5796 training loss: 0.0131372413598001\n",
      "epoch: 12 trial 5797 training loss: 0.025962076615542173\n",
      "epoch: 12 trial 5798 training loss: 0.10920290276408195\n",
      "epoch: 12 trial 5799 training loss: 0.046252816915512085\n",
      "epoch: 12 trial 5800 training loss: 0.01667562802322209\n",
      "epoch: 12 trial 5801 training loss: 0.023303788155317307\n",
      "epoch: 12 trial 5802 training loss: 0.08115566894412041\n",
      "epoch: 12 trial 5803 training loss: 0.013854075688868761\n",
      "epoch: 12 trial 5804 training loss: 0.050439340993762016\n",
      "epoch: 12 trial 5805 training loss: 0.03425214905291796\n",
      "epoch: 12 trial 5806 training loss: 0.0389810623601079\n",
      "epoch: 12 trial 5807 training loss: 0.013836854603141546\n",
      "epoch: 12 trial 5808 training loss: 0.06018296629190445\n",
      "epoch: 13 trial 5809 training loss: 0.012180293444544077\n",
      "epoch: 13 trial 5810 training loss: 0.012618945445865393\n",
      "epoch: 13 trial 5811 training loss: 0.013154741376638412\n",
      "epoch: 13 trial 5812 training loss: 0.06280278973281384\n",
      "epoch: 13 trial 5813 training loss: 0.004131183843128383\n",
      "epoch: 13 trial 5814 training loss: 0.005564519204199314\n",
      "epoch: 13 trial 5815 training loss: 0.02774704247713089\n",
      "epoch: 13 trial 5816 training loss: 0.024668481666594744\n",
      "epoch: 13 trial 5817 training loss: 0.0068110974971205\n",
      "epoch: 13 trial 5818 training loss: 0.013519030530005693\n",
      "epoch: 13 trial 5819 training loss: 0.05892118811607361\n",
      "epoch: 13 trial 5820 training loss: 0.030126087367534637\n",
      "epoch: 13 trial 5821 training loss: 0.018544735852628946\n",
      "epoch: 13 trial 5822 training loss: 0.031520514748990536\n",
      "epoch: 13 trial 5823 training loss: 0.01729131769388914\n",
      "epoch: 13 trial 5824 training loss: 0.025692901574075222\n",
      "epoch: 13 trial 5825 training loss: 0.11384418979287148\n",
      "epoch: 13 trial 5826 training loss: 0.049297611229121685\n",
      "epoch: 13 trial 5827 training loss: 0.013611648697406054\n",
      "epoch: 13 trial 5828 training loss: 0.020290786866098642\n",
      "epoch: 13 trial 5829 training loss: 0.01720929192379117\n",
      "epoch: 13 trial 5830 training loss: 0.07925639301538467\n",
      "epoch: 13 trial 5831 training loss: 0.009020688477903605\n",
      "epoch: 13 trial 5832 training loss: 0.004979062243364751\n",
      "epoch: 13 trial 5833 training loss: 0.13536974228918552\n",
      "epoch: 13 trial 5834 training loss: 0.02520663570612669\n",
      "epoch: 13 trial 5835 training loss: 0.010373045457527041\n",
      "epoch: 13 trial 5836 training loss: 0.10795974731445312\n",
      "epoch: 13 trial 5837 training loss: 0.01983031863346696\n",
      "epoch: 13 trial 5838 training loss: 0.02609184943139553\n",
      "epoch: 13 trial 5839 training loss: 0.01680508814752102\n",
      "epoch: 13 trial 5840 training loss: 0.014098813757300377\n",
      "epoch: 13 trial 5841 training loss: 0.033443924970924854\n",
      "epoch: 13 trial 5842 training loss: 0.05151854455471039\n",
      "epoch: 13 trial 5843 training loss: 0.01820374373346567\n",
      "epoch: 13 trial 5844 training loss: 0.06319480948150158\n",
      "epoch: 13 trial 5845 training loss: 0.13841761834919453\n",
      "epoch: 13 trial 5846 training loss: 0.2974700480699539\n",
      "epoch: 13 trial 5847 training loss: 0.03585698176175356\n",
      "epoch: 13 trial 5848 training loss: 0.10055143758654594\n",
      "epoch: 13 trial 5849 training loss: 0.10720722936093807\n",
      "epoch: 13 trial 5850 training loss: 0.014339132932946086\n",
      "epoch: 13 trial 5851 training loss: 0.08958367072045803\n",
      "epoch: 13 trial 5852 training loss: 0.045911939814686775\n",
      "epoch: 13 trial 5853 training loss: 0.02611248893663287\n",
      "epoch: 13 trial 5854 training loss: 0.059414779767394066\n",
      "epoch: 13 trial 5855 training loss: 0.013441589660942554\n",
      "epoch: 13 trial 5856 training loss: 0.02224458707496524\n",
      "epoch: 13 trial 5857 training loss: 0.013372966088354588\n",
      "epoch: 13 trial 5858 training loss: 0.0966900996863842\n",
      "epoch: 13 trial 5859 training loss: 0.036996494978666306\n",
      "epoch: 13 trial 5860 training loss: 0.11720848083496094\n",
      "epoch: 13 trial 5861 training loss: 0.03875988442450762\n",
      "epoch: 13 trial 5862 training loss: 0.04276530258357525\n",
      "epoch: 13 trial 5863 training loss: 0.031113724689930677\n",
      "epoch: 13 trial 5864 training loss: 0.06255804933607578\n",
      "epoch: 13 trial 5865 training loss: 0.02614075504243374\n",
      "epoch: 13 trial 5866 training loss: 0.01866822922602296\n",
      "epoch: 13 trial 5867 training loss: 0.02751561440527439\n",
      "epoch: 13 trial 5868 training loss: 0.08768432028591633\n",
      "epoch: 13 trial 5869 training loss: 0.022391512524336576\n",
      "epoch: 13 trial 5870 training loss: 0.11177257634699345\n",
      "epoch: 13 trial 5871 training loss: 0.02235994953662157\n",
      "epoch: 13 trial 5872 training loss: 0.004256377229467034\n",
      "epoch: 13 trial 5873 training loss: 0.037143753841519356\n",
      "epoch: 13 trial 5874 training loss: 0.046202341094613075\n",
      "epoch: 13 trial 5875 training loss: 0.021188474725931883\n",
      "epoch: 13 trial 5876 training loss: 0.021121869329363108\n",
      "epoch: 13 trial 5877 training loss: 0.05248887650668621\n",
      "epoch: 13 trial 5878 training loss: 0.02864207560196519\n",
      "epoch: 13 trial 5879 training loss: 0.04492921940982342\n",
      "epoch: 13 trial 5880 training loss: 0.03361185174435377\n",
      "epoch: 13 trial 5881 training loss: 0.016964375972747803\n",
      "epoch: 13 trial 5882 training loss: 0.07168704085052013\n",
      "epoch: 13 trial 5883 training loss: 0.013188842684030533\n",
      "epoch: 13 trial 5884 training loss: 0.02462704200297594\n",
      "epoch: 13 trial 5885 training loss: 0.029279202688485384\n",
      "epoch: 13 trial 5886 training loss: 0.08019888587296009\n",
      "epoch: 13 trial 5887 training loss: 0.019203824922442436\n",
      "epoch: 13 trial 5888 training loss: 0.06193831376731396\n",
      "epoch: 13 trial 5889 training loss: 0.027274277061223984\n",
      "epoch: 13 trial 5890 training loss: 0.046888552606105804\n",
      "epoch: 13 trial 5891 training loss: 0.054417879320681095\n",
      "epoch: 13 trial 5892 training loss: 0.020242294296622276\n",
      "epoch: 13 trial 5893 training loss: 0.06530571915209293\n",
      "epoch: 13 trial 5894 training loss: 0.014494148315861821\n",
      "epoch: 13 trial 5895 training loss: 0.03215819876641035\n",
      "epoch: 13 trial 5896 training loss: 0.08002515695989132\n",
      "epoch: 13 trial 5897 training loss: 0.05344255082309246\n",
      "epoch: 13 trial 5898 training loss: 0.005660235183313489\n",
      "epoch: 13 trial 5899 training loss: 0.03965535759925842\n",
      "epoch: 13 trial 5900 training loss: 0.02469729632139206\n",
      "epoch: 13 trial 5901 training loss: 0.022366265300661325\n",
      "epoch: 13 trial 5902 training loss: 0.06924052722752094\n",
      "epoch: 13 trial 5903 training loss: 0.04356504883617163\n",
      "epoch: 13 trial 5904 training loss: 0.049766830168664455\n",
      "epoch: 13 trial 5905 training loss: 0.013538250932469964\n",
      "epoch: 13 trial 5906 training loss: 0.09298934787511826\n",
      "epoch: 13 trial 5907 training loss: 0.030853639356791973\n",
      "epoch: 13 trial 5908 training loss: 0.03715149452909827\n",
      "epoch: 13 trial 5909 training loss: 0.02535693533718586\n",
      "epoch: 13 trial 5910 training loss: 0.08897318504750729\n",
      "epoch: 13 trial 5911 training loss: 0.04336517956107855\n",
      "epoch: 13 trial 5912 training loss: 0.03058863803744316\n",
      "epoch: 13 trial 5913 training loss: 0.23201391100883484\n",
      "epoch: 13 trial 5914 training loss: 0.06270747445523739\n",
      "epoch: 13 trial 5915 training loss: 0.02874948550015688\n",
      "epoch: 13 trial 5916 training loss: 0.1632160134613514\n",
      "epoch: 13 trial 5917 training loss: 0.07841998338699341\n",
      "epoch: 13 trial 5918 training loss: 0.024980385787785053\n",
      "epoch: 13 trial 5919 training loss: 0.022849502973258495\n",
      "epoch: 13 trial 5920 training loss: 0.02314475132152438\n",
      "epoch: 13 trial 5921 training loss: 0.010998297249898314\n",
      "epoch: 13 trial 5922 training loss: 0.007864634040743113\n",
      "epoch: 13 trial 5923 training loss: 0.061350347474217415\n",
      "epoch: 13 trial 5924 training loss: 0.018598763272166252\n",
      "epoch: 13 trial 5925 training loss: 0.023465855047106743\n",
      "epoch: 13 trial 5926 training loss: 0.034570569172501564\n",
      "epoch: 13 trial 5927 training loss: 0.03233577497303486\n",
      "epoch: 13 trial 5928 training loss: 0.031944082118570805\n",
      "epoch: 13 trial 5929 training loss: 0.024688792880624533\n",
      "epoch: 13 trial 5930 training loss: 0.03657682612538338\n",
      "epoch: 13 trial 5931 training loss: 0.07640010491013527\n",
      "epoch: 13 trial 5932 training loss: 0.04179549589753151\n",
      "epoch: 13 trial 5933 training loss: 0.009537778329104185\n",
      "epoch: 13 trial 5934 training loss: 0.022350591141730547\n",
      "epoch: 13 trial 5935 training loss: 0.020918203052133322\n",
      "epoch: 13 trial 5936 training loss: 0.026725157164037228\n",
      "epoch: 13 trial 5937 training loss: 0.14717593044042587\n",
      "epoch: 13 trial 5938 training loss: 0.029885663650929928\n",
      "epoch: 13 trial 5939 training loss: 0.052600967697799206\n",
      "epoch: 13 trial 5940 training loss: 0.02835478074848652\n",
      "epoch: 13 trial 5941 training loss: 0.03216276876628399\n",
      "epoch: 13 trial 5942 training loss: 0.04273822344839573\n",
      "epoch: 13 trial 5943 training loss: 0.00999671476893127\n",
      "epoch: 13 trial 5944 training loss: 0.04248282313346863\n",
      "epoch: 13 trial 5945 training loss: 0.016700513660907745\n",
      "epoch: 13 trial 5946 training loss: 0.10961142182350159\n",
      "epoch: 13 trial 5947 training loss: 0.016823014710098505\n",
      "epoch: 13 trial 5948 training loss: 0.055817095562815666\n",
      "epoch: 13 trial 5949 training loss: 0.03719600383192301\n",
      "epoch: 13 trial 5950 training loss: 0.02876390190795064\n",
      "epoch: 13 trial 5951 training loss: 0.025478086899966\n",
      "epoch: 13 trial 5952 training loss: 0.10195942595601082\n",
      "epoch: 13 trial 5953 training loss: 0.011439516674727201\n",
      "epoch: 13 trial 5954 training loss: 0.017879214137792587\n",
      "epoch: 13 trial 5955 training loss: 0.1074846163392067\n",
      "epoch: 13 trial 5956 training loss: 0.015384274302050471\n",
      "epoch: 13 trial 5957 training loss: 0.03492654487490654\n",
      "epoch: 13 trial 5958 training loss: 0.055773308500647545\n",
      "epoch: 13 trial 5959 training loss: 0.03158345818519592\n",
      "epoch: 13 trial 5960 training loss: 0.02290894603356719\n",
      "epoch: 13 trial 5961 training loss: 0.01601294055581093\n",
      "epoch: 13 trial 5962 training loss: 0.03539185971021652\n",
      "epoch: 13 trial 5963 training loss: 0.01437264820560813\n",
      "epoch: 13 trial 5964 training loss: 0.01063866657204926\n",
      "epoch: 13 trial 5965 training loss: 0.029191745445132256\n",
      "epoch: 13 trial 5966 training loss: 0.0041715934639796615\n",
      "epoch: 13 trial 5967 training loss: 0.012250695377588272\n",
      "epoch: 13 trial 5968 training loss: 0.019201959017664194\n",
      "epoch: 13 trial 5969 training loss: 0.0103355897590518\n",
      "epoch: 13 trial 5970 training loss: 0.05686543509364128\n",
      "epoch: 13 trial 5971 training loss: 0.004231549915857613\n",
      "epoch: 13 trial 5972 training loss: 0.0544186532497406\n",
      "epoch: 13 trial 5973 training loss: 0.00918367994017899\n",
      "epoch: 13 trial 5974 training loss: 0.022369815967977047\n",
      "epoch: 13 trial 5975 training loss: 0.027541719377040863\n",
      "epoch: 13 trial 5976 training loss: 0.010560753522440791\n",
      "epoch: 13 trial 5977 training loss: 0.03886834904551506\n",
      "epoch: 13 trial 5978 training loss: 0.12148210033774376\n",
      "epoch: 13 trial 5979 training loss: 0.047051033936440945\n",
      "epoch: 13 trial 5980 training loss: 0.008193704532459378\n",
      "epoch: 13 trial 5981 training loss: 0.06069624423980713\n",
      "epoch: 13 trial 5982 training loss: 0.03234663885086775\n",
      "epoch: 13 trial 5983 training loss: 0.02978493459522724\n",
      "epoch: 13 trial 5984 training loss: 0.020602446049451828\n",
      "epoch: 13 trial 5985 training loss: 0.023012666031718254\n",
      "epoch: 13 trial 5986 training loss: 0.0386377302929759\n",
      "epoch: 13 trial 5987 training loss: 0.04087465163320303\n",
      "epoch: 13 trial 5988 training loss: 0.04014016408473253\n",
      "epoch: 13 trial 5989 training loss: 0.07108892686665058\n",
      "epoch: 13 trial 5990 training loss: 0.012410680297762156\n",
      "epoch: 13 trial 5991 training loss: 0.044734722934663296\n",
      "epoch: 13 trial 5992 training loss: 0.02000140445306897\n",
      "epoch: 13 trial 5993 training loss: 0.018893073312938213\n",
      "epoch: 13 trial 5994 training loss: 0.04573345184326172\n",
      "epoch: 13 trial 5995 training loss: 0.05523620918393135\n",
      "epoch: 13 trial 5996 training loss: 0.02142265485599637\n",
      "epoch: 13 trial 5997 training loss: 0.04333976097404957\n",
      "epoch: 13 trial 5998 training loss: 0.054989357478916645\n",
      "epoch: 13 trial 5999 training loss: 0.061416203156113625\n",
      "epoch: 13 trial 6000 training loss: 0.04001626279205084\n",
      "epoch: 13 trial 6001 training loss: 0.02868572436273098\n",
      "epoch: 13 trial 6002 training loss: 0.01566002005711198\n",
      "epoch: 13 trial 6003 training loss: 0.03050070721656084\n",
      "epoch: 13 trial 6004 training loss: 0.027117091231048107\n",
      "epoch: 13 trial 6005 training loss: 0.026075081899762154\n",
      "epoch: 13 trial 6006 training loss: 0.0648507121950388\n",
      "epoch: 13 trial 6007 training loss: 0.09692146070301533\n",
      "epoch: 13 trial 6008 training loss: 0.011928639141842723\n",
      "epoch: 13 trial 6009 training loss: 0.00606870511546731\n",
      "epoch: 13 trial 6010 training loss: 0.00917993183247745\n",
      "epoch: 13 trial 6011 training loss: 0.01028021750971675\n",
      "epoch: 13 trial 6012 training loss: 0.015381766017526388\n",
      "epoch: 13 trial 6013 training loss: 0.016281897434964776\n",
      "epoch: 13 trial 6014 training loss: 0.06822817958891392\n",
      "epoch: 13 trial 6015 training loss: 0.03729099314659834\n",
      "epoch: 13 trial 6016 training loss: 0.016408764757215977\n",
      "epoch: 13 trial 6017 training loss: 0.09838938526809216\n",
      "epoch: 13 trial 6018 training loss: 0.04596647899597883\n",
      "epoch: 13 trial 6019 training loss: 0.05981673486530781\n",
      "epoch: 13 trial 6020 training loss: 0.062223649583756924\n",
      "epoch: 13 trial 6021 training loss: 0.1129174754023552\n",
      "epoch: 13 trial 6022 training loss: 0.014006251934915781\n",
      "epoch: 13 trial 6023 training loss: 0.0077089667320251465\n",
      "epoch: 13 trial 6024 training loss: 0.02728569507598877\n",
      "epoch: 13 trial 6025 training loss: 0.021924948785454035\n",
      "epoch: 13 trial 6026 training loss: 0.02014717599377036\n",
      "epoch: 13 trial 6027 training loss: 0.04835215676575899\n",
      "epoch: 13 trial 6028 training loss: 0.05935620702803135\n",
      "epoch: 13 trial 6029 training loss: 0.03310031723231077\n",
      "epoch: 13 trial 6030 training loss: 0.05764741264283657\n",
      "epoch: 13 trial 6031 training loss: 0.04295986611396074\n",
      "epoch: 13 trial 6032 training loss: 0.08217207714915276\n",
      "epoch: 13 trial 6033 training loss: 0.010702645406126976\n",
      "epoch: 13 trial 6034 training loss: 0.010193068534135818\n",
      "epoch: 13 trial 6035 training loss: 0.025500664487481117\n",
      "epoch: 13 trial 6036 training loss: 0.027459936682134867\n",
      "epoch: 13 trial 6037 training loss: 0.04637589864432812\n",
      "epoch: 13 trial 6038 training loss: 0.014203866012394428\n",
      "epoch: 13 trial 6039 training loss: 0.1546906866133213\n",
      "epoch: 13 trial 6040 training loss: 0.03324116114526987\n",
      "epoch: 13 trial 6041 training loss: 0.11259212717413902\n",
      "epoch: 13 trial 6042 training loss: 0.09123913198709488\n",
      "epoch: 13 trial 6043 training loss: 0.01576280710287392\n",
      "epoch: 13 trial 6044 training loss: 0.055745311081409454\n",
      "epoch: 13 trial 6045 training loss: 0.018873273860663176\n",
      "epoch: 13 trial 6046 training loss: 0.04629821330308914\n",
      "epoch: 13 trial 6047 training loss: 0.04445644002407789\n",
      "epoch: 13 trial 6048 training loss: 0.09117285907268524\n",
      "epoch: 13 trial 6049 training loss: 0.019230217207223177\n",
      "epoch: 13 trial 6050 training loss: 0.03867470286786556\n",
      "epoch: 13 trial 6051 training loss: 0.038542614318430424\n",
      "epoch: 13 trial 6052 training loss: 0.020384599920362234\n",
      "epoch: 13 trial 6053 training loss: 0.05690927617251873\n",
      "epoch: 13 trial 6054 training loss: 0.01841280795633793\n",
      "epoch: 13 trial 6055 training loss: 0.052406626753509045\n",
      "epoch: 13 trial 6056 training loss: 0.02449106704443693\n",
      "epoch: 13 trial 6057 training loss: 0.02734902873635292\n",
      "epoch: 13 trial 6058 training loss: 0.046764759346842766\n",
      "epoch: 13 trial 6059 training loss: 0.012050557881593704\n",
      "epoch: 13 trial 6060 training loss: 0.004945744993165135\n",
      "epoch: 13 trial 6061 training loss: 0.029776600189507008\n",
      "epoch: 13 trial 6062 training loss: 0.09626742266118526\n",
      "epoch: 13 trial 6063 training loss: 0.019394030328840017\n",
      "epoch: 13 trial 6064 training loss: 0.07925347983837128\n",
      "epoch: 13 trial 6065 training loss: 0.03912346065044403\n",
      "epoch: 13 trial 6066 training loss: 0.023193298373371363\n",
      "epoch: 13 trial 6067 training loss: 0.16806527972221375\n",
      "epoch: 13 trial 6068 training loss: 0.014153331518173218\n",
      "epoch: 13 trial 6069 training loss: 0.01377503527328372\n",
      "epoch: 13 trial 6070 training loss: 0.010592172155156732\n",
      "epoch: 13 trial 6071 training loss: 0.0035164090804755688\n",
      "epoch: 13 trial 6072 training loss: 0.024236833676695824\n",
      "epoch: 13 trial 6073 training loss: 0.04002976790070534\n",
      "epoch: 13 trial 6074 training loss: 0.04685265105217695\n",
      "epoch: 13 trial 6075 training loss: 0.036389351822435856\n",
      "epoch: 13 trial 6076 training loss: 0.035751708783209324\n",
      "epoch: 13 trial 6077 training loss: 0.02485099295154214\n",
      "epoch: 13 trial 6078 training loss: 0.07115862891077995\n",
      "epoch: 13 trial 6079 training loss: 0.016121267806738615\n",
      "epoch: 13 trial 6080 training loss: 0.007506638765335083\n",
      "epoch: 13 trial 6081 training loss: 0.022493762895464897\n",
      "epoch: 13 trial 6082 training loss: 0.007014504401013255\n",
      "epoch: 13 trial 6083 training loss: 0.06875168532133102\n",
      "epoch: 13 trial 6084 training loss: 0.004214853863231838\n",
      "epoch: 13 trial 6085 training loss: 0.026581100653856993\n",
      "epoch: 13 trial 6086 training loss: 0.014604669064283371\n",
      "epoch: 13 trial 6087 training loss: 0.0626277681440115\n",
      "epoch: 13 trial 6088 training loss: 0.03158221300691366\n",
      "epoch: 13 trial 6089 training loss: 0.025950273498892784\n",
      "epoch: 13 trial 6090 training loss: 0.04045129753649235\n",
      "epoch: 13 trial 6091 training loss: 0.06590688787400723\n",
      "epoch: 13 trial 6092 training loss: 0.030120898969471455\n",
      "epoch: 13 trial 6093 training loss: 0.06920196861028671\n",
      "epoch: 13 trial 6094 training loss: 0.01827555149793625\n",
      "epoch: 13 trial 6095 training loss: 0.07460727542638779\n",
      "epoch: 13 trial 6096 training loss: 0.021233139093965292\n",
      "epoch: 13 trial 6097 training loss: 0.00947275897487998\n",
      "epoch: 13 trial 6098 training loss: 0.04642939567565918\n",
      "epoch: 13 trial 6099 training loss: 0.02891202550381422\n",
      "epoch: 13 trial 6100 training loss: 0.01625299733132124\n",
      "epoch: 13 trial 6101 training loss: 0.017586423084139824\n",
      "epoch: 13 trial 6102 training loss: 0.009362657321617007\n",
      "epoch: 13 trial 6103 training loss: 0.02200824348255992\n",
      "epoch: 13 trial 6104 training loss: 0.031022904440760612\n",
      "epoch: 13 trial 6105 training loss: 0.017876210156828165\n",
      "epoch: 13 trial 6106 training loss: 0.034298285841941833\n",
      "epoch: 13 trial 6107 training loss: 0.08490430563688278\n",
      "epoch: 13 trial 6108 training loss: 0.03827499412000179\n",
      "epoch: 13 trial 6109 training loss: 0.015564130153506994\n",
      "epoch: 13 trial 6110 training loss: 0.005228574620559812\n",
      "epoch: 13 trial 6111 training loss: 0.007969742873683572\n",
      "epoch: 13 trial 6112 training loss: 0.030345586128532887\n",
      "epoch: 13 trial 6113 training loss: 0.05765858292579651\n",
      "epoch: 13 trial 6114 training loss: 0.04503634199500084\n",
      "epoch: 13 trial 6115 training loss: 0.002757957379799336\n",
      "epoch: 13 trial 6116 training loss: 0.035539669916033745\n",
      "epoch: 13 trial 6117 training loss: 0.03966464102268219\n",
      "epoch: 13 trial 6118 training loss: 0.02303797658532858\n",
      "epoch: 13 trial 6119 training loss: 0.01697133155539632\n",
      "epoch: 13 trial 6120 training loss: 0.020354284904897213\n",
      "epoch: 13 trial 6121 training loss: 0.04045368544757366\n",
      "epoch: 13 trial 6122 training loss: 0.02039420884102583\n",
      "epoch: 13 trial 6123 training loss: 0.038546785712242126\n",
      "epoch: 13 trial 6124 training loss: 0.07178780809044838\n",
      "epoch: 13 trial 6125 training loss: 0.03236270695924759\n",
      "epoch: 13 trial 6126 training loss: 0.0319936303421855\n",
      "epoch: 13 trial 6127 training loss: 0.06824556924402714\n",
      "epoch: 13 trial 6128 training loss: 0.03503917157649994\n",
      "epoch: 13 trial 6129 training loss: 0.06842105649411678\n",
      "epoch: 13 trial 6130 training loss: 0.018087601754814386\n",
      "epoch: 13 trial 6131 training loss: 0.031248158309608698\n",
      "epoch: 13 trial 6132 training loss: 0.025426046922802925\n",
      "epoch: 13 trial 6133 training loss: 0.021706885658204556\n",
      "epoch: 13 trial 6134 training loss: 0.025399545207619667\n",
      "epoch: 13 trial 6135 training loss: 0.08933301456272602\n",
      "epoch: 13 trial 6136 training loss: 0.012462574522942305\n",
      "epoch: 13 trial 6137 training loss: 0.09128282591700554\n",
      "epoch: 13 trial 6138 training loss: 0.024154591374099255\n",
      "epoch: 13 trial 6139 training loss: 0.006194844376295805\n",
      "epoch: 13 trial 6140 training loss: 0.0500476136803627\n",
      "epoch: 13 trial 6141 training loss: 0.058572039008140564\n",
      "epoch: 13 trial 6142 training loss: 0.009578165831044316\n",
      "epoch: 13 trial 6143 training loss: 0.03392537962645292\n",
      "epoch: 13 trial 6144 training loss: 0.00792060443200171\n",
      "epoch: 13 trial 6145 training loss: 0.03433011192828417\n",
      "epoch: 13 trial 6146 training loss: 0.019524957984685898\n",
      "epoch: 13 trial 6147 training loss: 0.0362694226205349\n",
      "epoch: 13 trial 6148 training loss: 0.2567553445696831\n",
      "epoch: 13 trial 6149 training loss: 0.05240596830844879\n",
      "epoch: 13 trial 6150 training loss: 0.008711950737051666\n",
      "epoch: 13 trial 6151 training loss: 0.01811129879206419\n",
      "epoch: 13 trial 6152 training loss: 0.14102277532219887\n",
      "epoch: 13 trial 6153 training loss: 0.04930527415126562\n",
      "epoch: 13 trial 6154 training loss: 0.07243857905268669\n",
      "epoch: 13 trial 6155 training loss: 0.03799689933657646\n",
      "epoch: 13 trial 6156 training loss: 0.03366876859217882\n",
      "epoch: 13 trial 6157 training loss: 0.01580451289191842\n",
      "epoch: 13 trial 6158 training loss: 0.07616844587028027\n",
      "epoch: 13 trial 6159 training loss: 0.02200614893808961\n",
      "epoch: 13 trial 6160 training loss: 0.08057200163602829\n",
      "epoch: 13 trial 6161 training loss: 0.014522182988002896\n",
      "epoch: 13 trial 6162 training loss: 0.006556180422194302\n",
      "epoch: 13 trial 6163 training loss: 0.034110596403479576\n",
      "epoch: 13 trial 6164 training loss: 0.005633025662973523\n",
      "epoch: 13 trial 6165 training loss: 0.014163111336529255\n",
      "epoch: 13 trial 6166 training loss: 0.08413521200418472\n",
      "epoch: 13 trial 6167 training loss: 0.07675443962216377\n",
      "epoch: 13 trial 6168 training loss: 0.024902086704969406\n",
      "epoch: 13 trial 6169 training loss: 0.10495379939675331\n",
      "epoch: 13 trial 6170 training loss: 0.10406123846769333\n",
      "epoch: 13 trial 6171 training loss: 0.02837191056460142\n",
      "epoch: 13 trial 6172 training loss: 0.062345342710614204\n",
      "epoch: 13 trial 6173 training loss: 0.04133415408432484\n",
      "epoch: 13 trial 6174 training loss: 0.00939751765690744\n",
      "epoch: 13 trial 6175 training loss: 0.007002950529567897\n",
      "epoch: 13 trial 6176 training loss: 0.13648928701877594\n",
      "epoch: 13 trial 6177 training loss: 0.06084459275007248\n",
      "epoch: 13 trial 6178 training loss: 0.040223428048193455\n",
      "epoch: 13 trial 6179 training loss: 0.05642484128475189\n",
      "epoch: 13 trial 6180 training loss: 0.02457529492676258\n",
      "epoch: 13 trial 6181 training loss: 0.03699292987585068\n",
      "epoch: 13 trial 6182 training loss: 0.019091309513896704\n",
      "epoch: 13 trial 6183 training loss: 0.048735772259533405\n",
      "epoch: 13 trial 6184 training loss: 0.031104336958378553\n",
      "epoch: 13 trial 6185 training loss: 0.04096347279846668\n",
      "epoch: 13 trial 6186 training loss: 0.08500587567687035\n",
      "epoch: 13 trial 6187 training loss: 0.029006462544202805\n",
      "epoch: 13 trial 6188 training loss: 0.010682953521609306\n",
      "epoch: 13 trial 6189 training loss: 0.02490169322118163\n",
      "epoch: 13 trial 6190 training loss: 0.0038713684771209955\n",
      "epoch: 13 trial 6191 training loss: 0.03995591960847378\n",
      "epoch: 13 trial 6192 training loss: 0.0395083948969841\n",
      "epoch: 13 trial 6193 training loss: 0.019177064765244722\n",
      "epoch: 13 trial 6194 training loss: 0.003464486508164555\n",
      "epoch: 13 trial 6195 training loss: 0.008530808612704277\n",
      "epoch: 13 trial 6196 training loss: 0.01925252703949809\n",
      "epoch: 13 trial 6197 training loss: 0.037858348339796066\n",
      "epoch: 13 trial 6198 training loss: 0.0053330870578065515\n",
      "epoch: 13 trial 6199 training loss: 0.004059479222632945\n",
      "epoch: 13 trial 6200 training loss: 0.006066073197871447\n",
      "epoch: 13 trial 6201 training loss: 0.009794940939173102\n",
      "epoch: 13 trial 6202 training loss: 0.011800752021372318\n",
      "epoch: 13 trial 6203 training loss: 0.023593976628035307\n",
      "epoch: 13 trial 6204 training loss: 0.10811355710029602\n",
      "epoch: 13 trial 6205 training loss: 0.019924670457839966\n",
      "epoch: 13 trial 6206 training loss: 0.023119001649320126\n",
      "epoch: 13 trial 6207 training loss: 0.028579524718225002\n",
      "epoch: 13 trial 6208 training loss: 0.042690617963671684\n",
      "epoch: 13 trial 6209 training loss: 0.03235336486250162\n",
      "epoch: 13 trial 6210 training loss: 0.036921543069183826\n",
      "epoch: 13 trial 6211 training loss: 0.037625749595463276\n",
      "epoch: 13 trial 6212 training loss: 0.10313957557082176\n",
      "epoch: 13 trial 6213 training loss: 0.12545291148126125\n",
      "epoch: 13 trial 6214 training loss: 0.02227290626615286\n",
      "epoch: 13 trial 6215 training loss: 0.023447895888239145\n",
      "epoch: 13 trial 6216 training loss: 0.0829309169203043\n",
      "epoch: 13 trial 6217 training loss: 0.015216742642223835\n",
      "epoch: 13 trial 6218 training loss: 0.06303063407540321\n",
      "epoch: 13 trial 6219 training loss: 0.09653005562722683\n",
      "epoch: 13 trial 6220 training loss: 0.10509823635220528\n",
      "epoch: 13 trial 6221 training loss: 0.040663295425474644\n",
      "epoch: 13 trial 6222 training loss: 0.10972302779555321\n",
      "epoch: 13 trial 6223 training loss: 0.0191913777962327\n",
      "epoch: 13 trial 6224 training loss: 0.05536199174821377\n",
      "epoch: 13 trial 6225 training loss: 0.03630891069769859\n",
      "epoch: 13 trial 6226 training loss: 0.10179711505770683\n",
      "epoch: 13 trial 6227 training loss: 0.007152102421969175\n",
      "epoch: 13 trial 6228 training loss: 0.008717404445633292\n",
      "epoch: 13 trial 6229 training loss: 0.01932255970314145\n",
      "epoch: 13 trial 6230 training loss: 0.030644015409052372\n",
      "epoch: 13 trial 6231 training loss: 0.053073227405548096\n",
      "epoch: 13 trial 6232 training loss: 0.05214781127870083\n",
      "epoch: 13 trial 6233 training loss: 0.015701439464464784\n",
      "epoch: 13 trial 6234 training loss: 0.05482500605285168\n",
      "epoch: 13 trial 6235 training loss: 0.017739522736519575\n",
      "epoch: 13 trial 6236 training loss: 0.016309045255184174\n",
      "epoch: 13 trial 6237 training loss: 0.020105100236833096\n",
      "epoch: 13 trial 6238 training loss: 0.012061010580509901\n",
      "epoch: 13 trial 6239 training loss: 0.019554081838577986\n",
      "epoch: 13 trial 6240 training loss: 0.04108641296625137\n",
      "epoch: 13 trial 6241 training loss: 0.027501086704432964\n",
      "epoch: 13 trial 6242 training loss: 0.024828916415572166\n",
      "epoch: 13 trial 6243 training loss: 0.022564096376299858\n",
      "epoch: 13 trial 6244 training loss: 0.017598350066691637\n",
      "epoch: 13 trial 6245 training loss: 0.00395273941103369\n",
      "epoch: 13 trial 6246 training loss: 0.011821652529761195\n",
      "epoch: 13 trial 6247 training loss: 0.015686339233070612\n",
      "epoch: 13 trial 6248 training loss: 0.013253109995275736\n",
      "epoch: 13 trial 6249 training loss: 0.005673276609741151\n",
      "epoch: 13 trial 6250 training loss: 0.01306054275482893\n",
      "epoch: 13 trial 6251 training loss: 0.08831322751939297\n",
      "epoch: 13 trial 6252 training loss: 0.011131027480587363\n",
      "epoch: 13 trial 6253 training loss: 0.034060072153806686\n",
      "epoch: 13 trial 6254 training loss: 0.01362322410568595\n",
      "epoch: 13 trial 6255 training loss: 0.04735478479415178\n",
      "epoch: 13 trial 6256 training loss: 0.03963026311248541\n",
      "epoch: 13 trial 6257 training loss: 0.03908269386738539\n",
      "epoch: 13 trial 6258 training loss: 0.025902103167027235\n",
      "epoch: 13 trial 6259 training loss: 0.017993026413023472\n",
      "epoch: 13 trial 6260 training loss: 0.10706873424351215\n",
      "epoch: 13 trial 6261 training loss: 0.005036833113990724\n",
      "epoch: 13 trial 6262 training loss: 0.01862141629680991\n",
      "epoch: 13 trial 6263 training loss: 0.11815579608082771\n",
      "epoch: 13 trial 6264 training loss: 0.027209912426769733\n",
      "epoch: 13 trial 6265 training loss: 0.022030092775821686\n",
      "epoch: 13 trial 6266 training loss: 0.0216721729375422\n",
      "epoch: 13 trial 6267 training loss: 0.013147456105798483\n",
      "epoch: 13 trial 6268 training loss: 0.024375169537961483\n",
      "epoch: 13 trial 6269 training loss: 0.013854953926056623\n",
      "epoch: 13 trial 6270 training loss: 0.06205521523952484\n",
      "epoch: 13 trial 6271 training loss: 0.16177726164460182\n",
      "epoch: 13 trial 6272 training loss: 0.07716377265751362\n",
      "epoch: 13 trial 6273 training loss: 0.028046700172126293\n",
      "epoch: 13 trial 6274 training loss: 0.07353012636303902\n",
      "epoch: 13 trial 6275 training loss: 0.10730882734060287\n",
      "epoch: 13 trial 6276 training loss: 0.024272187612950802\n",
      "epoch: 13 trial 6277 training loss: 0.07562250085175037\n",
      "epoch: 13 trial 6278 training loss: 0.00780515605583787\n",
      "epoch: 13 trial 6279 training loss: 0.04656194243580103\n",
      "epoch: 13 trial 6280 training loss: 0.013111887499690056\n",
      "epoch: 13 trial 6281 training loss: 0.027633083052933216\n",
      "epoch: 13 trial 6282 training loss: 0.10788439214229584\n",
      "epoch: 13 trial 6283 training loss: 0.043494390323758125\n",
      "epoch: 13 trial 6284 training loss: 0.019027894362807274\n",
      "epoch: 13 trial 6285 training loss: 0.020707246381789446\n",
      "epoch: 13 trial 6286 training loss: 0.08003212697803974\n",
      "epoch: 13 trial 6287 training loss: 0.01600132090970874\n",
      "epoch: 13 trial 6288 training loss: 0.05048130080103874\n",
      "epoch: 13 trial 6289 training loss: 0.035926599986851215\n",
      "epoch: 13 trial 6290 training loss: 0.041447230614721775\n",
      "epoch: 13 trial 6291 training loss: 0.013064454542472959\n",
      "epoch: 13 trial 6292 training loss: 0.05764300189912319\n",
      "epoch: 14 trial 6293 training loss: 0.012166505679488182\n",
      "epoch: 14 trial 6294 training loss: 0.013626817613840103\n",
      "epoch: 14 trial 6295 training loss: 0.012709148228168488\n",
      "epoch: 14 trial 6296 training loss: 0.0646403469145298\n",
      "epoch: 14 trial 6297 training loss: 0.004496465669944882\n",
      "epoch: 14 trial 6298 training loss: 0.0055791434133425355\n",
      "epoch: 14 trial 6299 training loss: 0.026643451303243637\n",
      "epoch: 14 trial 6300 training loss: 0.026892979629337788\n",
      "epoch: 14 trial 6301 training loss: 0.007010159315541387\n",
      "epoch: 14 trial 6302 training loss: 0.01302721630781889\n",
      "epoch: 14 trial 6303 training loss: 0.05705139972269535\n",
      "epoch: 14 trial 6304 training loss: 0.029911663383245468\n",
      "epoch: 14 trial 6305 training loss: 0.016449544113129377\n",
      "epoch: 14 trial 6306 training loss: 0.026170443277806044\n",
      "epoch: 14 trial 6307 training loss: 0.02120225364342332\n",
      "epoch: 14 trial 6308 training loss: 0.02360947383567691\n",
      "epoch: 14 trial 6309 training loss: 0.11802399158477783\n",
      "epoch: 14 trial 6310 training loss: 0.048633414320647717\n",
      "epoch: 14 trial 6311 training loss: 0.016115125734359026\n",
      "epoch: 14 trial 6312 training loss: 0.02044338872656226\n",
      "epoch: 14 trial 6313 training loss: 0.016289763152599335\n",
      "epoch: 14 trial 6314 training loss: 0.07689413987100124\n",
      "epoch: 14 trial 6315 training loss: 0.010033292230218649\n",
      "epoch: 14 trial 6316 training loss: 0.005258124903775752\n",
      "epoch: 14 trial 6317 training loss: 0.14355537667870522\n",
      "epoch: 14 trial 6318 training loss: 0.022831209935247898\n",
      "epoch: 14 trial 6319 training loss: 0.009336117189377546\n",
      "epoch: 14 trial 6320 training loss: 0.11013982817530632\n",
      "epoch: 14 trial 6321 training loss: 0.022156931925565004\n",
      "epoch: 14 trial 6322 training loss: 0.022889786399900913\n",
      "epoch: 14 trial 6323 training loss: 0.016076928470283747\n",
      "epoch: 14 trial 6324 training loss: 0.013667695689946413\n",
      "epoch: 14 trial 6325 training loss: 0.03403208218514919\n",
      "epoch: 14 trial 6326 training loss: 0.05226257536560297\n",
      "epoch: 14 trial 6327 training loss: 0.01826184755191207\n",
      "epoch: 14 trial 6328 training loss: 0.06331734359264374\n",
      "epoch: 14 trial 6329 training loss: 0.14617770537734032\n",
      "epoch: 14 trial 6330 training loss: 0.3030751124024391\n",
      "epoch: 14 trial 6331 training loss: 0.04234771244227886\n",
      "epoch: 14 trial 6332 training loss: 0.09864035621285439\n",
      "epoch: 14 trial 6333 training loss: 0.10097570344805717\n",
      "epoch: 14 trial 6334 training loss: 0.012326992116868496\n",
      "epoch: 14 trial 6335 training loss: 0.0770492423325777\n",
      "epoch: 14 trial 6336 training loss: 0.04239441081881523\n",
      "epoch: 14 trial 6337 training loss: 0.029770384542644024\n",
      "epoch: 14 trial 6338 training loss: 0.055722808465361595\n",
      "epoch: 14 trial 6339 training loss: 0.01225115917623043\n",
      "epoch: 14 trial 6340 training loss: 0.020925148390233517\n",
      "epoch: 14 trial 6341 training loss: 0.014122845605015755\n",
      "epoch: 14 trial 6342 training loss: 0.0906057208776474\n",
      "epoch: 14 trial 6343 training loss: 0.03760464582592249\n",
      "epoch: 14 trial 6344 training loss: 0.11953277140855789\n",
      "epoch: 14 trial 6345 training loss: 0.03745219297707081\n",
      "epoch: 14 trial 6346 training loss: 0.0401134230196476\n",
      "epoch: 14 trial 6347 training loss: 0.03406610433012247\n",
      "epoch: 14 trial 6348 training loss: 0.05824468471109867\n",
      "epoch: 14 trial 6349 training loss: 0.025772073306143284\n",
      "epoch: 14 trial 6350 training loss: 0.01636077743023634\n",
      "epoch: 14 trial 6351 training loss: 0.014153270982205868\n",
      "epoch: 14 trial 6352 training loss: 0.07885190472006798\n",
      "epoch: 14 trial 6353 training loss: 0.026747009716928005\n",
      "epoch: 14 trial 6354 training loss: 0.10584300011396408\n",
      "epoch: 14 trial 6355 training loss: 0.022278265561908484\n",
      "epoch: 14 trial 6356 training loss: 0.0026653058594092727\n",
      "epoch: 14 trial 6357 training loss: 0.05784936062991619\n",
      "epoch: 14 trial 6358 training loss: 0.07181665394455194\n",
      "epoch: 14 trial 6359 training loss: 0.031589608639478683\n",
      "epoch: 14 trial 6360 training loss: 0.018756486475467682\n",
      "epoch: 14 trial 6361 training loss: 0.054033199325203896\n",
      "epoch: 14 trial 6362 training loss: 0.06701911985874176\n",
      "epoch: 14 trial 6363 training loss: 0.04626838956028223\n",
      "epoch: 14 trial 6364 training loss: 0.045455182902514935\n",
      "epoch: 14 trial 6365 training loss: 0.012544350698590279\n",
      "epoch: 14 trial 6366 training loss: 0.07009960524737835\n",
      "epoch: 14 trial 6367 training loss: 0.014976995531469584\n",
      "epoch: 14 trial 6368 training loss: 0.025218768045306206\n",
      "epoch: 14 trial 6369 training loss: 0.0326046384871006\n",
      "epoch: 14 trial 6370 training loss: 0.07709508389234543\n",
      "epoch: 14 trial 6371 training loss: 0.019252363592386246\n",
      "epoch: 14 trial 6372 training loss: 0.06877108477056026\n",
      "epoch: 14 trial 6373 training loss: 0.05345589201897383\n",
      "epoch: 14 trial 6374 training loss: 0.04558224603533745\n",
      "epoch: 14 trial 6375 training loss: 0.041751627810299397\n",
      "epoch: 14 trial 6376 training loss: 0.025647115893661976\n",
      "epoch: 14 trial 6377 training loss: 0.07569424994289875\n",
      "epoch: 14 trial 6378 training loss: 0.014788671862334013\n",
      "epoch: 14 trial 6379 training loss: 0.03023960255086422\n",
      "epoch: 14 trial 6380 training loss: 0.11106526851654053\n",
      "epoch: 14 trial 6381 training loss: 0.05955536477267742\n",
      "epoch: 14 trial 6382 training loss: 0.00908692553639412\n",
      "epoch: 14 trial 6383 training loss: 0.0456664003431797\n",
      "epoch: 14 trial 6384 training loss: 0.02476906729862094\n",
      "epoch: 14 trial 6385 training loss: 0.017794211395084858\n",
      "epoch: 14 trial 6386 training loss: 0.07834954001009464\n",
      "epoch: 14 trial 6387 training loss: 0.04665018152445555\n",
      "epoch: 14 trial 6388 training loss: 0.05395072139799595\n",
      "epoch: 14 trial 6389 training loss: 0.021458065137267113\n",
      "epoch: 14 trial 6390 training loss: 0.08468018844723701\n",
      "epoch: 14 trial 6391 training loss: 0.03256176132708788\n",
      "epoch: 14 trial 6392 training loss: 0.033126107417047024\n",
      "epoch: 14 trial 6393 training loss: 0.02239387622103095\n",
      "epoch: 14 trial 6394 training loss: 0.10498172231018543\n",
      "epoch: 14 trial 6395 training loss: 0.05112671758979559\n",
      "epoch: 14 trial 6396 training loss: 0.030768390744924545\n",
      "epoch: 14 trial 6397 training loss: 0.242549367249012\n",
      "epoch: 14 trial 6398 training loss: 0.055788762867450714\n",
      "epoch: 14 trial 6399 training loss: 0.02681812085211277\n",
      "epoch: 14 trial 6400 training loss: 0.1733211874961853\n",
      "epoch: 14 trial 6401 training loss: 0.08137165382504463\n",
      "epoch: 14 trial 6402 training loss: 0.024995462503284216\n",
      "epoch: 14 trial 6403 training loss: 0.021387902088463306\n",
      "epoch: 14 trial 6404 training loss: 0.025434705428779125\n",
      "epoch: 14 trial 6405 training loss: 0.012412698939442635\n",
      "epoch: 14 trial 6406 training loss: 0.00824341969564557\n",
      "epoch: 14 trial 6407 training loss: 0.06125699542462826\n",
      "epoch: 14 trial 6408 training loss: 0.017517149448394775\n",
      "epoch: 14 trial 6409 training loss: 0.02452979516237974\n",
      "epoch: 14 trial 6410 training loss: 0.035141363739967346\n",
      "epoch: 14 trial 6411 training loss: 0.032030087895691395\n",
      "epoch: 14 trial 6412 training loss: 0.0331512400880456\n",
      "epoch: 14 trial 6413 training loss: 0.023748550098389387\n",
      "epoch: 14 trial 6414 training loss: 0.03969655837863684\n",
      "epoch: 14 trial 6415 training loss: 0.08037207089364529\n",
      "epoch: 14 trial 6416 training loss: 0.040552107617259026\n",
      "epoch: 14 trial 6417 training loss: 0.008733896305784583\n",
      "epoch: 14 trial 6418 training loss: 0.02054991154000163\n",
      "epoch: 14 trial 6419 training loss: 0.019428332801908255\n",
      "epoch: 14 trial 6420 training loss: 0.024845676962286234\n",
      "epoch: 14 trial 6421 training loss: 0.14839473739266396\n",
      "epoch: 14 trial 6422 training loss: 0.032579670660197735\n",
      "epoch: 14 trial 6423 training loss: 0.05568960215896368\n",
      "epoch: 14 trial 6424 training loss: 0.027729038149118423\n",
      "epoch: 14 trial 6425 training loss: 0.03468709718436003\n",
      "epoch: 14 trial 6426 training loss: 0.04868229851126671\n",
      "epoch: 14 trial 6427 training loss: 0.01003613811917603\n",
      "epoch: 14 trial 6428 training loss: 0.0425069872289896\n",
      "epoch: 14 trial 6429 training loss: 0.01753156865015626\n",
      "epoch: 14 trial 6430 training loss: 0.1185271143913269\n",
      "epoch: 14 trial 6431 training loss: 0.016734530683606863\n",
      "epoch: 14 trial 6432 training loss: 0.06344108283519745\n",
      "epoch: 14 trial 6433 training loss: 0.037528728134930134\n",
      "epoch: 14 trial 6434 training loss: 0.03179782535880804\n",
      "epoch: 14 trial 6435 training loss: 0.028144920244812965\n",
      "epoch: 14 trial 6436 training loss: 0.08584596961736679\n",
      "epoch: 14 trial 6437 training loss: 0.012724213302135468\n",
      "epoch: 14 trial 6438 training loss: 0.015032379887998104\n",
      "epoch: 14 trial 6439 training loss: 0.10340312123298645\n",
      "epoch: 14 trial 6440 training loss: 0.0197677887044847\n",
      "epoch: 14 trial 6441 training loss: 0.0397011898458004\n",
      "epoch: 14 trial 6442 training loss: 0.0593918152153492\n",
      "epoch: 14 trial 6443 training loss: 0.027761286590248346\n",
      "epoch: 14 trial 6444 training loss: 0.024706427939236164\n",
      "epoch: 14 trial 6445 training loss: 0.016988529358059168\n",
      "epoch: 14 trial 6446 training loss: 0.0367263350635767\n",
      "epoch: 14 trial 6447 training loss: 0.013630484929308295\n",
      "epoch: 14 trial 6448 training loss: 0.011152419028803706\n",
      "epoch: 14 trial 6449 training loss: 0.031094761565327644\n",
      "epoch: 14 trial 6450 training loss: 0.00408426986541599\n",
      "epoch: 14 trial 6451 training loss: 0.011336219031363726\n",
      "epoch: 14 trial 6452 training loss: 0.019107656087726355\n",
      "epoch: 14 trial 6453 training loss: 0.010415179887786508\n",
      "epoch: 14 trial 6454 training loss: 0.05669932998716831\n",
      "epoch: 14 trial 6455 training loss: 0.00338921754155308\n",
      "epoch: 14 trial 6456 training loss: 0.05618717521429062\n",
      "epoch: 14 trial 6457 training loss: 0.008572914870455861\n",
      "epoch: 14 trial 6458 training loss: 0.023103120271116495\n",
      "epoch: 14 trial 6459 training loss: 0.0277840755879879\n",
      "epoch: 14 trial 6460 training loss: 0.011647040024399757\n",
      "epoch: 14 trial 6461 training loss: 0.03692605160176754\n",
      "epoch: 14 trial 6462 training loss: 0.11627531424164772\n",
      "epoch: 14 trial 6463 training loss: 0.045256162993609905\n",
      "epoch: 14 trial 6464 training loss: 0.008165794191882014\n",
      "epoch: 14 trial 6465 training loss: 0.05779652204364538\n",
      "epoch: 14 trial 6466 training loss: 0.027496484108269215\n",
      "epoch: 14 trial 6467 training loss: 0.025938939303159714\n",
      "epoch: 14 trial 6468 training loss: 0.021410568617284298\n",
      "epoch: 14 trial 6469 training loss: 0.02182873897254467\n",
      "epoch: 14 trial 6470 training loss: 0.036011938005685806\n",
      "epoch: 14 trial 6471 training loss: 0.04153374116867781\n",
      "epoch: 14 trial 6472 training loss: 0.03761355904862285\n",
      "epoch: 14 trial 6473 training loss: 0.06722383387386799\n",
      "epoch: 14 trial 6474 training loss: 0.012483728118240833\n",
      "epoch: 14 trial 6475 training loss: 0.04458630923181772\n",
      "epoch: 14 trial 6476 training loss: 0.01889636693522334\n",
      "epoch: 14 trial 6477 training loss: 0.0179859334602952\n",
      "epoch: 14 trial 6478 training loss: 0.04995411727577448\n",
      "epoch: 14 trial 6479 training loss: 0.05720571056008339\n",
      "epoch: 14 trial 6480 training loss: 0.02382641052827239\n",
      "epoch: 14 trial 6481 training loss: 0.0474230470135808\n",
      "epoch: 14 trial 6482 training loss: 0.061946555972099304\n",
      "epoch: 14 trial 6483 training loss: 0.06270569004118443\n",
      "epoch: 14 trial 6484 training loss: 0.0326885636895895\n",
      "epoch: 14 trial 6485 training loss: 0.02337712747976184\n",
      "epoch: 14 trial 6486 training loss: 0.014110338874161243\n",
      "epoch: 14 trial 6487 training loss: 0.028884670697152615\n",
      "epoch: 14 trial 6488 training loss: 0.028378632850944996\n",
      "epoch: 14 trial 6489 training loss: 0.024215423967689276\n",
      "epoch: 14 trial 6490 training loss: 0.061940500512719154\n",
      "epoch: 14 trial 6491 training loss: 0.09760677814483643\n",
      "epoch: 14 trial 6492 training loss: 0.012737121898680925\n",
      "epoch: 14 trial 6493 training loss: 0.005952423671260476\n",
      "epoch: 14 trial 6494 training loss: 0.008826849749311805\n",
      "epoch: 14 trial 6495 training loss: 0.01033423445187509\n",
      "epoch: 14 trial 6496 training loss: 0.014731142669916153\n",
      "epoch: 14 trial 6497 training loss: 0.018249014392495155\n",
      "epoch: 14 trial 6498 training loss: 0.06752549298107624\n",
      "epoch: 14 trial 6499 training loss: 0.03717679716646671\n",
      "epoch: 14 trial 6500 training loss: 0.016111364122480154\n",
      "epoch: 14 trial 6501 training loss: 0.09616116993129253\n",
      "epoch: 14 trial 6502 training loss: 0.04544603452086449\n",
      "epoch: 14 trial 6503 training loss: 0.06260646134614944\n",
      "epoch: 14 trial 6504 training loss: 0.06355986930429935\n",
      "epoch: 14 trial 6505 training loss: 0.12118218094110489\n",
      "epoch: 14 trial 6506 training loss: 0.013316437602043152\n",
      "epoch: 14 trial 6507 training loss: 0.007114190724678338\n",
      "epoch: 14 trial 6508 training loss: 0.024619233794510365\n",
      "epoch: 14 trial 6509 training loss: 0.02318373927846551\n",
      "epoch: 14 trial 6510 training loss: 0.019220449961721897\n",
      "epoch: 14 trial 6511 training loss: 0.04949073866009712\n",
      "epoch: 14 trial 6512 training loss: 0.061271173879504204\n",
      "epoch: 14 trial 6513 training loss: 0.03469537943601608\n",
      "epoch: 14 trial 6514 training loss: 0.057068537920713425\n",
      "epoch: 14 trial 6515 training loss: 0.039812395349144936\n",
      "epoch: 14 trial 6516 training loss: 0.09066259488463402\n",
      "epoch: 14 trial 6517 training loss: 0.01080045523121953\n",
      "epoch: 14 trial 6518 training loss: 0.01257411390542984\n",
      "epoch: 14 trial 6519 training loss: 0.02599998749792576\n",
      "epoch: 14 trial 6520 training loss: 0.028906118124723434\n",
      "epoch: 14 trial 6521 training loss: 0.04783928859978914\n",
      "epoch: 14 trial 6522 training loss: 0.013999695423990488\n",
      "epoch: 14 trial 6523 training loss: 0.1555088683962822\n",
      "epoch: 14 trial 6524 training loss: 0.034406643360853195\n",
      "epoch: 14 trial 6525 training loss: 0.12581704929471016\n",
      "epoch: 14 trial 6526 training loss: 0.09829031117260456\n",
      "epoch: 14 trial 6527 training loss: 0.015822339337319136\n",
      "epoch: 14 trial 6528 training loss: 0.05571018438786268\n",
      "epoch: 14 trial 6529 training loss: 0.016832320019602776\n",
      "epoch: 14 trial 6530 training loss: 0.05428426992148161\n",
      "epoch: 14 trial 6531 training loss: 0.045411392115056515\n",
      "epoch: 14 trial 6532 training loss: 0.09737441502511501\n",
      "epoch: 14 trial 6533 training loss: 0.019037327263504267\n",
      "epoch: 14 trial 6534 training loss: 0.03943517338484526\n",
      "epoch: 14 trial 6535 training loss: 0.036082932725548744\n",
      "epoch: 14 trial 6536 training loss: 0.024771766737103462\n",
      "epoch: 14 trial 6537 training loss: 0.05895407125353813\n",
      "epoch: 14 trial 6538 training loss: 0.018782966304570436\n",
      "epoch: 14 trial 6539 training loss: 0.04894230421632528\n",
      "epoch: 14 trial 6540 training loss: 0.023587530944496393\n",
      "epoch: 14 trial 6541 training loss: 0.027764148078858852\n",
      "epoch: 14 trial 6542 training loss: 0.04123273864388466\n",
      "epoch: 14 trial 6543 training loss: 0.011736118234694004\n",
      "epoch: 14 trial 6544 training loss: 0.004821241251192987\n",
      "epoch: 14 trial 6545 training loss: 0.030582956038415432\n",
      "epoch: 14 trial 6546 training loss: 0.09468966908752918\n",
      "epoch: 14 trial 6547 training loss: 0.018246084451675415\n",
      "epoch: 14 trial 6548 training loss: 0.08405200578272343\n",
      "epoch: 14 trial 6549 training loss: 0.041663738898932934\n",
      "epoch: 14 trial 6550 training loss: 0.023041827138513327\n",
      "epoch: 14 trial 6551 training loss: 0.1736186482012272\n",
      "epoch: 14 trial 6552 training loss: 0.01390745211392641\n",
      "epoch: 14 trial 6553 training loss: 0.01338678877800703\n",
      "epoch: 14 trial 6554 training loss: 0.010547212790697813\n",
      "epoch: 14 trial 6555 training loss: 0.003755704266950488\n",
      "epoch: 14 trial 6556 training loss: 0.026654991321265697\n",
      "epoch: 14 trial 6557 training loss: 0.03648201655596495\n",
      "epoch: 14 trial 6558 training loss: 0.047655132599174976\n",
      "epoch: 14 trial 6559 training loss: 0.030315211974084377\n",
      "epoch: 14 trial 6560 training loss: 0.037011949345469475\n",
      "epoch: 14 trial 6561 training loss: 0.02215104317292571\n",
      "epoch: 14 trial 6562 training loss: 0.08277589827775955\n",
      "epoch: 14 trial 6563 training loss: 0.01734744757413864\n",
      "epoch: 14 trial 6564 training loss: 0.0071389926597476006\n",
      "epoch: 14 trial 6565 training loss: 0.021456430666148663\n",
      "epoch: 14 trial 6566 training loss: 0.006888445117510855\n",
      "epoch: 14 trial 6567 training loss: 0.07002453692257404\n",
      "epoch: 14 trial 6568 training loss: 0.004171630367636681\n",
      "epoch: 14 trial 6569 training loss: 0.02415599301457405\n",
      "epoch: 14 trial 6570 training loss: 0.014369665645062923\n",
      "epoch: 14 trial 6571 training loss: 0.06223866157233715\n",
      "epoch: 14 trial 6572 training loss: 0.03405400458723307\n",
      "epoch: 14 trial 6573 training loss: 0.02471558330580592\n",
      "epoch: 14 trial 6574 training loss: 0.04442160576581955\n",
      "epoch: 14 trial 6575 training loss: 0.06275526992976665\n",
      "epoch: 14 trial 6576 training loss: 0.030624188482761383\n",
      "epoch: 14 trial 6577 training loss: 0.06935831718146801\n",
      "epoch: 14 trial 6578 training loss: 0.021931794472038746\n",
      "epoch: 14 trial 6579 training loss: 0.07756569422781467\n",
      "epoch: 14 trial 6580 training loss: 0.02098413882777095\n",
      "epoch: 14 trial 6581 training loss: 0.009033190086483955\n",
      "epoch: 14 trial 6582 training loss: 0.05091916769742966\n",
      "epoch: 14 trial 6583 training loss: 0.028810943476855755\n",
      "epoch: 14 trial 6584 training loss: 0.01634075678884983\n",
      "epoch: 14 trial 6585 training loss: 0.016235261224210262\n",
      "epoch: 14 trial 6586 training loss: 0.009084602817893028\n",
      "epoch: 14 trial 6587 training loss: 0.02249918133020401\n",
      "epoch: 14 trial 6588 training loss: 0.03155644331127405\n",
      "epoch: 14 trial 6589 training loss: 0.01746807200834155\n",
      "epoch: 14 trial 6590 training loss: 0.03384462604299188\n",
      "epoch: 14 trial 6591 training loss: 0.08431732468307018\n",
      "epoch: 14 trial 6592 training loss: 0.03573610074818134\n",
      "epoch: 14 trial 6593 training loss: 0.015532344114035368\n",
      "epoch: 14 trial 6594 training loss: 0.005720319226384163\n",
      "epoch: 14 trial 6595 training loss: 0.007589668151922524\n",
      "epoch: 14 trial 6596 training loss: 0.02838827995583415\n",
      "epoch: 14 trial 6597 training loss: 0.04932728502899408\n",
      "epoch: 14 trial 6598 training loss: 0.04917342774569988\n",
      "epoch: 14 trial 6599 training loss: 0.0028711995691992342\n",
      "epoch: 14 trial 6600 training loss: 0.03310686536133289\n",
      "epoch: 14 trial 6601 training loss: 0.035775480791926384\n",
      "epoch: 14 trial 6602 training loss: 0.02248072298243642\n",
      "epoch: 14 trial 6603 training loss: 0.011751377955079079\n",
      "epoch: 14 trial 6604 training loss: 0.01687702350318432\n",
      "epoch: 14 trial 6605 training loss: 0.0378089128062129\n",
      "epoch: 14 trial 6606 training loss: 0.02397652016952634\n",
      "epoch: 14 trial 6607 training loss: 0.03347841929644346\n",
      "epoch: 14 trial 6608 training loss: 0.06555821374058723\n",
      "epoch: 14 trial 6609 training loss: 0.025680217426270247\n",
      "epoch: 14 trial 6610 training loss: 0.03365786839276552\n",
      "epoch: 14 trial 6611 training loss: 0.06633151136338711\n",
      "epoch: 14 trial 6612 training loss: 0.03323831409215927\n",
      "epoch: 14 trial 6613 training loss: 0.06433637253940105\n",
      "epoch: 14 trial 6614 training loss: 0.018065834417939186\n",
      "epoch: 14 trial 6615 training loss: 0.03120086621493101\n",
      "epoch: 14 trial 6616 training loss: 0.026719313580542803\n",
      "epoch: 14 trial 6617 training loss: 0.021538279950618744\n",
      "epoch: 14 trial 6618 training loss: 0.02263372763991356\n",
      "epoch: 14 trial 6619 training loss: 0.08804396912455559\n",
      "epoch: 14 trial 6620 training loss: 0.012683170614764094\n",
      "epoch: 14 trial 6621 training loss: 0.08827296271920204\n",
      "epoch: 14 trial 6622 training loss: 0.02615238633006811\n",
      "epoch: 14 trial 6623 training loss: 0.006420774501748383\n",
      "epoch: 14 trial 6624 training loss: 0.053436681628227234\n",
      "epoch: 14 trial 6625 training loss: 0.06099749729037285\n",
      "epoch: 14 trial 6626 training loss: 0.00982110621407628\n",
      "epoch: 14 trial 6627 training loss: 0.03451527561992407\n",
      "epoch: 14 trial 6628 training loss: 0.008196471026167274\n",
      "epoch: 14 trial 6629 training loss: 0.030834483914077282\n",
      "epoch: 14 trial 6630 training loss: 0.019412935711443424\n",
      "epoch: 14 trial 6631 training loss: 0.03734532184898853\n",
      "epoch: 14 trial 6632 training loss: 0.2724299281835556\n",
      "epoch: 14 trial 6633 training loss: 0.0519811287522316\n",
      "epoch: 14 trial 6634 training loss: 0.008684962056577206\n",
      "epoch: 14 trial 6635 training loss: 0.017651414033025503\n",
      "epoch: 14 trial 6636 training loss: 0.13870565965771675\n",
      "epoch: 14 trial 6637 training loss: 0.0465692738071084\n",
      "epoch: 14 trial 6638 training loss: 0.0766205433756113\n",
      "epoch: 14 trial 6639 training loss: 0.04142357874661684\n",
      "epoch: 14 trial 6640 training loss: 0.03192697558552027\n",
      "epoch: 14 trial 6641 training loss: 0.015963140409439802\n",
      "epoch: 14 trial 6642 training loss: 0.07490269839763641\n",
      "epoch: 14 trial 6643 training loss: 0.02322741085663438\n",
      "epoch: 14 trial 6644 training loss: 0.08426389656960964\n",
      "epoch: 14 trial 6645 training loss: 0.01515145436860621\n",
      "epoch: 14 trial 6646 training loss: 0.007091164938174188\n",
      "epoch: 14 trial 6647 training loss: 0.03629259206354618\n",
      "epoch: 14 trial 6648 training loss: 0.006296329433098435\n",
      "epoch: 14 trial 6649 training loss: 0.013436633162200451\n",
      "epoch: 14 trial 6650 training loss: 0.09207135066390038\n",
      "epoch: 14 trial 6651 training loss: 0.07423613034188747\n",
      "epoch: 14 trial 6652 training loss: 0.024989101570099592\n",
      "epoch: 14 trial 6653 training loss: 0.105128925293684\n",
      "epoch: 14 trial 6654 training loss: 0.10550874099135399\n",
      "epoch: 14 trial 6655 training loss: 0.024021331686526537\n",
      "epoch: 14 trial 6656 training loss: 0.06397724617272615\n",
      "epoch: 14 trial 6657 training loss: 0.03800255246460438\n",
      "epoch: 14 trial 6658 training loss: 0.009245954803191125\n",
      "epoch: 14 trial 6659 training loss: 0.007047714781947434\n",
      "epoch: 14 trial 6660 training loss: 0.14347335323691368\n",
      "epoch: 14 trial 6661 training loss: 0.06439278647303581\n",
      "epoch: 14 trial 6662 training loss: 0.04508345201611519\n",
      "epoch: 14 trial 6663 training loss: 0.05220469739288092\n",
      "epoch: 14 trial 6664 training loss: 0.026398987974971533\n",
      "epoch: 14 trial 6665 training loss: 0.036584438756108284\n",
      "epoch: 14 trial 6666 training loss: 0.018210359383374453\n",
      "epoch: 14 trial 6667 training loss: 0.04508508276194334\n",
      "epoch: 14 trial 6668 training loss: 0.029183117672801018\n",
      "epoch: 14 trial 6669 training loss: 0.0435960553586483\n",
      "epoch: 14 trial 6670 training loss: 0.08424786664545536\n",
      "epoch: 14 trial 6671 training loss: 0.024946654215455055\n",
      "epoch: 14 trial 6672 training loss: 0.010938344290480018\n",
      "epoch: 14 trial 6673 training loss: 0.025434033945202827\n",
      "epoch: 14 trial 6674 training loss: 0.0032307831570506096\n",
      "epoch: 14 trial 6675 training loss: 0.04307260736823082\n",
      "epoch: 14 trial 6676 training loss: 0.036546497605741024\n",
      "epoch: 14 trial 6677 training loss: 0.016439611557871103\n",
      "epoch: 14 trial 6678 training loss: 0.0037169402930885553\n",
      "epoch: 14 trial 6679 training loss: 0.00751919683534652\n",
      "epoch: 14 trial 6680 training loss: 0.01826310670003295\n",
      "epoch: 14 trial 6681 training loss: 0.03311737859621644\n",
      "epoch: 14 trial 6682 training loss: 0.005186135531403124\n",
      "epoch: 14 trial 6683 training loss: 0.003920782939530909\n",
      "epoch: 14 trial 6684 training loss: 0.006133233546279371\n",
      "epoch: 14 trial 6685 training loss: 0.010147108929231763\n",
      "epoch: 14 trial 6686 training loss: 0.012474170420318842\n",
      "epoch: 14 trial 6687 training loss: 0.021681010257452726\n",
      "epoch: 14 trial 6688 training loss: 0.11110102757811546\n",
      "epoch: 14 trial 6689 training loss: 0.016549224499613047\n",
      "epoch: 14 trial 6690 training loss: 0.023500397335737944\n",
      "epoch: 14 trial 6691 training loss: 0.02711984235793352\n",
      "epoch: 14 trial 6692 training loss: 0.04146726056933403\n",
      "epoch: 14 trial 6693 training loss: 0.032467533834278584\n",
      "epoch: 14 trial 6694 training loss: 0.037175157107412815\n",
      "epoch: 14 trial 6695 training loss: 0.036836517974734306\n",
      "epoch: 14 trial 6696 training loss: 0.0880232285708189\n",
      "epoch: 14 trial 6697 training loss: 0.12311764433979988\n",
      "epoch: 14 trial 6698 training loss: 0.021380076184868813\n",
      "epoch: 14 trial 6699 training loss: 0.02079065516591072\n",
      "epoch: 14 trial 6700 training loss: 0.09161519259214401\n",
      "epoch: 14 trial 6701 training loss: 0.013194168219342828\n",
      "epoch: 14 trial 6702 training loss: 0.06052900291979313\n",
      "epoch: 14 trial 6703 training loss: 0.10202986374497414\n",
      "epoch: 14 trial 6704 training loss: 0.10119504295289516\n",
      "epoch: 14 trial 6705 training loss: 0.04195532435551286\n",
      "epoch: 14 trial 6706 training loss: 0.10823542438447475\n",
      "epoch: 14 trial 6707 training loss: 0.01767945336177945\n",
      "epoch: 14 trial 6708 training loss: 0.051908036693930626\n",
      "epoch: 14 trial 6709 training loss: 0.03417585510760546\n",
      "epoch: 14 trial 6710 training loss: 0.09495007619261742\n",
      "epoch: 14 trial 6711 training loss: 0.007249087793752551\n",
      "epoch: 14 trial 6712 training loss: 0.0074763179291039705\n",
      "epoch: 14 trial 6713 training loss: 0.019139239564538002\n",
      "epoch: 14 trial 6714 training loss: 0.029701105318963528\n",
      "epoch: 14 trial 6715 training loss: 0.05031066574156284\n",
      "epoch: 14 trial 6716 training loss: 0.054520963691174984\n",
      "epoch: 14 trial 6717 training loss: 0.015175952576100826\n",
      "epoch: 14 trial 6718 training loss: 0.0561226774007082\n",
      "epoch: 14 trial 6719 training loss: 0.015894368290901184\n",
      "epoch: 14 trial 6720 training loss: 0.015919333789497614\n",
      "epoch: 14 trial 6721 training loss: 0.020825643092393875\n",
      "epoch: 14 trial 6722 training loss: 0.013527905568480492\n",
      "epoch: 14 trial 6723 training loss: 0.019010388758033514\n",
      "epoch: 14 trial 6724 training loss: 0.04095155280083418\n",
      "epoch: 14 trial 6725 training loss: 0.028041062876582146\n",
      "epoch: 14 trial 6726 training loss: 0.023423983715474606\n",
      "epoch: 14 trial 6727 training loss: 0.023448884021490812\n",
      "epoch: 14 trial 6728 training loss: 0.017228745389729738\n",
      "epoch: 14 trial 6729 training loss: 0.004584841663017869\n",
      "epoch: 14 trial 6730 training loss: 0.011200556764379144\n",
      "epoch: 14 trial 6731 training loss: 0.016022703144699335\n",
      "epoch: 14 trial 6732 training loss: 0.014728294685482979\n",
      "epoch: 14 trial 6733 training loss: 0.005686257849447429\n",
      "epoch: 14 trial 6734 training loss: 0.01304040546528995\n",
      "epoch: 14 trial 6735 training loss: 0.0825575739145279\n",
      "epoch: 14 trial 6736 training loss: 0.01177350734360516\n",
      "epoch: 14 trial 6737 training loss: 0.03332545980811119\n",
      "epoch: 14 trial 6738 training loss: 0.013745107688009739\n",
      "epoch: 14 trial 6739 training loss: 0.04776269756257534\n",
      "epoch: 14 trial 6740 training loss: 0.04093390982598066\n",
      "epoch: 14 trial 6741 training loss: 0.03780790138989687\n",
      "epoch: 14 trial 6742 training loss: 0.027530460618436337\n",
      "epoch: 14 trial 6743 training loss: 0.01863504177890718\n",
      "epoch: 14 trial 6744 training loss: 0.10963599383831024\n",
      "epoch: 14 trial 6745 training loss: 0.005769110168330371\n",
      "epoch: 14 trial 6746 training loss: 0.018803640268743038\n",
      "epoch: 14 trial 6747 training loss: 0.11489314027130604\n",
      "epoch: 14 trial 6748 training loss: 0.0289359618909657\n",
      "epoch: 14 trial 6749 training loss: 0.02271399088203907\n",
      "epoch: 14 trial 6750 training loss: 0.023553994949907064\n",
      "epoch: 14 trial 6751 training loss: 0.012975005432963371\n",
      "epoch: 14 trial 6752 training loss: 0.026012279093265533\n",
      "epoch: 14 trial 6753 training loss: 0.013111797394230962\n",
      "epoch: 14 trial 6754 training loss: 0.061121607199311256\n",
      "epoch: 14 trial 6755 training loss: 0.15755467861890793\n",
      "epoch: 14 trial 6756 training loss: 0.0746601764112711\n",
      "epoch: 14 trial 6757 training loss: 0.028637646697461605\n",
      "epoch: 14 trial 6758 training loss: 0.0739281103014946\n",
      "epoch: 14 trial 6759 training loss: 0.10314085707068443\n",
      "epoch: 14 trial 6760 training loss: 0.02516747545450926\n",
      "epoch: 14 trial 6761 training loss: 0.07311775535345078\n",
      "epoch: 14 trial 6762 training loss: 0.007495878729969263\n",
      "epoch: 14 trial 6763 training loss: 0.04504423774778843\n",
      "epoch: 14 trial 6764 training loss: 0.013493293430656195\n",
      "epoch: 14 trial 6765 training loss: 0.02870785351842642\n",
      "epoch: 14 trial 6766 training loss: 0.1090458482503891\n",
      "epoch: 14 trial 6767 training loss: 0.04460275359451771\n",
      "epoch: 14 trial 6768 training loss: 0.01855943165719509\n",
      "epoch: 14 trial 6769 training loss: 0.01952081872150302\n",
      "epoch: 14 trial 6770 training loss: 0.07964242249727249\n",
      "epoch: 14 trial 6771 training loss: 0.014857126865535975\n",
      "epoch: 14 trial 6772 training loss: 0.05141050927340984\n",
      "epoch: 14 trial 6773 training loss: 0.03654472157359123\n",
      "epoch: 14 trial 6774 training loss: 0.04696097411215305\n",
      "epoch: 14 trial 6775 training loss: 0.014576489804312587\n",
      "epoch: 14 trial 6776 training loss: 0.05528265517205\n",
      "epoch: 15 trial 6777 training loss: 0.012820424046367407\n",
      "epoch: 15 trial 6778 training loss: 0.013360421173274517\n",
      "epoch: 15 trial 6779 training loss: 0.01209524623118341\n",
      "epoch: 15 trial 6780 training loss: 0.06199900060892105\n",
      "epoch: 15 trial 6781 training loss: 0.004459794843569398\n",
      "epoch: 15 trial 6782 training loss: 0.005105200340040028\n",
      "epoch: 15 trial 6783 training loss: 0.028434913605451584\n",
      "epoch: 15 trial 6784 training loss: 0.028461242094635963\n",
      "epoch: 15 trial 6785 training loss: 0.006377695593982935\n",
      "epoch: 15 trial 6786 training loss: 0.012359687825664878\n",
      "epoch: 15 trial 6787 training loss: 0.058189960196614265\n",
      "epoch: 15 trial 6788 training loss: 0.030800064094364643\n",
      "epoch: 15 trial 6789 training loss: 0.017155154142528772\n",
      "epoch: 15 trial 6790 training loss: 0.024764771573245525\n",
      "epoch: 15 trial 6791 training loss: 0.02194577269256115\n",
      "epoch: 15 trial 6792 training loss: 0.02607495104894042\n",
      "epoch: 15 trial 6793 training loss: 0.12170501798391342\n",
      "epoch: 15 trial 6794 training loss: 0.05164506006985903\n",
      "epoch: 15 trial 6795 training loss: 0.01617275970056653\n",
      "epoch: 15 trial 6796 training loss: 0.01927063101902604\n",
      "epoch: 15 trial 6797 training loss: 0.017476661130785942\n",
      "epoch: 15 trial 6798 training loss: 0.08739542961120605\n",
      "epoch: 15 trial 6799 training loss: 0.011035375762730837\n",
      "epoch: 15 trial 6800 training loss: 0.004698816570453346\n",
      "epoch: 15 trial 6801 training loss: 0.13334618508815765\n",
      "epoch: 15 trial 6802 training loss: 0.0218037785962224\n",
      "epoch: 15 trial 6803 training loss: 0.00935125071555376\n",
      "epoch: 15 trial 6804 training loss: 0.10426190495491028\n",
      "epoch: 15 trial 6805 training loss: 0.018058783374726772\n",
      "epoch: 15 trial 6806 training loss: 0.020710422191768885\n",
      "epoch: 15 trial 6807 training loss: 0.0189619823358953\n",
      "epoch: 15 trial 6808 training loss: 0.014704997418448329\n",
      "epoch: 15 trial 6809 training loss: 0.03698143642395735\n",
      "epoch: 15 trial 6810 training loss: 0.05279393307864666\n",
      "epoch: 15 trial 6811 training loss: 0.01844412786886096\n",
      "epoch: 15 trial 6812 training loss: 0.061383867636322975\n",
      "epoch: 15 trial 6813 training loss: 0.15537194162607193\n",
      "epoch: 15 trial 6814 training loss: 0.26308736577630043\n",
      "epoch: 15 trial 6815 training loss: 0.04479588475078344\n",
      "epoch: 15 trial 6816 training loss: 0.09097840636968613\n",
      "epoch: 15 trial 6817 training loss: 0.09029321372509003\n",
      "epoch: 15 trial 6818 training loss: 0.011349772568792105\n",
      "epoch: 15 trial 6819 training loss: 0.0841272696852684\n",
      "epoch: 15 trial 6820 training loss: 0.03925491403788328\n",
      "epoch: 15 trial 6821 training loss: 0.027803688310086727\n",
      "epoch: 15 trial 6822 training loss: 0.06183667108416557\n",
      "epoch: 15 trial 6823 training loss: 0.012146730674430728\n",
      "epoch: 15 trial 6824 training loss: 0.02317423978820443\n",
      "epoch: 15 trial 6825 training loss: 0.013453834224492311\n",
      "epoch: 15 trial 6826 training loss: 0.09807213023304939\n",
      "epoch: 15 trial 6827 training loss: 0.03541594464331865\n",
      "epoch: 15 trial 6828 training loss: 0.10835257917642593\n",
      "epoch: 15 trial 6829 training loss: 0.0370996268466115\n",
      "epoch: 15 trial 6830 training loss: 0.04122899193316698\n",
      "epoch: 15 trial 6831 training loss: 0.03331109881401062\n",
      "epoch: 15 trial 6832 training loss: 0.061493560671806335\n",
      "epoch: 15 trial 6833 training loss: 0.024991170968860388\n",
      "epoch: 15 trial 6834 training loss: 0.017618818674236536\n",
      "epoch: 15 trial 6835 training loss: 0.02667468460276723\n",
      "epoch: 15 trial 6836 training loss: 0.08494418486952782\n",
      "epoch: 15 trial 6837 training loss: 0.018470992799848318\n",
      "epoch: 15 trial 6838 training loss: 0.12843623757362366\n",
      "epoch: 15 trial 6839 training loss: 0.02052169432863593\n",
      "epoch: 15 trial 6840 training loss: 0.0029933625482954085\n",
      "epoch: 15 trial 6841 training loss: 0.0453575998544693\n",
      "epoch: 15 trial 6842 training loss: 0.05425231624394655\n",
      "epoch: 15 trial 6843 training loss: 0.021790087688714266\n",
      "epoch: 15 trial 6844 training loss: 0.019366662949323654\n",
      "epoch: 15 trial 6845 training loss: 0.04940103180706501\n",
      "epoch: 15 trial 6846 training loss: 0.03893946297466755\n",
      "epoch: 15 trial 6847 training loss: 0.044521158561110497\n",
      "epoch: 15 trial 6848 training loss: 0.03557702247053385\n",
      "epoch: 15 trial 6849 training loss: 0.012225381564348936\n",
      "epoch: 15 trial 6850 training loss: 0.06798571906983852\n",
      "epoch: 15 trial 6851 training loss: 0.013913808390498161\n",
      "epoch: 15 trial 6852 training loss: 0.02433632779866457\n",
      "epoch: 15 trial 6853 training loss: 0.030261144042015076\n",
      "epoch: 15 trial 6854 training loss: 0.08227193355560303\n",
      "epoch: 15 trial 6855 training loss: 0.019959870260208845\n",
      "epoch: 15 trial 6856 training loss: 0.05656650383025408\n",
      "epoch: 15 trial 6857 training loss: 0.031170019879937172\n",
      "epoch: 15 trial 6858 training loss: 0.04861572477966547\n",
      "epoch: 15 trial 6859 training loss: 0.04967500455677509\n",
      "epoch: 15 trial 6860 training loss: 0.021020641550421715\n",
      "epoch: 15 trial 6861 training loss: 0.07443054020404816\n",
      "epoch: 15 trial 6862 training loss: 0.014471819857135415\n",
      "epoch: 15 trial 6863 training loss: 0.031983074732124805\n",
      "epoch: 15 trial 6864 training loss: 0.07790580578148365\n",
      "epoch: 15 trial 6865 training loss: 0.05595065373927355\n",
      "epoch: 15 trial 6866 training loss: 0.005562939681112766\n",
      "epoch: 15 trial 6867 training loss: 0.04044453240931034\n",
      "epoch: 15 trial 6868 training loss: 0.024882523342967033\n",
      "epoch: 15 trial 6869 training loss: 0.022672484163194895\n",
      "epoch: 15 trial 6870 training loss: 0.0690123476088047\n",
      "epoch: 15 trial 6871 training loss: 0.04815315920859575\n",
      "epoch: 15 trial 6872 training loss: 0.05385264754295349\n",
      "epoch: 15 trial 6873 training loss: 0.010471887653693557\n",
      "epoch: 15 trial 6874 training loss: 0.08861153572797775\n",
      "epoch: 15 trial 6875 training loss: 0.02997603826224804\n",
      "epoch: 15 trial 6876 training loss: 0.03943774849176407\n",
      "epoch: 15 trial 6877 training loss: 0.025924183428287506\n",
      "epoch: 15 trial 6878 training loss: 0.09534214995801449\n",
      "epoch: 15 trial 6879 training loss: 0.04856747481971979\n",
      "epoch: 15 trial 6880 training loss: 0.029790185391902924\n",
      "epoch: 15 trial 6881 training loss: 0.24084967374801636\n",
      "epoch: 15 trial 6882 training loss: 0.0652222465723753\n",
      "epoch: 15 trial 6883 training loss: 0.02701186155900359\n",
      "epoch: 15 trial 6884 training loss: 0.15136102214455605\n",
      "epoch: 15 trial 6885 training loss: 0.0815729983150959\n",
      "epoch: 15 trial 6886 training loss: 0.025964749976992607\n",
      "epoch: 15 trial 6887 training loss: 0.025922144763171673\n",
      "epoch: 15 trial 6888 training loss: 0.025295640341937542\n",
      "epoch: 15 trial 6889 training loss: 0.013971125707030296\n",
      "epoch: 15 trial 6890 training loss: 0.008390337461605668\n",
      "epoch: 15 trial 6891 training loss: 0.06081688404083252\n",
      "epoch: 15 trial 6892 training loss: 0.017561045475304127\n",
      "epoch: 15 trial 6893 training loss: 0.02318274788558483\n",
      "epoch: 15 trial 6894 training loss: 0.037610797211527824\n",
      "epoch: 15 trial 6895 training loss: 0.02897348301485181\n",
      "epoch: 15 trial 6896 training loss: 0.03419253136962652\n",
      "epoch: 15 trial 6897 training loss: 0.025460761040449142\n",
      "epoch: 15 trial 6898 training loss: 0.037684337235987186\n",
      "epoch: 15 trial 6899 training loss: 0.08109547756612301\n",
      "epoch: 15 trial 6900 training loss: 0.04065686743706465\n",
      "epoch: 15 trial 6901 training loss: 0.010061800945550203\n",
      "epoch: 15 trial 6902 training loss: 0.021488626021891832\n",
      "epoch: 15 trial 6903 training loss: 0.02078263508155942\n",
      "epoch: 15 trial 6904 training loss: 0.0244561773724854\n",
      "epoch: 15 trial 6905 training loss: 0.1522546075284481\n",
      "epoch: 15 trial 6906 training loss: 0.02942693280056119\n",
      "epoch: 15 trial 6907 training loss: 0.051185837015509605\n",
      "epoch: 15 trial 6908 training loss: 0.028972762636840343\n",
      "epoch: 15 trial 6909 training loss: 0.033276223577558994\n",
      "epoch: 15 trial 6910 training loss: 0.048983581364154816\n",
      "epoch: 15 trial 6911 training loss: 0.00829070177860558\n",
      "epoch: 15 trial 6912 training loss: 0.04063259996473789\n",
      "epoch: 15 trial 6913 training loss: 0.015408662613481283\n",
      "epoch: 15 trial 6914 training loss: 0.11175878718495369\n",
      "epoch: 15 trial 6915 training loss: 0.019430271349847317\n",
      "epoch: 15 trial 6916 training loss: 0.0675654448568821\n",
      "epoch: 15 trial 6917 training loss: 0.030890438705682755\n",
      "epoch: 15 trial 6918 training loss: 0.03102254867553711\n",
      "epoch: 15 trial 6919 training loss: 0.02700972557067871\n",
      "epoch: 15 trial 6920 training loss: 0.10302451066672802\n",
      "epoch: 15 trial 6921 training loss: 0.01020993385463953\n",
      "epoch: 15 trial 6922 training loss: 0.015381534118205309\n",
      "epoch: 15 trial 6923 training loss: 0.08890908397734165\n",
      "epoch: 15 trial 6924 training loss: 0.018519725184887648\n",
      "epoch: 15 trial 6925 training loss: 0.032809450291097164\n",
      "epoch: 15 trial 6926 training loss: 0.05421547032892704\n",
      "epoch: 15 trial 6927 training loss: 0.025357761420309544\n",
      "epoch: 15 trial 6928 training loss: 0.022082707844674587\n",
      "epoch: 15 trial 6929 training loss: 0.015830470249056816\n",
      "epoch: 15 trial 6930 training loss: 0.03573850076645613\n",
      "epoch: 15 trial 6931 training loss: 0.01351315900683403\n",
      "epoch: 15 trial 6932 training loss: 0.010856500128284097\n",
      "epoch: 15 trial 6933 training loss: 0.031740802340209484\n",
      "epoch: 15 trial 6934 training loss: 0.004033635486848652\n",
      "epoch: 15 trial 6935 training loss: 0.01176748238503933\n",
      "epoch: 15 trial 6936 training loss: 0.018440817017108202\n",
      "epoch: 15 trial 6937 training loss: 0.010361912194639444\n",
      "epoch: 15 trial 6938 training loss: 0.0566384419798851\n",
      "epoch: 15 trial 6939 training loss: 0.003612922562751919\n",
      "epoch: 15 trial 6940 training loss: 0.0581282377243042\n",
      "epoch: 15 trial 6941 training loss: 0.01047209626995027\n",
      "epoch: 15 trial 6942 training loss: 0.020789629314094782\n",
      "epoch: 15 trial 6943 training loss: 0.026369416154921055\n",
      "epoch: 15 trial 6944 training loss: 0.01278278511017561\n",
      "epoch: 15 trial 6945 training loss: 0.03617693018168211\n",
      "epoch: 15 trial 6946 training loss: 0.11846864968538284\n",
      "epoch: 15 trial 6947 training loss: 0.04824868682771921\n",
      "epoch: 15 trial 6948 training loss: 0.0090814761351794\n",
      "epoch: 15 trial 6949 training loss: 0.06360625475645065\n",
      "epoch: 15 trial 6950 training loss: 0.031267534010112286\n",
      "epoch: 15 trial 6951 training loss: 0.02707595843821764\n",
      "epoch: 15 trial 6952 training loss: 0.019003781955689192\n",
      "epoch: 15 trial 6953 training loss: 0.024431880097836256\n",
      "epoch: 15 trial 6954 training loss: 0.039728617295622826\n",
      "epoch: 15 trial 6955 training loss: 0.041102778166532516\n",
      "epoch: 15 trial 6956 training loss: 0.041457219049334526\n",
      "epoch: 15 trial 6957 training loss: 0.0712419506162405\n",
      "epoch: 15 trial 6958 training loss: 0.011132946703583002\n",
      "epoch: 15 trial 6959 training loss: 0.044187613762915134\n",
      "epoch: 15 trial 6960 training loss: 0.0177593557164073\n",
      "epoch: 15 trial 6961 training loss: 0.01787464553490281\n",
      "epoch: 15 trial 6962 training loss: 0.04109055548906326\n",
      "epoch: 15 trial 6963 training loss: 0.05501735210418701\n",
      "epoch: 15 trial 6964 training loss: 0.02187334466725588\n",
      "epoch: 15 trial 6965 training loss: 0.04451657086610794\n",
      "epoch: 15 trial 6966 training loss: 0.06387070938944817\n",
      "epoch: 15 trial 6967 training loss: 0.06395050697028637\n",
      "epoch: 15 trial 6968 training loss: 0.03006504289805889\n",
      "epoch: 15 trial 6969 training loss: 0.027447615284472704\n",
      "epoch: 15 trial 6970 training loss: 0.01565966894850135\n",
      "epoch: 15 trial 6971 training loss: 0.025944487657397985\n",
      "epoch: 15 trial 6972 training loss: 0.03204383794218302\n",
      "epoch: 15 trial 6973 training loss: 0.02697513811290264\n",
      "epoch: 15 trial 6974 training loss: 0.05992305092513561\n",
      "epoch: 15 trial 6975 training loss: 0.09802438877522945\n",
      "epoch: 15 trial 6976 training loss: 0.012203842168673873\n",
      "epoch: 15 trial 6977 training loss: 0.005974380299448967\n",
      "epoch: 15 trial 6978 training loss: 0.008754623588174582\n",
      "epoch: 15 trial 6979 training loss: 0.009756140876561403\n",
      "epoch: 15 trial 6980 training loss: 0.014428656082600355\n",
      "epoch: 15 trial 6981 training loss: 0.01866259193047881\n",
      "epoch: 15 trial 6982 training loss: 0.07115131244063377\n",
      "epoch: 15 trial 6983 training loss: 0.039527710527181625\n",
      "epoch: 15 trial 6984 training loss: 0.015128310769796371\n",
      "epoch: 15 trial 6985 training loss: 0.10043087974190712\n",
      "epoch: 15 trial 6986 training loss: 0.04046874959021807\n",
      "epoch: 15 trial 6987 training loss: 0.0593456719070673\n",
      "epoch: 15 trial 6988 training loss: 0.061178967356681824\n",
      "epoch: 15 trial 6989 training loss: 0.10572729259729385\n",
      "epoch: 15 trial 6990 training loss: 0.01489536464214325\n",
      "epoch: 15 trial 6991 training loss: 0.00904168444685638\n",
      "epoch: 15 trial 6992 training loss: 0.024462750647217035\n",
      "epoch: 15 trial 6993 training loss: 0.021166945341974497\n",
      "epoch: 15 trial 6994 training loss: 0.019949920941144228\n",
      "epoch: 15 trial 6995 training loss: 0.04663503356277943\n",
      "epoch: 15 trial 6996 training loss: 0.06254779174923897\n",
      "epoch: 15 trial 6997 training loss: 0.035878751426935196\n",
      "epoch: 15 trial 6998 training loss: 0.054348474368453026\n",
      "epoch: 15 trial 6999 training loss: 0.03972590062767267\n",
      "epoch: 15 trial 7000 training loss: 0.08222802355885506\n",
      "epoch: 15 trial 7001 training loss: 0.010900761932134628\n",
      "epoch: 15 trial 7002 training loss: 0.011808102950453758\n",
      "epoch: 15 trial 7003 training loss: 0.026490459218621254\n",
      "epoch: 15 trial 7004 training loss: 0.028001241851598024\n",
      "epoch: 15 trial 7005 training loss: 0.04641381185501814\n",
      "epoch: 15 trial 7006 training loss: 0.014022887218743563\n",
      "epoch: 15 trial 7007 training loss: 0.161398246884346\n",
      "epoch: 15 trial 7008 training loss: 0.03194383531808853\n",
      "epoch: 15 trial 7009 training loss: 0.12348856404423714\n",
      "epoch: 15 trial 7010 training loss: 0.104183504357934\n",
      "epoch: 15 trial 7011 training loss: 0.016167455818504095\n",
      "epoch: 15 trial 7012 training loss: 0.055424622260034084\n",
      "epoch: 15 trial 7013 training loss: 0.015933468472212553\n",
      "epoch: 15 trial 7014 training loss: 0.04750442877411842\n",
      "epoch: 15 trial 7015 training loss: 0.047212968580424786\n",
      "epoch: 15 trial 7016 training loss: 0.0961819663643837\n",
      "epoch: 15 trial 7017 training loss: 0.017222088295966387\n",
      "epoch: 15 trial 7018 training loss: 0.03904668614268303\n",
      "epoch: 15 trial 7019 training loss: 0.040186891332268715\n",
      "epoch: 15 trial 7020 training loss: 0.025516395922750235\n",
      "epoch: 15 trial 7021 training loss: 0.05753032676875591\n",
      "epoch: 15 trial 7022 training loss: 0.018857593182474375\n",
      "epoch: 15 trial 7023 training loss: 0.04313162341713905\n",
      "epoch: 15 trial 7024 training loss: 0.021537166088819504\n",
      "epoch: 15 trial 7025 training loss: 0.02630576677620411\n",
      "epoch: 15 trial 7026 training loss: 0.038210056722164154\n",
      "epoch: 15 trial 7027 training loss: 0.011631716508418322\n",
      "epoch: 15 trial 7028 training loss: 0.005014877766370773\n",
      "epoch: 15 trial 7029 training loss: 0.030132743529975414\n",
      "epoch: 15 trial 7030 training loss: 0.10074406117200851\n",
      "epoch: 15 trial 7031 training loss: 0.019912210758775473\n",
      "epoch: 15 trial 7032 training loss: 0.09346520155668259\n",
      "epoch: 15 trial 7033 training loss: 0.043401178903877735\n",
      "epoch: 15 trial 7034 training loss: 0.023638384882360697\n",
      "epoch: 15 trial 7035 training loss: 0.16779039427638054\n",
      "epoch: 15 trial 7036 training loss: 0.012170887319371104\n",
      "epoch: 15 trial 7037 training loss: 0.013767408207058907\n",
      "epoch: 15 trial 7038 training loss: 0.01142473192885518\n",
      "epoch: 15 trial 7039 training loss: 0.003714679041877389\n",
      "epoch: 15 trial 7040 training loss: 0.02504864986985922\n",
      "epoch: 15 trial 7041 training loss: 0.03663239814341068\n",
      "epoch: 15 trial 7042 training loss: 0.045553949661552906\n",
      "epoch: 15 trial 7043 training loss: 0.03145504370331764\n",
      "epoch: 15 trial 7044 training loss: 0.04463693778961897\n",
      "epoch: 15 trial 7045 training loss: 0.021462809294462204\n",
      "epoch: 15 trial 7046 training loss: 0.0853569246828556\n",
      "epoch: 15 trial 7047 training loss: 0.01731337746605277\n",
      "epoch: 15 trial 7048 training loss: 0.007098442642018199\n",
      "epoch: 15 trial 7049 training loss: 0.021790756843984127\n",
      "epoch: 15 trial 7050 training loss: 0.006353880395181477\n",
      "epoch: 15 trial 7051 training loss: 0.07017706893384457\n",
      "epoch: 15 trial 7052 training loss: 0.0035621135029941797\n",
      "epoch: 15 trial 7053 training loss: 0.024300536140799522\n",
      "epoch: 15 trial 7054 training loss: 0.01514654140919447\n",
      "epoch: 15 trial 7055 training loss: 0.06331593170762062\n",
      "epoch: 15 trial 7056 training loss: 0.03989222273230553\n",
      "epoch: 15 trial 7057 training loss: 0.02216827403753996\n",
      "epoch: 15 trial 7058 training loss: 0.04537705425173044\n",
      "epoch: 15 trial 7059 training loss: 0.06620696187019348\n",
      "epoch: 15 trial 7060 training loss: 0.031387620605528355\n",
      "epoch: 15 trial 7061 training loss: 0.07249166630208492\n",
      "epoch: 15 trial 7062 training loss: 0.019103947561234236\n",
      "epoch: 15 trial 7063 training loss: 0.07321720011532307\n",
      "epoch: 15 trial 7064 training loss: 0.019801992923021317\n",
      "epoch: 15 trial 7065 training loss: 0.00949319009669125\n",
      "epoch: 15 trial 7066 training loss: 0.04761133994907141\n",
      "epoch: 15 trial 7067 training loss: 0.027827590238302946\n",
      "epoch: 15 trial 7068 training loss: 0.01821849960833788\n",
      "epoch: 15 trial 7069 training loss: 0.019845078699290752\n",
      "epoch: 15 trial 7070 training loss: 0.008859273977577686\n",
      "epoch: 15 trial 7071 training loss: 0.022969569079577923\n",
      "epoch: 15 trial 7072 training loss: 0.030124919023364782\n",
      "epoch: 15 trial 7073 training loss: 0.0197368785738945\n",
      "epoch: 15 trial 7074 training loss: 0.03614581283181906\n",
      "epoch: 15 trial 7075 training loss: 0.08374675922095776\n",
      "epoch: 15 trial 7076 training loss: 0.038403586484491825\n",
      "epoch: 15 trial 7077 training loss: 0.014456221368163824\n",
      "epoch: 15 trial 7078 training loss: 0.005811621900647879\n",
      "epoch: 15 trial 7079 training loss: 0.008188881445676088\n",
      "epoch: 15 trial 7080 training loss: 0.02914255391806364\n",
      "epoch: 15 trial 7081 training loss: 0.05177298653870821\n",
      "epoch: 15 trial 7082 training loss: 0.04383016563951969\n",
      "epoch: 15 trial 7083 training loss: 0.0030520380823872983\n",
      "epoch: 15 trial 7084 training loss: 0.03623625449836254\n",
      "epoch: 15 trial 7085 training loss: 0.039990879595279694\n",
      "epoch: 15 trial 7086 training loss: 0.0215975446626544\n",
      "epoch: 15 trial 7087 training loss: 0.014382759807631373\n",
      "epoch: 15 trial 7088 training loss: 0.018223377177491784\n",
      "epoch: 15 trial 7089 training loss: 0.043497975915670395\n",
      "epoch: 15 trial 7090 training loss: 0.02295295847579837\n",
      "epoch: 15 trial 7091 training loss: 0.03525485750287771\n",
      "epoch: 15 trial 7092 training loss: 0.07016108557581902\n",
      "epoch: 15 trial 7093 training loss: 0.02738974615931511\n",
      "epoch: 15 trial 7094 training loss: 0.03358559217303991\n",
      "epoch: 15 trial 7095 training loss: 0.06695179268717766\n",
      "epoch: 15 trial 7096 training loss: 0.035215988755226135\n",
      "epoch: 15 trial 7097 training loss: 0.06933242082595825\n",
      "epoch: 15 trial 7098 training loss: 0.017804334871470928\n",
      "epoch: 15 trial 7099 training loss: 0.03420464787632227\n",
      "epoch: 15 trial 7100 training loss: 0.025704785250127316\n",
      "epoch: 15 trial 7101 training loss: 0.022113794926553965\n",
      "epoch: 15 trial 7102 training loss: 0.02786488365381956\n",
      "epoch: 15 trial 7103 training loss: 0.09518579207360744\n",
      "epoch: 15 trial 7104 training loss: 0.012749038869515061\n",
      "epoch: 15 trial 7105 training loss: 0.08772947825491428\n",
      "epoch: 15 trial 7106 training loss: 0.026994663756340742\n",
      "epoch: 15 trial 7107 training loss: 0.006101020146161318\n",
      "epoch: 15 trial 7108 training loss: 0.047583164647221565\n",
      "epoch: 15 trial 7109 training loss: 0.05929895117878914\n",
      "epoch: 15 trial 7110 training loss: 0.01061503728851676\n",
      "epoch: 15 trial 7111 training loss: 0.03493731562048197\n",
      "epoch: 15 trial 7112 training loss: 0.008382753236219287\n",
      "epoch: 15 trial 7113 training loss: 0.03526380658149719\n",
      "epoch: 15 trial 7114 training loss: 0.021434723865240812\n",
      "epoch: 15 trial 7115 training loss: 0.03661733865737915\n",
      "epoch: 15 trial 7116 training loss: 0.26466427743434906\n",
      "epoch: 15 trial 7117 training loss: 0.054866304621100426\n",
      "epoch: 15 trial 7118 training loss: 0.0083199015352875\n",
      "epoch: 15 trial 7119 training loss: 0.016000085277482867\n",
      "epoch: 15 trial 7120 training loss: 0.13204898685216904\n",
      "epoch: 15 trial 7121 training loss: 0.046080457046628\n",
      "epoch: 15 trial 7122 training loss: 0.0736060831695795\n",
      "epoch: 15 trial 7123 training loss: 0.042733680456876755\n",
      "epoch: 15 trial 7124 training loss: 0.03400771878659725\n",
      "epoch: 15 trial 7125 training loss: 0.014064165530726314\n",
      "epoch: 15 trial 7126 training loss: 0.07189160026609898\n",
      "epoch: 15 trial 7127 training loss: 0.023088036570698023\n",
      "epoch: 15 trial 7128 training loss: 0.07846247591078281\n",
      "epoch: 15 trial 7129 training loss: 0.014519342919811606\n",
      "epoch: 15 trial 7130 training loss: 0.007221949170343578\n",
      "epoch: 15 trial 7131 training loss: 0.02711770450696349\n",
      "epoch: 15 trial 7132 training loss: 0.005991258309222758\n",
      "epoch: 15 trial 7133 training loss: 0.012438647914677858\n",
      "epoch: 15 trial 7134 training loss: 0.09424784779548645\n",
      "epoch: 15 trial 7135 training loss: 0.07553661987185478\n",
      "epoch: 15 trial 7136 training loss: 0.025850425008684397\n",
      "epoch: 15 trial 7137 training loss: 0.10902896523475647\n",
      "epoch: 15 trial 7138 training loss: 0.10119973868131638\n",
      "epoch: 15 trial 7139 training loss: 0.025604715570807457\n",
      "epoch: 15 trial 7140 training loss: 0.06271051056683064\n",
      "epoch: 15 trial 7141 training loss: 0.03855677880346775\n",
      "epoch: 15 trial 7142 training loss: 0.01000112434849143\n",
      "epoch: 15 trial 7143 training loss: 0.005671827355399728\n",
      "epoch: 15 trial 7144 training loss: 0.13674353435635567\n",
      "epoch: 15 trial 7145 training loss: 0.06478475593030453\n",
      "epoch: 15 trial 7146 training loss: 0.042085081338882446\n",
      "epoch: 15 trial 7147 training loss: 0.05548381805419922\n",
      "epoch: 15 trial 7148 training loss: 0.02480537537485361\n",
      "epoch: 15 trial 7149 training loss: 0.03686238545924425\n",
      "epoch: 15 trial 7150 training loss: 0.018486774526536465\n",
      "epoch: 15 trial 7151 training loss: 0.049810947850346565\n",
      "epoch: 15 trial 7152 training loss: 0.027051957324147224\n",
      "epoch: 15 trial 7153 training loss: 0.0417963108047843\n",
      "epoch: 15 trial 7154 training loss: 0.08609593845903873\n",
      "epoch: 15 trial 7155 training loss: 0.02454819018021226\n",
      "epoch: 15 trial 7156 training loss: 0.009402999770827591\n",
      "epoch: 15 trial 7157 training loss: 0.026486321352422237\n",
      "epoch: 15 trial 7158 training loss: 0.003367769531905651\n",
      "epoch: 15 trial 7159 training loss: 0.03729754872620106\n",
      "epoch: 15 trial 7160 training loss: 0.04039599746465683\n",
      "epoch: 15 trial 7161 training loss: 0.014196064788848162\n",
      "epoch: 15 trial 7162 training loss: 0.0033877281239256263\n",
      "epoch: 15 trial 7163 training loss: 0.006540803937241435\n",
      "epoch: 15 trial 7164 training loss: 0.017840903252363205\n",
      "epoch: 15 trial 7165 training loss: 0.03627781476825476\n",
      "epoch: 15 trial 7166 training loss: 0.005333650624379516\n",
      "epoch: 15 trial 7167 training loss: 0.004303877358324826\n",
      "epoch: 15 trial 7168 training loss: 0.006182825658470392\n",
      "epoch: 15 trial 7169 training loss: 0.009402598021551967\n",
      "epoch: 15 trial 7170 training loss: 0.011490106116980314\n",
      "epoch: 15 trial 7171 training loss: 0.02259408961981535\n",
      "epoch: 15 trial 7172 training loss: 0.10759792104363441\n",
      "epoch: 15 trial 7173 training loss: 0.01738118566572666\n",
      "epoch: 15 trial 7174 training loss: 0.020992991980165243\n",
      "epoch: 15 trial 7175 training loss: 0.028990630991756916\n",
      "epoch: 15 trial 7176 training loss: 0.04386688582599163\n",
      "epoch: 15 trial 7177 training loss: 0.033204492181539536\n",
      "epoch: 15 trial 7178 training loss: 0.036284904927015305\n",
      "epoch: 15 trial 7179 training loss: 0.03859784919768572\n",
      "epoch: 15 trial 7180 training loss: 0.08947169221937656\n",
      "epoch: 15 trial 7181 training loss: 0.12400081008672714\n",
      "epoch: 15 trial 7182 training loss: 0.02244016993790865\n",
      "epoch: 15 trial 7183 training loss: 0.02273298380896449\n",
      "epoch: 15 trial 7184 training loss: 0.08734035305678844\n",
      "epoch: 15 trial 7185 training loss: 0.014542481862008572\n",
      "epoch: 15 trial 7186 training loss: 0.061389053240418434\n",
      "epoch: 15 trial 7187 training loss: 0.09999084658920765\n",
      "epoch: 15 trial 7188 training loss: 0.10402843728661537\n",
      "epoch: 15 trial 7189 training loss: 0.042063274420797825\n",
      "epoch: 15 trial 7190 training loss: 0.11072658747434616\n",
      "epoch: 15 trial 7191 training loss: 0.022364183329045773\n",
      "epoch: 15 trial 7192 training loss: 0.055747345089912415\n",
      "epoch: 15 trial 7193 training loss: 0.03280741348862648\n",
      "epoch: 15 trial 7194 training loss: 0.09405716694891453\n",
      "epoch: 15 trial 7195 training loss: 0.007162236142903566\n",
      "epoch: 15 trial 7196 training loss: 0.007563851540908217\n",
      "epoch: 15 trial 7197 training loss: 0.018076755572110415\n",
      "epoch: 15 trial 7198 training loss: 0.03229858446866274\n",
      "epoch: 15 trial 7199 training loss: 0.05464038532227278\n",
      "epoch: 15 trial 7200 training loss: 0.05084175802767277\n",
      "epoch: 15 trial 7201 training loss: 0.014539110008627176\n",
      "epoch: 15 trial 7202 training loss: 0.05640038102865219\n",
      "epoch: 15 trial 7203 training loss: 0.014854070264846087\n",
      "epoch: 15 trial 7204 training loss: 0.01512496080249548\n",
      "epoch: 15 trial 7205 training loss: 0.01988256722688675\n",
      "epoch: 15 trial 7206 training loss: 0.014122986933216453\n",
      "epoch: 15 trial 7207 training loss: 0.01909505156800151\n",
      "epoch: 15 trial 7208 training loss: 0.04199443943798542\n",
      "epoch: 15 trial 7209 training loss: 0.02642768155783415\n",
      "epoch: 15 trial 7210 training loss: 0.023956369142979383\n",
      "epoch: 15 trial 7211 training loss: 0.02135788882151246\n",
      "epoch: 15 trial 7212 training loss: 0.016889984719455242\n",
      "epoch: 15 trial 7213 training loss: 0.004617886734195054\n",
      "epoch: 15 trial 7214 training loss: 0.012058448512107134\n",
      "epoch: 15 trial 7215 training loss: 0.01564651820808649\n",
      "epoch: 15 trial 7216 training loss: 0.015311698894947767\n",
      "epoch: 15 trial 7217 training loss: 0.005456934799440205\n",
      "epoch: 15 trial 7218 training loss: 0.013133833650499582\n",
      "epoch: 15 trial 7219 training loss: 0.08143775723874569\n",
      "epoch: 15 trial 7220 training loss: 0.010980973951518536\n",
      "epoch: 15 trial 7221 training loss: 0.03093294519931078\n",
      "epoch: 15 trial 7222 training loss: 0.01358950324356556\n",
      "epoch: 15 trial 7223 training loss: 0.04812938440591097\n",
      "epoch: 15 trial 7224 training loss: 0.040387134067714214\n",
      "epoch: 15 trial 7225 training loss: 0.03664605971425772\n",
      "epoch: 15 trial 7226 training loss: 0.025952807627618313\n",
      "epoch: 15 trial 7227 training loss: 0.018877236172556877\n",
      "epoch: 15 trial 7228 training loss: 0.10302447527647018\n",
      "epoch: 15 trial 7229 training loss: 0.005455752951093018\n",
      "epoch: 15 trial 7230 training loss: 0.017833816818892956\n",
      "epoch: 15 trial 7231 training loss: 0.12456086277961731\n",
      "epoch: 15 trial 7232 training loss: 0.027779548428952694\n",
      "epoch: 15 trial 7233 training loss: 0.020792927127331495\n",
      "epoch: 15 trial 7234 training loss: 0.020267536398023367\n",
      "epoch: 15 trial 7235 training loss: 0.012031444814056158\n",
      "epoch: 15 trial 7236 training loss: 0.026253914460539818\n",
      "epoch: 15 trial 7237 training loss: 0.014136562123894691\n",
      "epoch: 15 trial 7238 training loss: 0.055337389931082726\n",
      "epoch: 15 trial 7239 training loss: 0.15850353613495827\n",
      "epoch: 15 trial 7240 training loss: 0.07103280536830425\n",
      "epoch: 15 trial 7241 training loss: 0.028472882695496082\n",
      "epoch: 15 trial 7242 training loss: 0.06373725831508636\n",
      "epoch: 15 trial 7243 training loss: 0.09955010376870632\n",
      "epoch: 15 trial 7244 training loss: 0.02559647662565112\n",
      "epoch: 15 trial 7245 training loss: 0.07411763444542885\n",
      "epoch: 15 trial 7246 training loss: 0.007420625304803252\n",
      "epoch: 15 trial 7247 training loss: 0.04261018522083759\n",
      "epoch: 15 trial 7248 training loss: 0.013207801152020693\n",
      "epoch: 15 trial 7249 training loss: 0.027769117616117\n",
      "epoch: 15 trial 7250 training loss: 0.11239707469940186\n",
      "epoch: 15 trial 7251 training loss: 0.043407718650996685\n",
      "epoch: 15 trial 7252 training loss: 0.018131693825125694\n",
      "epoch: 15 trial 7253 training loss: 0.020999471191316843\n",
      "epoch: 15 trial 7254 training loss: 0.07523436285555363\n",
      "epoch: 15 trial 7255 training loss: 0.013904816471040249\n",
      "epoch: 15 trial 7256 training loss: 0.05569521151483059\n",
      "epoch: 15 trial 7257 training loss: 0.03781764954328537\n",
      "epoch: 15 trial 7258 training loss: 0.04038486164063215\n",
      "epoch: 15 trial 7259 training loss: 0.012278114911168814\n",
      "epoch: 15 trial 7260 training loss: 0.052278789691627026\n",
      "epoch: 16 trial 7261 training loss: 0.012109727133065462\n",
      "epoch: 16 trial 7262 training loss: 0.012963778804987669\n",
      "epoch: 16 trial 7263 training loss: 0.012460772413760424\n",
      "epoch: 16 trial 7264 training loss: 0.060145096853375435\n",
      "epoch: 16 trial 7265 training loss: 0.004289338365197182\n",
      "epoch: 16 trial 7266 training loss: 0.00539699662476778\n",
      "epoch: 16 trial 7267 training loss: 0.027642817702144384\n",
      "epoch: 16 trial 7268 training loss: 0.02679866272956133\n",
      "epoch: 16 trial 7269 training loss: 0.006257985369302332\n",
      "epoch: 16 trial 7270 training loss: 0.013494072016328573\n",
      "epoch: 16 trial 7271 training loss: 0.05362290423363447\n",
      "epoch: 16 trial 7272 training loss: 0.03174437861889601\n",
      "epoch: 16 trial 7273 training loss: 0.015047726221382618\n",
      "epoch: 16 trial 7274 training loss: 0.023765953723341227\n",
      "epoch: 16 trial 7275 training loss: 0.021803353913128376\n",
      "epoch: 16 trial 7276 training loss: 0.0221515828743577\n",
      "epoch: 16 trial 7277 training loss: 0.1225999966263771\n",
      "epoch: 16 trial 7278 training loss: 0.04793662950396538\n",
      "epoch: 16 trial 7279 training loss: 0.014069828670471907\n",
      "epoch: 16 trial 7280 training loss: 0.01945078605785966\n",
      "epoch: 16 trial 7281 training loss: 0.017310752999037504\n",
      "epoch: 16 trial 7282 training loss: 0.08422199450433254\n",
      "epoch: 16 trial 7283 training loss: 0.009355433052405715\n",
      "epoch: 16 trial 7284 training loss: 0.0035355507861822844\n",
      "epoch: 16 trial 7285 training loss: 0.1357995606958866\n",
      "epoch: 16 trial 7286 training loss: 0.025912552140653133\n",
      "epoch: 16 trial 7287 training loss: 0.010059627937152982\n",
      "epoch: 16 trial 7288 training loss: 0.09432394057512283\n",
      "epoch: 16 trial 7289 training loss: 0.012955042300745845\n",
      "epoch: 16 trial 7290 training loss: 0.02035882556810975\n",
      "epoch: 16 trial 7291 training loss: 0.02239591209217906\n",
      "epoch: 16 trial 7292 training loss: 0.01262421882711351\n",
      "epoch: 16 trial 7293 training loss: 0.03146420046687126\n",
      "epoch: 16 trial 7294 training loss: 0.04481478314846754\n",
      "epoch: 16 trial 7295 training loss: 0.016025343909859657\n",
      "epoch: 16 trial 7296 training loss: 0.056880541145801544\n",
      "epoch: 16 trial 7297 training loss: 0.1402074284851551\n",
      "epoch: 16 trial 7298 training loss: 0.2993095740675926\n",
      "epoch: 16 trial 7299 training loss: 0.042092678137123585\n",
      "epoch: 16 trial 7300 training loss: 0.09549094550311565\n",
      "epoch: 16 trial 7301 training loss: 0.10378389433026314\n",
      "epoch: 16 trial 7302 training loss: 0.011550935450941324\n",
      "epoch: 16 trial 7303 training loss: 0.07999624870717525\n",
      "epoch: 16 trial 7304 training loss: 0.03453469090163708\n",
      "epoch: 16 trial 7305 training loss: 0.0258442428894341\n",
      "epoch: 16 trial 7306 training loss: 0.05725138261914253\n",
      "epoch: 16 trial 7307 training loss: 0.010966446483507752\n",
      "epoch: 16 trial 7308 training loss: 0.029799124225974083\n",
      "epoch: 16 trial 7309 training loss: 0.015961253782734275\n",
      "epoch: 16 trial 7310 training loss: 0.09525138698518276\n",
      "epoch: 16 trial 7311 training loss: 0.03722536563873291\n",
      "epoch: 16 trial 7312 training loss: 0.11441579461097717\n",
      "epoch: 16 trial 7313 training loss: 0.036839185282588005\n",
      "epoch: 16 trial 7314 training loss: 0.043114593252539635\n",
      "epoch: 16 trial 7315 training loss: 0.03372984705492854\n",
      "epoch: 16 trial 7316 training loss: 0.05303770396858454\n",
      "epoch: 16 trial 7317 training loss: 0.02640698617324233\n",
      "epoch: 16 trial 7318 training loss: 0.016722179017961025\n",
      "epoch: 16 trial 7319 training loss: 0.025831735227257013\n",
      "epoch: 16 trial 7320 training loss: 0.08561902493238449\n",
      "epoch: 16 trial 7321 training loss: 0.022368669509887695\n",
      "epoch: 16 trial 7322 training loss: 0.11330517288297415\n",
      "epoch: 16 trial 7323 training loss: 0.019114957191050053\n",
      "epoch: 16 trial 7324 training loss: 0.003040312265511602\n",
      "epoch: 16 trial 7325 training loss: 0.047545118257403374\n",
      "epoch: 16 trial 7326 training loss: 0.053643723018467426\n",
      "epoch: 16 trial 7327 training loss: 0.02098642708733678\n",
      "epoch: 16 trial 7328 training loss: 0.020145120099186897\n",
      "epoch: 16 trial 7329 training loss: 0.0495408596470952\n",
      "epoch: 16 trial 7330 training loss: 0.03526026103645563\n",
      "epoch: 16 trial 7331 training loss: 0.04398976545780897\n",
      "epoch: 16 trial 7332 training loss: 0.03357100021094084\n",
      "epoch: 16 trial 7333 training loss: 0.015465031145140529\n",
      "epoch: 16 trial 7334 training loss: 0.06888802163302898\n",
      "epoch: 16 trial 7335 training loss: 0.01423344574868679\n",
      "epoch: 16 trial 7336 training loss: 0.023649650625884533\n",
      "epoch: 16 trial 7337 training loss: 0.029048267751932144\n",
      "epoch: 16 trial 7338 training loss: 0.08048962987959385\n",
      "epoch: 16 trial 7339 training loss: 0.017466761637479067\n",
      "epoch: 16 trial 7340 training loss: 0.05746886506676674\n",
      "epoch: 16 trial 7341 training loss: 0.03388814628124237\n",
      "epoch: 16 trial 7342 training loss: 0.04660300072282553\n",
      "epoch: 16 trial 7343 training loss: 0.051670282147824764\n",
      "epoch: 16 trial 7344 training loss: 0.02157224854454398\n",
      "epoch: 16 trial 7345 training loss: 0.07159669138491154\n",
      "epoch: 16 trial 7346 training loss: 0.01579280709847808\n",
      "epoch: 16 trial 7347 training loss: 0.029252472333610058\n",
      "epoch: 16 trial 7348 training loss: 0.07179016433656216\n",
      "epoch: 16 trial 7349 training loss: 0.0519638080149889\n",
      "epoch: 16 trial 7350 training loss: 0.005737751838751137\n",
      "epoch: 16 trial 7351 training loss: 0.040078421123325825\n",
      "epoch: 16 trial 7352 training loss: 0.026485571637749672\n",
      "epoch: 16 trial 7353 training loss: 0.0236896020360291\n",
      "epoch: 16 trial 7354 training loss: 0.06820766627788544\n",
      "epoch: 16 trial 7355 training loss: 0.04718842916190624\n",
      "epoch: 16 trial 7356 training loss: 0.051795097067952156\n",
      "epoch: 16 trial 7357 training loss: 0.009984705597162247\n",
      "epoch: 16 trial 7358 training loss: 0.0840925183147192\n",
      "epoch: 16 trial 7359 training loss: 0.02658320777118206\n",
      "epoch: 16 trial 7360 training loss: 0.04395634774118662\n",
      "epoch: 16 trial 7361 training loss: 0.02782214991748333\n",
      "epoch: 16 trial 7362 training loss: 0.09363939054310322\n",
      "epoch: 16 trial 7363 training loss: 0.048366619274020195\n",
      "epoch: 16 trial 7364 training loss: 0.030745758675038815\n",
      "epoch: 16 trial 7365 training loss: 0.2269570603966713\n",
      "epoch: 16 trial 7366 training loss: 0.06565484032034874\n",
      "epoch: 16 trial 7367 training loss: 0.02732335077598691\n",
      "epoch: 16 trial 7368 training loss: 0.15825293958187103\n",
      "epoch: 16 trial 7369 training loss: 0.07946894317865372\n",
      "epoch: 16 trial 7370 training loss: 0.026508728973567486\n",
      "epoch: 16 trial 7371 training loss: 0.025994674302637577\n",
      "epoch: 16 trial 7372 training loss: 0.025978578254580498\n",
      "epoch: 16 trial 7373 training loss: 0.014840041287243366\n",
      "epoch: 16 trial 7374 training loss: 0.008549255784600973\n",
      "epoch: 16 trial 7375 training loss: 0.06209453567862511\n",
      "epoch: 16 trial 7376 training loss: 0.016662380658090115\n",
      "epoch: 16 trial 7377 training loss: 0.02542319241911173\n",
      "epoch: 16 trial 7378 training loss: 0.0371248098090291\n",
      "epoch: 16 trial 7379 training loss: 0.029433247167617083\n",
      "epoch: 16 trial 7380 training loss: 0.03209289629012346\n",
      "epoch: 16 trial 7381 training loss: 0.024289578665047884\n",
      "epoch: 16 trial 7382 training loss: 0.0394027279689908\n",
      "epoch: 16 trial 7383 training loss: 0.08212293311953545\n",
      "epoch: 16 trial 7384 training loss: 0.038532160222530365\n",
      "epoch: 16 trial 7385 training loss: 0.009323689388111234\n",
      "epoch: 16 trial 7386 training loss: 0.020749276038259268\n",
      "epoch: 16 trial 7387 training loss: 0.020244513172656298\n",
      "epoch: 16 trial 7388 training loss: 0.024900703690946102\n",
      "epoch: 16 trial 7389 training loss: 0.13962578400969505\n",
      "epoch: 16 trial 7390 training loss: 0.030219064094126225\n",
      "epoch: 16 trial 7391 training loss: 0.04907352477312088\n",
      "epoch: 16 trial 7392 training loss: 0.027335847727954388\n",
      "epoch: 16 trial 7393 training loss: 0.03359179198741913\n",
      "epoch: 16 trial 7394 training loss: 0.043280238285660744\n",
      "epoch: 16 trial 7395 training loss: 0.009535251185297966\n",
      "epoch: 16 trial 7396 training loss: 0.04144078213721514\n",
      "epoch: 16 trial 7397 training loss: 0.014568254351615906\n",
      "epoch: 16 trial 7398 training loss: 0.11646992713212967\n",
      "epoch: 16 trial 7399 training loss: 0.019724128302186728\n",
      "epoch: 16 trial 7400 training loss: 0.06748104654252529\n",
      "epoch: 16 trial 7401 training loss: 0.033532291650772095\n",
      "epoch: 16 trial 7402 training loss: 0.03209391515702009\n",
      "epoch: 16 trial 7403 training loss: 0.025785334408283234\n",
      "epoch: 16 trial 7404 training loss: 0.11227291077375412\n",
      "epoch: 16 trial 7405 training loss: 0.009224026929587126\n",
      "epoch: 16 trial 7406 training loss: 0.015149309299886227\n",
      "epoch: 16 trial 7407 training loss: 0.08483528904616833\n",
      "epoch: 16 trial 7408 training loss: 0.017204504692927003\n",
      "epoch: 16 trial 7409 training loss: 0.03174336161464453\n",
      "epoch: 16 trial 7410 training loss: 0.054803838953375816\n",
      "epoch: 16 trial 7411 training loss: 0.026105985045433044\n",
      "epoch: 16 trial 7412 training loss: 0.021771957632154226\n",
      "epoch: 16 trial 7413 training loss: 0.015081329736858606\n",
      "epoch: 16 trial 7414 training loss: 0.0336717925965786\n",
      "epoch: 16 trial 7415 training loss: 0.012962480774149299\n",
      "epoch: 16 trial 7416 training loss: 0.009526991518214345\n",
      "epoch: 16 trial 7417 training loss: 0.03230082057416439\n",
      "epoch: 16 trial 7418 training loss: 0.0044499828945845366\n",
      "epoch: 16 trial 7419 training loss: 0.01084757107309997\n",
      "epoch: 16 trial 7420 training loss: 0.018844932783395052\n",
      "epoch: 16 trial 7421 training loss: 0.010863618925213814\n",
      "epoch: 16 trial 7422 training loss: 0.05608978122472763\n",
      "epoch: 16 trial 7423 training loss: 0.0043753322679549456\n",
      "epoch: 16 trial 7424 training loss: 0.056652420200407505\n",
      "epoch: 16 trial 7425 training loss: 0.011339320102706552\n",
      "epoch: 16 trial 7426 training loss: 0.02268583094701171\n",
      "epoch: 16 trial 7427 training loss: 0.024748611263930798\n",
      "epoch: 16 trial 7428 training loss: 0.012945175636559725\n",
      "epoch: 16 trial 7429 training loss: 0.03608234319835901\n",
      "epoch: 16 trial 7430 training loss: 0.11860919371247292\n",
      "epoch: 16 trial 7431 training loss: 0.046725752763450146\n",
      "epoch: 16 trial 7432 training loss: 0.008812152780592442\n",
      "epoch: 16 trial 7433 training loss: 0.06623770296573639\n",
      "epoch: 16 trial 7434 training loss: 0.03057432919740677\n",
      "epoch: 16 trial 7435 training loss: 0.024001211393624544\n",
      "epoch: 16 trial 7436 training loss: 0.0175422471947968\n",
      "epoch: 16 trial 7437 training loss: 0.02467533340677619\n",
      "epoch: 16 trial 7438 training loss: 0.035448373295366764\n",
      "epoch: 16 trial 7439 training loss: 0.04122566618025303\n",
      "epoch: 16 trial 7440 training loss: 0.04117952845990658\n",
      "epoch: 16 trial 7441 training loss: 0.07544622384011745\n",
      "epoch: 16 trial 7442 training loss: 0.009597816737368703\n",
      "epoch: 16 trial 7443 training loss: 0.05065827909857035\n",
      "epoch: 16 trial 7444 training loss: 0.018869638442993164\n",
      "epoch: 16 trial 7445 training loss: 0.017154095228761435\n",
      "epoch: 16 trial 7446 training loss: 0.04350095707923174\n",
      "epoch: 16 trial 7447 training loss: 0.054473210126161575\n",
      "epoch: 16 trial 7448 training loss: 0.020225781947374344\n",
      "epoch: 16 trial 7449 training loss: 0.042467537336051464\n",
      "epoch: 16 trial 7450 training loss: 0.06514895707368851\n",
      "epoch: 16 trial 7451 training loss: 0.06801361031830311\n",
      "epoch: 16 trial 7452 training loss: 0.02532815281301737\n",
      "epoch: 16 trial 7453 training loss: 0.022803657222539186\n",
      "epoch: 16 trial 7454 training loss: 0.014077247120440006\n",
      "epoch: 16 trial 7455 training loss: 0.028613340109586716\n",
      "epoch: 16 trial 7456 training loss: 0.030229988507926464\n",
      "epoch: 16 trial 7457 training loss: 0.026833294425159693\n",
      "epoch: 16 trial 7458 training loss: 0.06840015389025211\n",
      "epoch: 16 trial 7459 training loss: 0.10252752341330051\n",
      "epoch: 16 trial 7460 training loss: 0.01344279176555574\n",
      "epoch: 16 trial 7461 training loss: 0.0061389446491375566\n",
      "epoch: 16 trial 7462 training loss: 0.00847844430245459\n",
      "epoch: 16 trial 7463 training loss: 0.009852836141362786\n",
      "epoch: 16 trial 7464 training loss: 0.015490095596760511\n",
      "epoch: 16 trial 7465 training loss: 0.01856279792264104\n",
      "epoch: 16 trial 7466 training loss: 0.08411858417093754\n",
      "epoch: 16 trial 7467 training loss: 0.03493971098214388\n",
      "epoch: 16 trial 7468 training loss: 0.01615951140411198\n",
      "epoch: 16 trial 7469 training loss: 0.10180569998919964\n",
      "epoch: 16 trial 7470 training loss: 0.047146305441856384\n",
      "epoch: 16 trial 7471 training loss: 0.06206745281815529\n",
      "epoch: 16 trial 7472 training loss: 0.06451122835278511\n",
      "epoch: 16 trial 7473 training loss: 0.12254847958683968\n",
      "epoch: 16 trial 7474 training loss: 0.012776171555742621\n",
      "epoch: 16 trial 7475 training loss: 0.007294240174815059\n",
      "epoch: 16 trial 7476 training loss: 0.022683839313685894\n",
      "epoch: 16 trial 7477 training loss: 0.02220735140144825\n",
      "epoch: 16 trial 7478 training loss: 0.01963672088459134\n",
      "epoch: 16 trial 7479 training loss: 0.04968354478478432\n",
      "epoch: 16 trial 7480 training loss: 0.06513294577598572\n",
      "epoch: 16 trial 7481 training loss: 0.03187090437859297\n",
      "epoch: 16 trial 7482 training loss: 0.05342604219913483\n",
      "epoch: 16 trial 7483 training loss: 0.03741762228310108\n",
      "epoch: 16 trial 7484 training loss: 0.08409751579165459\n",
      "epoch: 16 trial 7485 training loss: 0.012088477844372392\n",
      "epoch: 16 trial 7486 training loss: 0.011697339592501521\n",
      "epoch: 16 trial 7487 training loss: 0.02502135979011655\n",
      "epoch: 16 trial 7488 training loss: 0.026791024953126907\n",
      "epoch: 16 trial 7489 training loss: 0.04569057282060385\n",
      "epoch: 16 trial 7490 training loss: 0.014827440958470106\n",
      "epoch: 16 trial 7491 training loss: 0.15878652036190033\n",
      "epoch: 16 trial 7492 training loss: 0.027300644665956497\n",
      "epoch: 16 trial 7493 training loss: 0.10928190127015114\n",
      "epoch: 16 trial 7494 training loss: 0.09471656009554863\n",
      "epoch: 16 trial 7495 training loss: 0.013883207691833377\n",
      "epoch: 16 trial 7496 training loss: 0.06173183582723141\n",
      "epoch: 16 trial 7497 training loss: 0.016883773263543844\n",
      "epoch: 16 trial 7498 training loss: 0.05196823179721832\n",
      "epoch: 16 trial 7499 training loss: 0.04592519253492355\n",
      "epoch: 16 trial 7500 training loss: 0.08795372396707535\n",
      "epoch: 16 trial 7501 training loss: 0.015932629350572824\n",
      "epoch: 16 trial 7502 training loss: 0.03722681105136871\n",
      "epoch: 16 trial 7503 training loss: 0.03481898084282875\n",
      "epoch: 16 trial 7504 training loss: 0.025737062096595764\n",
      "epoch: 16 trial 7505 training loss: 0.061208995059132576\n",
      "epoch: 16 trial 7506 training loss: 0.018132229335606098\n",
      "epoch: 16 trial 7507 training loss: 0.0427319947630167\n",
      "epoch: 16 trial 7508 training loss: 0.02202452626079321\n",
      "epoch: 16 trial 7509 training loss: 0.02486715791746974\n",
      "epoch: 16 trial 7510 training loss: 0.039133600890636444\n",
      "epoch: 16 trial 7511 training loss: 0.011820213869214058\n",
      "epoch: 16 trial 7512 training loss: 0.006155526265501976\n",
      "epoch: 16 trial 7513 training loss: 0.030773631297051907\n",
      "epoch: 16 trial 7514 training loss: 0.09301921352744102\n",
      "epoch: 16 trial 7515 training loss: 0.019385752268135548\n",
      "epoch: 16 trial 7516 training loss: 0.08922342583537102\n",
      "epoch: 16 trial 7517 training loss: 0.04575453046709299\n",
      "epoch: 16 trial 7518 training loss: 0.02468780754134059\n",
      "epoch: 16 trial 7519 training loss: 0.15512892976403236\n",
      "epoch: 16 trial 7520 training loss: 0.012918110936880112\n",
      "epoch: 16 trial 7521 training loss: 0.012852413346990943\n",
      "epoch: 16 trial 7522 training loss: 0.010566621785983443\n",
      "epoch: 16 trial 7523 training loss: 0.004280985449440777\n",
      "epoch: 16 trial 7524 training loss: 0.028659704606980085\n",
      "epoch: 16 trial 7525 training loss: 0.034339381381869316\n",
      "epoch: 16 trial 7526 training loss: 0.046976566314697266\n",
      "epoch: 16 trial 7527 training loss: 0.03133101016283035\n",
      "epoch: 16 trial 7528 training loss: 0.03991624806076288\n",
      "epoch: 16 trial 7529 training loss: 0.020039535127580166\n",
      "epoch: 16 trial 7530 training loss: 0.0821203701198101\n",
      "epoch: 16 trial 7531 training loss: 0.017413153313100338\n",
      "epoch: 16 trial 7532 training loss: 0.007147877244278789\n",
      "epoch: 16 trial 7533 training loss: 0.021848957519978285\n",
      "epoch: 16 trial 7534 training loss: 0.006329549942165613\n",
      "epoch: 16 trial 7535 training loss: 0.065632164478302\n",
      "epoch: 16 trial 7536 training loss: 0.0037519066827371716\n",
      "epoch: 16 trial 7537 training loss: 0.022164123132824898\n",
      "epoch: 16 trial 7538 training loss: 0.01443290151655674\n",
      "epoch: 16 trial 7539 training loss: 0.06028022989630699\n",
      "epoch: 16 trial 7540 training loss: 0.038305751979351044\n",
      "epoch: 16 trial 7541 training loss: 0.02187610836699605\n",
      "epoch: 16 trial 7542 training loss: 0.04330920148640871\n",
      "epoch: 16 trial 7543 training loss: 0.06403372064232826\n",
      "epoch: 16 trial 7544 training loss: 0.029380626045167446\n",
      "epoch: 16 trial 7545 training loss: 0.07262432016432285\n",
      "epoch: 16 trial 7546 training loss: 0.020027749706059694\n",
      "epoch: 16 trial 7547 training loss: 0.07386280037462711\n",
      "epoch: 16 trial 7548 training loss: 0.020101742818951607\n",
      "epoch: 16 trial 7549 training loss: 0.009215136058628559\n",
      "epoch: 16 trial 7550 training loss: 0.04602277185767889\n",
      "epoch: 16 trial 7551 training loss: 0.029589521698653698\n",
      "epoch: 16 trial 7552 training loss: 0.01733412127941847\n",
      "epoch: 16 trial 7553 training loss: 0.019487083423882723\n",
      "epoch: 16 trial 7554 training loss: 0.009588416898623109\n",
      "epoch: 16 trial 7555 training loss: 0.022403538692742586\n",
      "epoch: 16 trial 7556 training loss: 0.03200613986700773\n",
      "epoch: 16 trial 7557 training loss: 0.015701700001955032\n",
      "epoch: 16 trial 7558 training loss: 0.03228523675352335\n",
      "epoch: 16 trial 7559 training loss: 0.08473067358136177\n",
      "epoch: 16 trial 7560 training loss: 0.03767778351902962\n",
      "epoch: 16 trial 7561 training loss: 0.013398523908108473\n",
      "epoch: 16 trial 7562 training loss: 0.005088348058052361\n",
      "epoch: 16 trial 7563 training loss: 0.007443987298756838\n",
      "epoch: 16 trial 7564 training loss: 0.027590382378548384\n",
      "epoch: 16 trial 7565 training loss: 0.050980305299162865\n",
      "epoch: 16 trial 7566 training loss: 0.04527804534882307\n",
      "epoch: 16 trial 7567 training loss: 0.00364420551341027\n",
      "epoch: 16 trial 7568 training loss: 0.03652809653431177\n",
      "epoch: 16 trial 7569 training loss: 0.0352952815592289\n",
      "epoch: 16 trial 7570 training loss: 0.021382318809628487\n",
      "epoch: 16 trial 7571 training loss: 0.014934360748156905\n",
      "epoch: 16 trial 7572 training loss: 0.0181348267942667\n",
      "epoch: 16 trial 7573 training loss: 0.04204206541180611\n",
      "epoch: 16 trial 7574 training loss: 0.023862052243202925\n",
      "epoch: 16 trial 7575 training loss: 0.034504225477576256\n",
      "epoch: 16 trial 7576 training loss: 0.07633463479578495\n",
      "epoch: 16 trial 7577 training loss: 0.02553355786949396\n",
      "epoch: 16 trial 7578 training loss: 0.028720194939523935\n",
      "epoch: 16 trial 7579 training loss: 0.06406205892562866\n",
      "epoch: 16 trial 7580 training loss: 0.033337184228003025\n",
      "epoch: 16 trial 7581 training loss: 0.061871832236647606\n",
      "epoch: 16 trial 7582 training loss: 0.017597145400941372\n",
      "epoch: 16 trial 7583 training loss: 0.03465334419161081\n",
      "epoch: 16 trial 7584 training loss: 0.026852660812437534\n",
      "epoch: 16 trial 7585 training loss: 0.02230327157303691\n",
      "epoch: 16 trial 7586 training loss: 0.02820146083831787\n",
      "epoch: 16 trial 7587 training loss: 0.08963658474385738\n",
      "epoch: 16 trial 7588 training loss: 0.013200168730691075\n",
      "epoch: 16 trial 7589 training loss: 0.08909214846789837\n",
      "epoch: 16 trial 7590 training loss: 0.027583550661802292\n",
      "epoch: 16 trial 7591 training loss: 0.007235416444018483\n",
      "epoch: 16 trial 7592 training loss: 0.046253930777311325\n",
      "epoch: 16 trial 7593 training loss: 0.057161686941981316\n",
      "epoch: 16 trial 7594 training loss: 0.009535989142023027\n",
      "epoch: 16 trial 7595 training loss: 0.033174154348671436\n",
      "epoch: 16 trial 7596 training loss: 0.008417274337261915\n",
      "epoch: 16 trial 7597 training loss: 0.03469996713101864\n",
      "epoch: 16 trial 7598 training loss: 0.02075845282524824\n",
      "epoch: 16 trial 7599 training loss: 0.034806251525878906\n",
      "epoch: 16 trial 7600 training loss: 0.2666972354054451\n",
      "epoch: 16 trial 7601 training loss: 0.054822443053126335\n",
      "epoch: 16 trial 7602 training loss: 0.008267096010968089\n",
      "epoch: 16 trial 7603 training loss: 0.017534948885440826\n",
      "epoch: 16 trial 7604 training loss: 0.13933495804667473\n",
      "epoch: 16 trial 7605 training loss: 0.05100675579160452\n",
      "epoch: 16 trial 7606 training loss: 0.07957159169018269\n",
      "epoch: 16 trial 7607 training loss: 0.04641467146575451\n",
      "epoch: 16 trial 7608 training loss: 0.03348839096724987\n",
      "epoch: 16 trial 7609 training loss: 0.01367663755081594\n",
      "epoch: 16 trial 7610 training loss: 0.0745120570063591\n",
      "epoch: 16 trial 7611 training loss: 0.026479778811335564\n",
      "epoch: 16 trial 7612 training loss: 0.08388266898691654\n",
      "epoch: 16 trial 7613 training loss: 0.014287236612290144\n",
      "epoch: 16 trial 7614 training loss: 0.0069012969033792615\n",
      "epoch: 16 trial 7615 training loss: 0.03213872294872999\n",
      "epoch: 16 trial 7616 training loss: 0.006070489878766239\n",
      "epoch: 16 trial 7617 training loss: 0.013237800914794207\n",
      "epoch: 16 trial 7618 training loss: 0.09487170167267323\n",
      "epoch: 16 trial 7619 training loss: 0.07104789651930332\n",
      "epoch: 16 trial 7620 training loss: 0.026493946090340614\n",
      "epoch: 16 trial 7621 training loss: 0.09970496781170368\n",
      "epoch: 16 trial 7622 training loss: 0.10594550520181656\n",
      "epoch: 16 trial 7623 training loss: 0.024630825966596603\n",
      "epoch: 16 trial 7624 training loss: 0.06010098196566105\n",
      "epoch: 16 trial 7625 training loss: 0.034809669479727745\n",
      "epoch: 16 trial 7626 training loss: 0.01164855994284153\n",
      "epoch: 16 trial 7627 training loss: 0.00473372801207006\n",
      "epoch: 16 trial 7628 training loss: 0.1329813338816166\n",
      "epoch: 16 trial 7629 training loss: 0.0722031332552433\n",
      "epoch: 16 trial 7630 training loss: 0.04648503661155701\n",
      "epoch: 16 trial 7631 training loss: 0.0524951396510005\n",
      "epoch: 16 trial 7632 training loss: 0.024354225024580956\n",
      "epoch: 16 trial 7633 training loss: 0.03901035897433758\n",
      "epoch: 16 trial 7634 training loss: 0.017670337576419115\n",
      "epoch: 16 trial 7635 training loss: 0.048418136313557625\n",
      "epoch: 16 trial 7636 training loss: 0.027154404670000076\n",
      "epoch: 16 trial 7637 training loss: 0.037454474717378616\n",
      "epoch: 16 trial 7638 training loss: 0.08470362983644009\n",
      "epoch: 16 trial 7639 training loss: 0.023054159246385098\n",
      "epoch: 16 trial 7640 training loss: 0.010097359772771597\n",
      "epoch: 16 trial 7641 training loss: 0.029688549228012562\n",
      "epoch: 16 trial 7642 training loss: 0.003464851761236787\n",
      "epoch: 16 trial 7643 training loss: 0.041985297575592995\n",
      "epoch: 16 trial 7644 training loss: 0.03998320735991001\n",
      "epoch: 16 trial 7645 training loss: 0.0124323396012187\n",
      "epoch: 16 trial 7646 training loss: 0.002969243680126965\n",
      "epoch: 16 trial 7647 training loss: 0.005159746273420751\n",
      "epoch: 16 trial 7648 training loss: 0.01772100990638137\n",
      "epoch: 16 trial 7649 training loss: 0.035453617572784424\n",
      "epoch: 16 trial 7650 training loss: 0.005130017176270485\n",
      "epoch: 16 trial 7651 training loss: 0.004286038223654032\n",
      "epoch: 16 trial 7652 training loss: 0.0062565968837589025\n",
      "epoch: 16 trial 7653 training loss: 0.009852235205471516\n",
      "epoch: 16 trial 7654 training loss: 0.011426545679569244\n",
      "epoch: 16 trial 7655 training loss: 0.022219892591238022\n",
      "epoch: 16 trial 7656 training loss: 0.10837247967720032\n",
      "epoch: 16 trial 7657 training loss: 0.016981039196252823\n",
      "epoch: 16 trial 7658 training loss: 0.020261650439351797\n",
      "epoch: 16 trial 7659 training loss: 0.028865003027021885\n",
      "epoch: 16 trial 7660 training loss: 0.042575267143547535\n",
      "epoch: 16 trial 7661 training loss: 0.03568638674914837\n",
      "epoch: 16 trial 7662 training loss: 0.03386968746781349\n",
      "epoch: 16 trial 7663 training loss: 0.03942146431654692\n",
      "epoch: 16 trial 7664 training loss: 0.0930467713624239\n",
      "epoch: 16 trial 7665 training loss: 0.12281114608049393\n",
      "epoch: 16 trial 7666 training loss: 0.019688881002366543\n",
      "epoch: 16 trial 7667 training loss: 0.026311627589166164\n",
      "epoch: 16 trial 7668 training loss: 0.07961329817771912\n",
      "epoch: 16 trial 7669 training loss: 0.016094813123345375\n",
      "epoch: 16 trial 7670 training loss: 0.06196468695998192\n",
      "epoch: 16 trial 7671 training loss: 0.10156632214784622\n",
      "epoch: 16 trial 7672 training loss: 0.094941645860672\n",
      "epoch: 16 trial 7673 training loss: 0.03911228943616152\n",
      "epoch: 16 trial 7674 training loss: 0.1058665756136179\n",
      "epoch: 16 trial 7675 training loss: 0.02068814868107438\n",
      "epoch: 16 trial 7676 training loss: 0.05144172068685293\n",
      "epoch: 16 trial 7677 training loss: 0.03425076324492693\n",
      "epoch: 16 trial 7678 training loss: 0.0927006397396326\n",
      "epoch: 16 trial 7679 training loss: 0.007035005372017622\n",
      "epoch: 16 trial 7680 training loss: 0.006582909729331732\n",
      "epoch: 16 trial 7681 training loss: 0.018890084698796272\n",
      "epoch: 16 trial 7682 training loss: 0.032823167741298676\n",
      "epoch: 16 trial 7683 training loss: 0.057906837202608585\n",
      "epoch: 16 trial 7684 training loss: 0.04880412854254246\n",
      "epoch: 16 trial 7685 training loss: 0.015462099574506283\n",
      "epoch: 16 trial 7686 training loss: 0.05444996617734432\n",
      "epoch: 16 trial 7687 training loss: 0.01667676493525505\n",
      "epoch: 16 trial 7688 training loss: 0.015164220239967108\n",
      "epoch: 16 trial 7689 training loss: 0.02040017396211624\n",
      "epoch: 16 trial 7690 training loss: 0.013092794455587864\n",
      "epoch: 16 trial 7691 training loss: 0.01981707103550434\n",
      "epoch: 16 trial 7692 training loss: 0.038669830188155174\n",
      "epoch: 16 trial 7693 training loss: 0.027072234079241753\n",
      "epoch: 16 trial 7694 training loss: 0.02237399574369192\n",
      "epoch: 16 trial 7695 training loss: 0.020587125793099403\n",
      "epoch: 16 trial 7696 training loss: 0.01779410894960165\n",
      "epoch: 16 trial 7697 training loss: 0.0044662459986284375\n",
      "epoch: 16 trial 7698 training loss: 0.011804067995399237\n",
      "epoch: 16 trial 7699 training loss: 0.015833976212888956\n",
      "epoch: 16 trial 7700 training loss: 0.015021800994873047\n",
      "epoch: 16 trial 7701 training loss: 0.0057710218243300915\n",
      "epoch: 16 trial 7702 training loss: 0.012620077468454838\n",
      "epoch: 16 trial 7703 training loss: 0.08270695619285107\n",
      "epoch: 16 trial 7704 training loss: 0.010868162848055363\n",
      "epoch: 16 trial 7705 training loss: 0.031754557974636555\n",
      "epoch: 16 trial 7706 training loss: 0.014069706201553345\n",
      "epoch: 16 trial 7707 training loss: 0.04512258339673281\n",
      "epoch: 16 trial 7708 training loss: 0.039381636306643486\n",
      "epoch: 16 trial 7709 training loss: 0.039585523307323456\n",
      "epoch: 16 trial 7710 training loss: 0.02602996863424778\n",
      "epoch: 16 trial 7711 training loss: 0.0213091685436666\n",
      "epoch: 16 trial 7712 training loss: 0.11053841188549995\n",
      "epoch: 16 trial 7713 training loss: 0.005511857918463647\n",
      "epoch: 16 trial 7714 training loss: 0.018559028394520283\n",
      "epoch: 16 trial 7715 training loss: 0.1294039785861969\n",
      "epoch: 16 trial 7716 training loss: 0.024944222066551447\n",
      "epoch: 16 trial 7717 training loss: 0.021209845785051584\n",
      "epoch: 16 trial 7718 training loss: 0.02200098056346178\n",
      "epoch: 16 trial 7719 training loss: 0.012880788883194327\n",
      "epoch: 16 trial 7720 training loss: 0.025472551118582487\n",
      "epoch: 16 trial 7721 training loss: 0.013954931870102882\n",
      "epoch: 16 trial 7722 training loss: 0.05899311229586601\n",
      "epoch: 16 trial 7723 training loss: 0.153069656342268\n",
      "epoch: 16 trial 7724 training loss: 0.07168486714363098\n",
      "epoch: 16 trial 7725 training loss: 0.029240046627819538\n",
      "epoch: 16 trial 7726 training loss: 0.06571117416024208\n",
      "epoch: 16 trial 7727 training loss: 0.0972560103982687\n",
      "epoch: 16 trial 7728 training loss: 0.02611084282398224\n",
      "epoch: 16 trial 7729 training loss: 0.0742967277765274\n",
      "epoch: 16 trial 7730 training loss: 0.0082862488925457\n",
      "epoch: 16 trial 7731 training loss: 0.04027759749442339\n",
      "epoch: 16 trial 7732 training loss: 0.012567886617034674\n",
      "epoch: 16 trial 7733 training loss: 0.03148653591051698\n",
      "epoch: 16 trial 7734 training loss: 0.1115061305463314\n",
      "epoch: 16 trial 7735 training loss: 0.04599249456077814\n",
      "epoch: 16 trial 7736 training loss: 0.018851067405194044\n",
      "epoch: 16 trial 7737 training loss: 0.021745804697275162\n",
      "epoch: 16 trial 7738 training loss: 0.0740991123020649\n",
      "epoch: 16 trial 7739 training loss: 0.014910213649272919\n",
      "epoch: 16 trial 7740 training loss: 0.0529454555362463\n",
      "epoch: 16 trial 7741 training loss: 0.03535044193267822\n",
      "epoch: 16 trial 7742 training loss: 0.04260533396154642\n",
      "epoch: 16 trial 7743 training loss: 0.013368330430239439\n",
      "epoch: 16 trial 7744 training loss: 0.05335984006524086\n",
      "epoch: 17 trial 7745 training loss: 0.013233417179435492\n",
      "epoch: 17 trial 7746 training loss: 0.01384995854459703\n",
      "epoch: 17 trial 7747 training loss: 0.011818885803222656\n",
      "epoch: 17 trial 7748 training loss: 0.05901520326733589\n",
      "epoch: 17 trial 7749 training loss: 0.004336583195254207\n",
      "epoch: 17 trial 7750 training loss: 0.005240429542027414\n",
      "epoch: 17 trial 7751 training loss: 0.02713877707719803\n",
      "epoch: 17 trial 7752 training loss: 0.027394223026931286\n",
      "epoch: 17 trial 7753 training loss: 0.005880394717678428\n",
      "epoch: 17 trial 7754 training loss: 0.013787019299343228\n",
      "epoch: 17 trial 7755 training loss: 0.049463823437690735\n",
      "epoch: 17 trial 7756 training loss: 0.033930647652596235\n",
      "epoch: 17 trial 7757 training loss: 0.01593177579343319\n",
      "epoch: 17 trial 7758 training loss: 0.01904587307944894\n",
      "epoch: 17 trial 7759 training loss: 0.02224984159693122\n",
      "epoch: 17 trial 7760 training loss: 0.022760972380638123\n",
      "epoch: 17 trial 7761 training loss: 0.11570519022643566\n",
      "epoch: 17 trial 7762 training loss: 0.05217550229281187\n",
      "epoch: 17 trial 7763 training loss: 0.017252106219530106\n",
      "epoch: 17 trial 7764 training loss: 0.02024149475619197\n",
      "epoch: 17 trial 7765 training loss: 0.017543801106512547\n",
      "epoch: 17 trial 7766 training loss: 0.07668820582330227\n",
      "epoch: 17 trial 7767 training loss: 0.014233248075470328\n",
      "epoch: 17 trial 7768 training loss: 0.0033488181652501225\n",
      "epoch: 17 trial 7769 training loss: 0.13570815324783325\n",
      "epoch: 17 trial 7770 training loss: 0.02335096476599574\n",
      "epoch: 17 trial 7771 training loss: 0.009725289652124047\n",
      "epoch: 17 trial 7772 training loss: 0.09819824062287807\n",
      "epoch: 17 trial 7773 training loss: 0.013097375631332397\n",
      "epoch: 17 trial 7774 training loss: 0.021822675596922636\n",
      "epoch: 17 trial 7775 training loss: 0.024480754043906927\n",
      "epoch: 17 trial 7776 training loss: 0.01342708058655262\n",
      "epoch: 17 trial 7777 training loss: 0.028637357521802187\n",
      "epoch: 17 trial 7778 training loss: 0.049092031084001064\n",
      "epoch: 17 trial 7779 training loss: 0.017338821664452553\n",
      "epoch: 17 trial 7780 training loss: 0.06811733357608318\n",
      "epoch: 17 trial 7781 training loss: 0.1499803401529789\n",
      "epoch: 17 trial 7782 training loss: 0.30441950261592865\n",
      "epoch: 17 trial 7783 training loss: 0.04414764791727066\n",
      "epoch: 17 trial 7784 training loss: 0.0841270163655281\n",
      "epoch: 17 trial 7785 training loss: 0.09938740357756615\n",
      "epoch: 17 trial 7786 training loss: 0.010473123984411359\n",
      "epoch: 17 trial 7787 training loss: 0.08196760714054108\n",
      "epoch: 17 trial 7788 training loss: 0.03655185550451279\n",
      "epoch: 17 trial 7789 training loss: 0.027650964446365833\n",
      "epoch: 17 trial 7790 training loss: 0.053116077557206154\n",
      "epoch: 17 trial 7791 training loss: 0.01206244365312159\n",
      "epoch: 17 trial 7792 training loss: 0.027931914664804935\n",
      "epoch: 17 trial 7793 training loss: 0.01593163749203086\n",
      "epoch: 17 trial 7794 training loss: 0.09720736742019653\n",
      "epoch: 17 trial 7795 training loss: 0.0400581331923604\n",
      "epoch: 17 trial 7796 training loss: 0.11864851042628288\n",
      "epoch: 17 trial 7797 training loss: 0.033533490262925625\n",
      "epoch: 17 trial 7798 training loss: 0.04216049239039421\n",
      "epoch: 17 trial 7799 training loss: 0.036662851460278034\n",
      "epoch: 17 trial 7800 training loss: 0.057692697271704674\n",
      "epoch: 17 trial 7801 training loss: 0.026855293661355972\n",
      "epoch: 17 trial 7802 training loss: 0.017616398632526398\n",
      "epoch: 17 trial 7803 training loss: 0.028706722892820835\n",
      "epoch: 17 trial 7804 training loss: 0.08550330810248852\n",
      "epoch: 17 trial 7805 training loss: 0.024955937638878822\n",
      "epoch: 17 trial 7806 training loss: 0.12413479015231133\n",
      "epoch: 17 trial 7807 training loss: 0.019215122796595097\n",
      "epoch: 17 trial 7808 training loss: 0.002849572221748531\n",
      "epoch: 17 trial 7809 training loss: 0.04986594431102276\n",
      "epoch: 17 trial 7810 training loss: 0.05197349190711975\n",
      "epoch: 17 trial 7811 training loss: 0.021931568160653114\n",
      "epoch: 17 trial 7812 training loss: 0.021118896082043648\n",
      "epoch: 17 trial 7813 training loss: 0.05519793927669525\n",
      "epoch: 17 trial 7814 training loss: 0.036204416304826736\n",
      "epoch: 17 trial 7815 training loss: 0.044862980023026466\n",
      "epoch: 17 trial 7816 training loss: 0.034064567647874355\n",
      "epoch: 17 trial 7817 training loss: 0.015688222832977772\n",
      "epoch: 17 trial 7818 training loss: 0.06787029653787613\n",
      "epoch: 17 trial 7819 training loss: 0.014519800897687674\n",
      "epoch: 17 trial 7820 training loss: 0.024662429932504892\n",
      "epoch: 17 trial 7821 training loss: 0.030882236547768116\n",
      "epoch: 17 trial 7822 training loss: 0.0826780367642641\n",
      "epoch: 17 trial 7823 training loss: 0.017032845877110958\n",
      "epoch: 17 trial 7824 training loss: 0.05250238720327616\n",
      "epoch: 17 trial 7825 training loss: 0.030210908968001604\n",
      "epoch: 17 trial 7826 training loss: 0.0453032273799181\n",
      "epoch: 17 trial 7827 training loss: 0.04687135573476553\n",
      "epoch: 17 trial 7828 training loss: 0.024289931636303663\n",
      "epoch: 17 trial 7829 training loss: 0.06808011420071125\n",
      "epoch: 17 trial 7830 training loss: 0.015963878482580185\n",
      "epoch: 17 trial 7831 training loss: 0.03046676190569997\n",
      "epoch: 17 trial 7832 training loss: 0.05643084738403559\n",
      "epoch: 17 trial 7833 training loss: 0.05510449968278408\n",
      "epoch: 17 trial 7834 training loss: 0.006249738158658147\n",
      "epoch: 17 trial 7835 training loss: 0.04026713874191046\n",
      "epoch: 17 trial 7836 training loss: 0.024498401675373316\n",
      "epoch: 17 trial 7837 training loss: 0.023824211210012436\n",
      "epoch: 17 trial 7838 training loss: 0.06690226681530476\n",
      "epoch: 17 trial 7839 training loss: 0.04777775052934885\n",
      "epoch: 17 trial 7840 training loss: 0.056457292288541794\n",
      "epoch: 17 trial 7841 training loss: 0.009864631341770291\n",
      "epoch: 17 trial 7842 training loss: 0.0844108983874321\n",
      "epoch: 17 trial 7843 training loss: 0.024594414979219437\n",
      "epoch: 17 trial 7844 training loss: 0.045162525959312916\n",
      "epoch: 17 trial 7845 training loss: 0.02863149158656597\n",
      "epoch: 17 trial 7846 training loss: 0.09237618185579777\n",
      "epoch: 17 trial 7847 training loss: 0.04796821344643831\n",
      "epoch: 17 trial 7848 training loss: 0.029369802214205265\n",
      "epoch: 17 trial 7849 training loss: 0.2283063717186451\n",
      "epoch: 17 trial 7850 training loss: 0.06720878183841705\n",
      "epoch: 17 trial 7851 training loss: 0.028306562453508377\n",
      "epoch: 17 trial 7852 training loss: 0.16634803265333176\n",
      "epoch: 17 trial 7853 training loss: 0.07829049043357372\n",
      "epoch: 17 trial 7854 training loss: 0.024688855279237032\n",
      "epoch: 17 trial 7855 training loss: 0.026034747250378132\n",
      "epoch: 17 trial 7856 training loss: 0.024201575201004744\n",
      "epoch: 17 trial 7857 training loss: 0.01410102704539895\n",
      "epoch: 17 trial 7858 training loss: 0.009230646071955562\n",
      "epoch: 17 trial 7859 training loss: 0.06064463034272194\n",
      "epoch: 17 trial 7860 training loss: 0.01857110345736146\n",
      "epoch: 17 trial 7861 training loss: 0.023199223447591066\n",
      "epoch: 17 trial 7862 training loss: 0.03857030253857374\n",
      "epoch: 17 trial 7863 training loss: 0.03139777109026909\n",
      "epoch: 17 trial 7864 training loss: 0.030753837898373604\n",
      "epoch: 17 trial 7865 training loss: 0.025306835770606995\n",
      "epoch: 17 trial 7866 training loss: 0.035475026816129684\n",
      "epoch: 17 trial 7867 training loss: 0.07772867009043694\n",
      "epoch: 17 trial 7868 training loss: 0.03737493418157101\n",
      "epoch: 17 trial 7869 training loss: 0.009270326234400272\n",
      "epoch: 17 trial 7870 training loss: 0.022131072357296944\n",
      "epoch: 17 trial 7871 training loss: 0.02078331122174859\n",
      "epoch: 17 trial 7872 training loss: 0.025487548671662807\n",
      "epoch: 17 trial 7873 training loss: 0.14123866334557533\n",
      "epoch: 17 trial 7874 training loss: 0.02750921342521906\n",
      "epoch: 17 trial 7875 training loss: 0.0486953929066658\n",
      "epoch: 17 trial 7876 training loss: 0.02762050088495016\n",
      "epoch: 17 trial 7877 training loss: 0.033142946660518646\n",
      "epoch: 17 trial 7878 training loss: 0.04729640111327171\n",
      "epoch: 17 trial 7879 training loss: 0.00990637275390327\n",
      "epoch: 17 trial 7880 training loss: 0.04083499126136303\n",
      "epoch: 17 trial 7881 training loss: 0.015913265524432063\n",
      "epoch: 17 trial 7882 training loss: 0.11813007667660713\n",
      "epoch: 17 trial 7883 training loss: 0.021514483261853456\n",
      "epoch: 17 trial 7884 training loss: 0.06522238813340664\n",
      "epoch: 17 trial 7885 training loss: 0.031342312693595886\n",
      "epoch: 17 trial 7886 training loss: 0.03169951029121876\n",
      "epoch: 17 trial 7887 training loss: 0.025198074523359537\n",
      "epoch: 17 trial 7888 training loss: 0.11732952482998371\n",
      "epoch: 17 trial 7889 training loss: 0.0091203220654279\n",
      "epoch: 17 trial 7890 training loss: 0.014361186884343624\n",
      "epoch: 17 trial 7891 training loss: 0.08164808712899685\n",
      "epoch: 17 trial 7892 training loss: 0.01977877924218774\n",
      "epoch: 17 trial 7893 training loss: 0.030311712995171547\n",
      "epoch: 17 trial 7894 training loss: 0.050702339969575405\n",
      "epoch: 17 trial 7895 training loss: 0.02655387483537197\n",
      "epoch: 17 trial 7896 training loss: 0.0201506819576025\n",
      "epoch: 17 trial 7897 training loss: 0.01579323038458824\n",
      "epoch: 17 trial 7898 training loss: 0.0382651137188077\n",
      "epoch: 17 trial 7899 training loss: 0.012068613548763096\n",
      "epoch: 17 trial 7900 training loss: 0.00959687796421349\n",
      "epoch: 17 trial 7901 training loss: 0.02857254259288311\n",
      "epoch: 17 trial 7902 training loss: 0.004268703167326748\n",
      "epoch: 17 trial 7903 training loss: 0.011099723167717457\n",
      "epoch: 17 trial 7904 training loss: 0.01666331454180181\n",
      "epoch: 17 trial 7905 training loss: 0.010160289471969008\n",
      "epoch: 17 trial 7906 training loss: 0.05841704551130533\n",
      "epoch: 17 trial 7907 training loss: 0.004298158455640078\n",
      "epoch: 17 trial 7908 training loss: 0.05182650685310364\n",
      "epoch: 17 trial 7909 training loss: 0.011855204356834292\n",
      "epoch: 17 trial 7910 training loss: 0.021318465005606413\n",
      "epoch: 17 trial 7911 training loss: 0.026357199531048536\n",
      "epoch: 17 trial 7912 training loss: 0.012803171295672655\n",
      "epoch: 17 trial 7913 training loss: 0.03622536268085241\n",
      "epoch: 17 trial 7914 training loss: 0.11529054492712021\n",
      "epoch: 17 trial 7915 training loss: 0.04841872677206993\n",
      "epoch: 17 trial 7916 training loss: 0.009030762012116611\n",
      "epoch: 17 trial 7917 training loss: 0.06524044834077358\n",
      "epoch: 17 trial 7918 training loss: 0.03289027884602547\n",
      "epoch: 17 trial 7919 training loss: 0.024567774962633848\n",
      "epoch: 17 trial 7920 training loss: 0.017411622684448957\n",
      "epoch: 17 trial 7921 training loss: 0.025151893496513367\n",
      "epoch: 17 trial 7922 training loss: 0.035739392042160034\n",
      "epoch: 17 trial 7923 training loss: 0.04009703826159239\n",
      "epoch: 17 trial 7924 training loss: 0.035387324169278145\n",
      "epoch: 17 trial 7925 training loss: 0.07679177448153496\n",
      "epoch: 17 trial 7926 training loss: 0.008407217916101217\n",
      "epoch: 17 trial 7927 training loss: 0.04970623925328255\n",
      "epoch: 17 trial 7928 training loss: 0.017721302807331085\n",
      "epoch: 17 trial 7929 training loss: 0.01843264326453209\n",
      "epoch: 17 trial 7930 training loss: 0.044009474106132984\n",
      "epoch: 17 trial 7931 training loss: 0.05445030517876148\n",
      "epoch: 17 trial 7932 training loss: 0.02012552274391055\n",
      "epoch: 17 trial 7933 training loss: 0.043364234268665314\n",
      "epoch: 17 trial 7934 training loss: 0.06712692603468895\n",
      "epoch: 17 trial 7935 training loss: 0.07537228055298328\n",
      "epoch: 17 trial 7936 training loss: 0.021336093079298735\n",
      "epoch: 17 trial 7937 training loss: 0.024315956979990005\n",
      "epoch: 17 trial 7938 training loss: 0.013360322220250964\n",
      "epoch: 17 trial 7939 training loss: 0.023285008035600185\n",
      "epoch: 17 trial 7940 training loss: 0.03422487247735262\n",
      "epoch: 17 trial 7941 training loss: 0.028544959612190723\n",
      "epoch: 17 trial 7942 training loss: 0.06571229174733162\n",
      "epoch: 17 trial 7943 training loss: 0.11461234465241432\n",
      "epoch: 17 trial 7944 training loss: 0.014822845812886953\n",
      "epoch: 17 trial 7945 training loss: 0.005906583624891937\n",
      "epoch: 17 trial 7946 training loss: 0.00867596291936934\n",
      "epoch: 17 trial 7947 training loss: 0.009663519682362676\n",
      "epoch: 17 trial 7948 training loss: 0.01600093860179186\n",
      "epoch: 17 trial 7949 training loss: 0.020614364184439182\n",
      "epoch: 17 trial 7950 training loss: 0.09029958210885525\n",
      "epoch: 17 trial 7951 training loss: 0.03649823181331158\n",
      "epoch: 17 trial 7952 training loss: 0.016059658490121365\n",
      "epoch: 17 trial 7953 training loss: 0.10063457302749157\n",
      "epoch: 17 trial 7954 training loss: 0.04600225109606981\n",
      "epoch: 17 trial 7955 training loss: 0.0647632498294115\n",
      "epoch: 17 trial 7956 training loss: 0.0643561901524663\n",
      "epoch: 17 trial 7957 training loss: 0.10345079191029072\n",
      "epoch: 17 trial 7958 training loss: 0.013129610335454345\n",
      "epoch: 17 trial 7959 training loss: 0.007467710878700018\n",
      "epoch: 17 trial 7960 training loss: 0.024595864582806826\n",
      "epoch: 17 trial 7961 training loss: 0.022490762174129486\n",
      "epoch: 17 trial 7962 training loss: 0.020735132973641157\n",
      "epoch: 17 trial 7963 training loss: 0.05019139498472214\n",
      "epoch: 17 trial 7964 training loss: 0.06916164234280586\n",
      "epoch: 17 trial 7965 training loss: 0.03345371689647436\n",
      "epoch: 17 trial 7966 training loss: 0.04785425774753094\n",
      "epoch: 17 trial 7967 training loss: 0.03624146059155464\n",
      "epoch: 17 trial 7968 training loss: 0.09156171046197414\n",
      "epoch: 17 trial 7969 training loss: 0.012169203953817487\n",
      "epoch: 17 trial 7970 training loss: 0.011621342040598392\n",
      "epoch: 17 trial 7971 training loss: 0.026279457844793797\n",
      "epoch: 17 trial 7972 training loss: 0.027658211067318916\n",
      "epoch: 17 trial 7973 training loss: 0.04666333273053169\n",
      "epoch: 17 trial 7974 training loss: 0.014459904283285141\n",
      "epoch: 17 trial 7975 training loss: 0.16604795306921005\n",
      "epoch: 17 trial 7976 training loss: 0.02447029808536172\n",
      "epoch: 17 trial 7977 training loss: 0.10784142836928368\n",
      "epoch: 17 trial 7978 training loss: 0.09457477740943432\n",
      "epoch: 17 trial 7979 training loss: 0.0144301219843328\n",
      "epoch: 17 trial 7980 training loss: 0.06258445605635643\n",
      "epoch: 17 trial 7981 training loss: 0.01555473543703556\n",
      "epoch: 17 trial 7982 training loss: 0.0435410076752305\n",
      "epoch: 17 trial 7983 training loss: 0.044494813308119774\n",
      "epoch: 17 trial 7984 training loss: 0.08554188162088394\n",
      "epoch: 17 trial 7985 training loss: 0.015487854834645987\n",
      "epoch: 17 trial 7986 training loss: 0.04033242352306843\n",
      "epoch: 17 trial 7987 training loss: 0.032140725292265415\n",
      "epoch: 17 trial 7988 training loss: 0.028096353635191917\n",
      "epoch: 17 trial 7989 training loss: 0.056041755713522434\n",
      "epoch: 17 trial 7990 training loss: 0.018258180934935808\n",
      "epoch: 17 trial 7991 training loss: 0.04632260091602802\n",
      "epoch: 17 trial 7992 training loss: 0.022473859135061502\n",
      "epoch: 17 trial 7993 training loss: 0.023922580759972334\n",
      "epoch: 17 trial 7994 training loss: 0.041604526340961456\n",
      "epoch: 17 trial 7995 training loss: 0.012167448876425624\n",
      "epoch: 17 trial 7996 training loss: 0.005610876833088696\n",
      "epoch: 17 trial 7997 training loss: 0.03428294509649277\n",
      "epoch: 17 trial 7998 training loss: 0.09520634822547436\n",
      "epoch: 17 trial 7999 training loss: 0.020254591014236212\n",
      "epoch: 17 trial 8000 training loss: 0.08715720474720001\n",
      "epoch: 17 trial 8001 training loss: 0.04783677775412798\n",
      "epoch: 17 trial 8002 training loss: 0.026622955687344074\n",
      "epoch: 17 trial 8003 training loss: 0.15561452135443687\n",
      "epoch: 17 trial 8004 training loss: 0.012031798018142581\n",
      "epoch: 17 trial 8005 training loss: 0.013910634443163872\n",
      "epoch: 17 trial 8006 training loss: 0.010785578982904553\n",
      "epoch: 17 trial 8007 training loss: 0.003841463476419449\n",
      "epoch: 17 trial 8008 training loss: 0.029940517619252205\n",
      "epoch: 17 trial 8009 training loss: 0.037129937671124935\n",
      "epoch: 17 trial 8010 training loss: 0.05071128532290459\n",
      "epoch: 17 trial 8011 training loss: 0.029363362584263086\n",
      "epoch: 17 trial 8012 training loss: 0.04290612414479256\n",
      "epoch: 17 trial 8013 training loss: 0.020456667058169842\n",
      "epoch: 17 trial 8014 training loss: 0.07441612891852856\n",
      "epoch: 17 trial 8015 training loss: 0.016479717567563057\n",
      "epoch: 17 trial 8016 training loss: 0.006731521454639733\n",
      "epoch: 17 trial 8017 training loss: 0.023283544462174177\n",
      "epoch: 17 trial 8018 training loss: 0.0063997660763561726\n",
      "epoch: 17 trial 8019 training loss: 0.07049121893942356\n",
      "epoch: 17 trial 8020 training loss: 0.004177578492090106\n",
      "epoch: 17 trial 8021 training loss: 0.02357705496251583\n",
      "epoch: 17 trial 8022 training loss: 0.014254194218665361\n",
      "epoch: 17 trial 8023 training loss: 0.06597454659640789\n",
      "epoch: 17 trial 8024 training loss: 0.038890489377081394\n",
      "epoch: 17 trial 8025 training loss: 0.022407162468880415\n",
      "epoch: 17 trial 8026 training loss: 0.039137174375355244\n",
      "epoch: 17 trial 8027 training loss: 0.062230322510004044\n",
      "epoch: 17 trial 8028 training loss: 0.0278201038017869\n",
      "epoch: 17 trial 8029 training loss: 0.06820434890687466\n",
      "epoch: 17 trial 8030 training loss: 0.017489860765635967\n",
      "epoch: 17 trial 8031 training loss: 0.07133394852280617\n",
      "epoch: 17 trial 8032 training loss: 0.01878140354529023\n",
      "epoch: 17 trial 8033 training loss: 0.008246945217251778\n",
      "epoch: 17 trial 8034 training loss: 0.04371284507215023\n",
      "epoch: 17 trial 8035 training loss: 0.028316307812929153\n",
      "epoch: 17 trial 8036 training loss: 0.016301750438287854\n",
      "epoch: 17 trial 8037 training loss: 0.018595909234136343\n",
      "epoch: 17 trial 8038 training loss: 0.010837947949767113\n",
      "epoch: 17 trial 8039 training loss: 0.020675952546298504\n",
      "epoch: 17 trial 8040 training loss: 0.03135712957009673\n",
      "epoch: 17 trial 8041 training loss: 0.017059639561921358\n",
      "epoch: 17 trial 8042 training loss: 0.03174459747970104\n",
      "epoch: 17 trial 8043 training loss: 0.08680044300854206\n",
      "epoch: 17 trial 8044 training loss: 0.036254389211535454\n",
      "epoch: 17 trial 8045 training loss: 0.013700596522539854\n",
      "epoch: 17 trial 8046 training loss: 0.004983188351616263\n",
      "epoch: 17 trial 8047 training loss: 0.0075915774796158075\n",
      "epoch: 17 trial 8048 training loss: 0.029534650966525078\n",
      "epoch: 17 trial 8049 training loss: 0.05073316115885973\n",
      "epoch: 17 trial 8050 training loss: 0.04132627882063389\n",
      "epoch: 17 trial 8051 training loss: 0.003950215643271804\n",
      "epoch: 17 trial 8052 training loss: 0.03484392911195755\n",
      "epoch: 17 trial 8053 training loss: 0.03260324103757739\n",
      "epoch: 17 trial 8054 training loss: 0.023109749890863895\n",
      "epoch: 17 trial 8055 training loss: 0.017160932067781687\n",
      "epoch: 17 trial 8056 training loss: 0.018583155469968915\n",
      "epoch: 17 trial 8057 training loss: 0.03734382800757885\n",
      "epoch: 17 trial 8058 training loss: 0.02423630701377988\n",
      "epoch: 17 trial 8059 training loss: 0.03574309218674898\n",
      "epoch: 17 trial 8060 training loss: 0.07690468430519104\n",
      "epoch: 17 trial 8061 training loss: 0.025666966568678617\n",
      "epoch: 17 trial 8062 training loss: 0.034874310716986656\n",
      "epoch: 17 trial 8063 training loss: 0.0683120172470808\n",
      "epoch: 17 trial 8064 training loss: 0.02903304109349847\n",
      "epoch: 17 trial 8065 training loss: 0.062033191323280334\n",
      "epoch: 17 trial 8066 training loss: 0.017873127944767475\n",
      "epoch: 17 trial 8067 training loss: 0.036135854199528694\n",
      "epoch: 17 trial 8068 training loss: 0.026704619638621807\n",
      "epoch: 17 trial 8069 training loss: 0.02325434936210513\n",
      "epoch: 17 trial 8070 training loss: 0.026609453838318586\n",
      "epoch: 17 trial 8071 training loss: 0.09034503251314163\n",
      "epoch: 17 trial 8072 training loss: 0.013034863863140345\n",
      "epoch: 17 trial 8073 training loss: 0.08356412686407566\n",
      "epoch: 17 trial 8074 training loss: 0.027840156108140945\n",
      "epoch: 17 trial 8075 training loss: 0.00708043435588479\n",
      "epoch: 17 trial 8076 training loss: 0.04733623191714287\n",
      "epoch: 17 trial 8077 training loss: 0.06060880608856678\n",
      "epoch: 17 trial 8078 training loss: 0.011414810083806515\n",
      "epoch: 17 trial 8079 training loss: 0.03455397114157677\n",
      "epoch: 17 trial 8080 training loss: 0.009112327126786113\n",
      "epoch: 17 trial 8081 training loss: 0.03316904790699482\n",
      "epoch: 17 trial 8082 training loss: 0.021692152600735426\n",
      "epoch: 17 trial 8083 training loss: 0.037131710909307\n",
      "epoch: 17 trial 8084 training loss: 0.2763412296772003\n",
      "epoch: 17 trial 8085 training loss: 0.05433298088610172\n",
      "epoch: 17 trial 8086 training loss: 0.008745325030758977\n",
      "epoch: 17 trial 8087 training loss: 0.01687548216432333\n",
      "epoch: 17 trial 8088 training loss: 0.13432691991329193\n",
      "epoch: 17 trial 8089 training loss: 0.04659292660653591\n",
      "epoch: 17 trial 8090 training loss: 0.0766344740986824\n",
      "epoch: 17 trial 8091 training loss: 0.04835349693894386\n",
      "epoch: 17 trial 8092 training loss: 0.03159630112349987\n",
      "epoch: 17 trial 8093 training loss: 0.011940442491322756\n",
      "epoch: 17 trial 8094 training loss: 0.07465193048119545\n",
      "epoch: 17 trial 8095 training loss: 0.029888211749494076\n",
      "epoch: 17 trial 8096 training loss: 0.0803064126521349\n",
      "epoch: 17 trial 8097 training loss: 0.012427089968696237\n",
      "epoch: 17 trial 8098 training loss: 0.005633447668515146\n",
      "epoch: 17 trial 8099 training loss: 0.023369827773422003\n",
      "epoch: 17 trial 8100 training loss: 0.006202450953423977\n",
      "epoch: 17 trial 8101 training loss: 0.01336963614448905\n",
      "epoch: 17 trial 8102 training loss: 0.10212471708655357\n",
      "epoch: 17 trial 8103 training loss: 0.07452665641903877\n",
      "epoch: 17 trial 8104 training loss: 0.025410020723938942\n",
      "epoch: 17 trial 8105 training loss: 0.10632966086268425\n",
      "epoch: 17 trial 8106 training loss: 0.09915408678352833\n",
      "epoch: 17 trial 8107 training loss: 0.021750756073743105\n",
      "epoch: 17 trial 8108 training loss: 0.05939837731420994\n",
      "epoch: 17 trial 8109 training loss: 0.0334189310669899\n",
      "epoch: 17 trial 8110 training loss: 0.011060806922614574\n",
      "epoch: 17 trial 8111 training loss: 0.005440558306872845\n",
      "epoch: 17 trial 8112 training loss: 0.12740163877606392\n",
      "epoch: 17 trial 8113 training loss: 0.06407285295426846\n",
      "epoch: 17 trial 8114 training loss: 0.04929759819060564\n",
      "epoch: 17 trial 8115 training loss: 0.056550679728388786\n",
      "epoch: 17 trial 8116 training loss: 0.02360470127314329\n",
      "epoch: 17 trial 8117 training loss: 0.03725878335535526\n",
      "epoch: 17 trial 8118 training loss: 0.015481420792639256\n",
      "epoch: 17 trial 8119 training loss: 0.04646087437868118\n",
      "epoch: 17 trial 8120 training loss: 0.025760439690202475\n",
      "epoch: 17 trial 8121 training loss: 0.03757581114768982\n",
      "epoch: 17 trial 8122 training loss: 0.08216607198119164\n",
      "epoch: 17 trial 8123 training loss: 0.022113872691988945\n",
      "epoch: 17 trial 8124 training loss: 0.010547626530751586\n",
      "epoch: 17 trial 8125 training loss: 0.030561585910618305\n",
      "epoch: 17 trial 8126 training loss: 0.004600429791025817\n",
      "epoch: 17 trial 8127 training loss: 0.03641748521476984\n",
      "epoch: 17 trial 8128 training loss: 0.040837787091732025\n",
      "epoch: 17 trial 8129 training loss: 0.011542401975020766\n",
      "epoch: 17 trial 8130 training loss: 0.002356826385948807\n",
      "epoch: 17 trial 8131 training loss: 0.0038264610338956118\n",
      "epoch: 17 trial 8132 training loss: 0.01809357339516282\n",
      "epoch: 17 trial 8133 training loss: 0.0366144273430109\n",
      "epoch: 17 trial 8134 training loss: 0.004235610133036971\n",
      "epoch: 17 trial 8135 training loss: 0.004212922067381442\n",
      "epoch: 17 trial 8136 training loss: 0.006101400009356439\n",
      "epoch: 17 trial 8137 training loss: 0.00900976313278079\n",
      "epoch: 17 trial 8138 training loss: 0.01135409134440124\n",
      "epoch: 17 trial 8139 training loss: 0.024126532953232527\n",
      "epoch: 17 trial 8140 training loss: 0.1120113842189312\n",
      "epoch: 17 trial 8141 training loss: 0.01980834361165762\n",
      "epoch: 17 trial 8142 training loss: 0.018629466649144888\n",
      "epoch: 17 trial 8143 training loss: 0.027552716434001923\n",
      "epoch: 17 trial 8144 training loss: 0.0384806664660573\n",
      "epoch: 17 trial 8145 training loss: 0.033326452132314444\n",
      "epoch: 17 trial 8146 training loss: 0.030550379771739244\n",
      "epoch: 17 trial 8147 training loss: 0.041654810309410095\n",
      "epoch: 17 trial 8148 training loss: 0.08177317678928375\n",
      "epoch: 17 trial 8149 training loss: 0.11434541456401348\n",
      "epoch: 17 trial 8150 training loss: 0.021795482840389013\n",
      "epoch: 17 trial 8151 training loss: 0.023117760196328163\n",
      "epoch: 17 trial 8152 training loss: 0.08588502183556557\n",
      "epoch: 17 trial 8153 training loss: 0.017729201819747686\n",
      "epoch: 17 trial 8154 training loss: 0.05813973397016525\n",
      "epoch: 17 trial 8155 training loss: 0.09492424316704273\n",
      "epoch: 17 trial 8156 training loss: 0.09484361670911312\n",
      "epoch: 17 trial 8157 training loss: 0.040883291978389025\n",
      "epoch: 17 trial 8158 training loss: 0.1101011410355568\n",
      "epoch: 17 trial 8159 training loss: 0.021047860383987427\n",
      "epoch: 17 trial 8160 training loss: 0.04891588445752859\n",
      "epoch: 17 trial 8161 training loss: 0.032152789644896984\n",
      "epoch: 17 trial 8162 training loss: 0.09519206546247005\n",
      "epoch: 17 trial 8163 training loss: 0.0079556277487427\n",
      "epoch: 17 trial 8164 training loss: 0.007240551873110235\n",
      "epoch: 17 trial 8165 training loss: 0.019495800137519836\n",
      "epoch: 17 trial 8166 training loss: 0.028478363528847694\n",
      "epoch: 17 trial 8167 training loss: 0.052207233384251595\n",
      "epoch: 17 trial 8168 training loss: 0.05214723199605942\n",
      "epoch: 17 trial 8169 training loss: 0.015137170907109976\n",
      "epoch: 17 trial 8170 training loss: 0.051854535937309265\n",
      "epoch: 17 trial 8171 training loss: 0.0167803056538105\n",
      "epoch: 17 trial 8172 training loss: 0.01560294022783637\n",
      "epoch: 17 trial 8173 training loss: 0.020333264488726854\n",
      "epoch: 17 trial 8174 training loss: 0.01224715425632894\n",
      "epoch: 17 trial 8175 training loss: 0.018961528781801462\n",
      "epoch: 17 trial 8176 training loss: 0.03969362657517195\n",
      "epoch: 17 trial 8177 training loss: 0.028144807554781437\n",
      "epoch: 17 trial 8178 training loss: 0.02359021734446287\n",
      "epoch: 17 trial 8179 training loss: 0.0218274868093431\n",
      "epoch: 17 trial 8180 training loss: 0.016651614103466272\n",
      "epoch: 17 trial 8181 training loss: 0.005031163454987109\n",
      "epoch: 17 trial 8182 training loss: 0.012296901550143957\n",
      "epoch: 17 trial 8183 training loss: 0.015982844401150942\n",
      "epoch: 17 trial 8184 training loss: 0.013744227355346084\n",
      "epoch: 17 trial 8185 training loss: 0.006115752970799804\n",
      "epoch: 17 trial 8186 training loss: 0.01191806560382247\n",
      "epoch: 17 trial 8187 training loss: 0.09305381402373314\n",
      "epoch: 17 trial 8188 training loss: 0.010747938184067607\n",
      "epoch: 17 trial 8189 training loss: 0.03260903246700764\n",
      "epoch: 17 trial 8190 training loss: 0.013325097737833858\n",
      "epoch: 17 trial 8191 training loss: 0.04835030436515808\n",
      "epoch: 17 trial 8192 training loss: 0.041600603610277176\n",
      "epoch: 17 trial 8193 training loss: 0.03750998340547085\n",
      "epoch: 17 trial 8194 training loss: 0.02627965807914734\n",
      "epoch: 17 trial 8195 training loss: 0.018303091172128916\n",
      "epoch: 17 trial 8196 training loss: 0.1002612579613924\n",
      "epoch: 17 trial 8197 training loss: 0.006179264863021672\n",
      "epoch: 17 trial 8198 training loss: 0.018523070495575666\n",
      "epoch: 17 trial 8199 training loss: 0.13084114342927933\n",
      "epoch: 17 trial 8200 training loss: 0.026897828094661236\n",
      "epoch: 17 trial 8201 training loss: 0.021817491855472326\n",
      "epoch: 17 trial 8202 training loss: 0.01772113936021924\n",
      "epoch: 17 trial 8203 training loss: 0.013374543283134699\n",
      "epoch: 17 trial 8204 training loss: 0.02642858074977994\n",
      "epoch: 17 trial 8205 training loss: 0.01313250744715333\n",
      "epoch: 17 trial 8206 training loss: 0.055621933192014694\n",
      "epoch: 17 trial 8207 training loss: 0.14937865734100342\n",
      "epoch: 17 trial 8208 training loss: 0.06845958344638348\n",
      "epoch: 17 trial 8209 training loss: 0.027645593509078026\n",
      "epoch: 17 trial 8210 training loss: 0.06752670928835869\n",
      "epoch: 17 trial 8211 training loss: 0.10094000399112701\n",
      "epoch: 17 trial 8212 training loss: 0.025756369344890118\n",
      "epoch: 17 trial 8213 training loss: 0.0702772494405508\n",
      "epoch: 17 trial 8214 training loss: 0.007106337929144502\n",
      "epoch: 17 trial 8215 training loss: 0.04107207711786032\n",
      "epoch: 17 trial 8216 training loss: 0.012683901470154524\n",
      "epoch: 17 trial 8217 training loss: 0.027321575209498405\n",
      "epoch: 17 trial 8218 training loss: 0.109652079641819\n",
      "epoch: 17 trial 8219 training loss: 0.04543771408498287\n",
      "epoch: 17 trial 8220 training loss: 0.019027774687856436\n",
      "epoch: 17 trial 8221 training loss: 0.02129834285005927\n",
      "epoch: 17 trial 8222 training loss: 0.07865653187036514\n",
      "epoch: 17 trial 8223 training loss: 0.013573321048170328\n",
      "epoch: 17 trial 8224 training loss: 0.05622890032827854\n",
      "epoch: 17 trial 8225 training loss: 0.03493359126150608\n",
      "epoch: 17 trial 8226 training loss: 0.045024050399661064\n",
      "epoch: 17 trial 8227 training loss: 0.013546440051868558\n",
      "epoch: 17 trial 8228 training loss: 0.05760054290294647\n",
      "epoch: 18 trial 8229 training loss: 0.013862721156328917\n",
      "epoch: 18 trial 8230 training loss: 0.013624421786516905\n",
      "epoch: 18 trial 8231 training loss: 0.012394006829708815\n",
      "epoch: 18 trial 8232 training loss: 0.0628683790564537\n",
      "epoch: 18 trial 8233 training loss: 0.004595077713020146\n",
      "epoch: 18 trial 8234 training loss: 0.005176767590455711\n",
      "epoch: 18 trial 8235 training loss: 0.02652901690453291\n",
      "epoch: 18 trial 8236 training loss: 0.03067935351282358\n",
      "epoch: 18 trial 8237 training loss: 0.006260581663809717\n",
      "epoch: 18 trial 8238 training loss: 0.013218371663242579\n",
      "epoch: 18 trial 8239 training loss: 0.05660532508045435\n",
      "epoch: 18 trial 8240 training loss: 0.033354305662214756\n",
      "epoch: 18 trial 8241 training loss: 0.016603710129857063\n",
      "epoch: 18 trial 8242 training loss: 0.018995750229805708\n",
      "epoch: 18 trial 8243 training loss: 0.02256639115512371\n",
      "epoch: 18 trial 8244 training loss: 0.022733244113624096\n",
      "epoch: 18 trial 8245 training loss: 0.111235611140728\n",
      "epoch: 18 trial 8246 training loss: 0.05179530754685402\n",
      "epoch: 18 trial 8247 training loss: 0.015951301902532578\n",
      "epoch: 18 trial 8248 training loss: 0.01995609560981393\n",
      "epoch: 18 trial 8249 training loss: 0.0172158177010715\n",
      "epoch: 18 trial 8250 training loss: 0.07754179462790489\n",
      "epoch: 18 trial 8251 training loss: 0.013301218627020717\n",
      "epoch: 18 trial 8252 training loss: 0.0035042515955865383\n",
      "epoch: 18 trial 8253 training loss: 0.12586568295955658\n",
      "epoch: 18 trial 8254 training loss: 0.027198432479053736\n",
      "epoch: 18 trial 8255 training loss: 0.010864832904189825\n",
      "epoch: 18 trial 8256 training loss: 0.09497538954019547\n",
      "epoch: 18 trial 8257 training loss: 0.010452450369484723\n",
      "epoch: 18 trial 8258 training loss: 0.020874623209238052\n",
      "epoch: 18 trial 8259 training loss: 0.02424888825044036\n",
      "epoch: 18 trial 8260 training loss: 0.01359073119238019\n",
      "epoch: 18 trial 8261 training loss: 0.028866835869848728\n",
      "epoch: 18 trial 8262 training loss: 0.04291671421378851\n",
      "epoch: 18 trial 8263 training loss: 0.017521020490676165\n",
      "epoch: 18 trial 8264 training loss: 0.05789891071617603\n",
      "epoch: 18 trial 8265 training loss: 0.15788980573415756\n",
      "epoch: 18 trial 8266 training loss: 0.26528962329030037\n",
      "epoch: 18 trial 8267 training loss: 0.04425117094069719\n",
      "epoch: 18 trial 8268 training loss: 0.08094977028667927\n",
      "epoch: 18 trial 8269 training loss: 0.09460524097084999\n",
      "epoch: 18 trial 8270 training loss: 0.011299655539914966\n",
      "epoch: 18 trial 8271 training loss: 0.08452804014086723\n",
      "epoch: 18 trial 8272 training loss: 0.03394712833687663\n",
      "epoch: 18 trial 8273 training loss: 0.02810467081144452\n",
      "epoch: 18 trial 8274 training loss: 0.05833272263407707\n",
      "epoch: 18 trial 8275 training loss: 0.010815878631547093\n",
      "epoch: 18 trial 8276 training loss: 0.029873923398554325\n",
      "epoch: 18 trial 8277 training loss: 0.01548399543389678\n",
      "epoch: 18 trial 8278 training loss: 0.09813749976456165\n",
      "epoch: 18 trial 8279 training loss: 0.03904543537646532\n",
      "epoch: 18 trial 8280 training loss: 0.11508556269109249\n",
      "epoch: 18 trial 8281 training loss: 0.03841053228825331\n",
      "epoch: 18 trial 8282 training loss: 0.04392963647842407\n",
      "epoch: 18 trial 8283 training loss: 0.0329241007566452\n",
      "epoch: 18 trial 8284 training loss: 0.061995742842555046\n",
      "epoch: 18 trial 8285 training loss: 0.0261768801137805\n",
      "epoch: 18 trial 8286 training loss: 0.016700088512152433\n",
      "epoch: 18 trial 8287 training loss: 0.02836601948365569\n",
      "epoch: 18 trial 8288 training loss: 0.08633074909448624\n",
      "epoch: 18 trial 8289 training loss: 0.024853677954524755\n",
      "epoch: 18 trial 8290 training loss: 0.1284925937652588\n",
      "epoch: 18 trial 8291 training loss: 0.01854567788541317\n",
      "epoch: 18 trial 8292 training loss: 0.003260166908148676\n",
      "epoch: 18 trial 8293 training loss: 0.04952182713896036\n",
      "epoch: 18 trial 8294 training loss: 0.0552015146240592\n",
      "epoch: 18 trial 8295 training loss: 0.022443390917032957\n",
      "epoch: 18 trial 8296 training loss: 0.020052459789440036\n",
      "epoch: 18 trial 8297 training loss: 0.054247960448265076\n",
      "epoch: 18 trial 8298 training loss: 0.037604144774377346\n",
      "epoch: 18 trial 8299 training loss: 0.04283282719552517\n",
      "epoch: 18 trial 8300 training loss: 0.03517008759081364\n",
      "epoch: 18 trial 8301 training loss: 0.01508705597370863\n",
      "epoch: 18 trial 8302 training loss: 0.06613701581954956\n",
      "epoch: 18 trial 8303 training loss: 0.01464925636537373\n",
      "epoch: 18 trial 8304 training loss: 0.02393209096044302\n",
      "epoch: 18 trial 8305 training loss: 0.02872209483757615\n",
      "epoch: 18 trial 8306 training loss: 0.08358832262456417\n",
      "epoch: 18 trial 8307 training loss: 0.01864856481552124\n",
      "epoch: 18 trial 8308 training loss: 0.05472351983189583\n",
      "epoch: 18 trial 8309 training loss: 0.029870691243559122\n",
      "epoch: 18 trial 8310 training loss: 0.04410406015813351\n",
      "epoch: 18 trial 8311 training loss: 0.04712785966694355\n",
      "epoch: 18 trial 8312 training loss: 0.024008520878851414\n",
      "epoch: 18 trial 8313 training loss: 0.06884115561842918\n",
      "epoch: 18 trial 8314 training loss: 0.015433531254529953\n",
      "epoch: 18 trial 8315 training loss: 0.030260820873081684\n",
      "epoch: 18 trial 8316 training loss: 0.058879319578409195\n",
      "epoch: 18 trial 8317 training loss: 0.051368837244808674\n",
      "epoch: 18 trial 8318 training loss: 0.005605558166280389\n",
      "epoch: 18 trial 8319 training loss: 0.03888620715588331\n",
      "epoch: 18 trial 8320 training loss: 0.023803760297596455\n",
      "epoch: 18 trial 8321 training loss: 0.02350158803164959\n",
      "epoch: 18 trial 8322 training loss: 0.06424111314117908\n",
      "epoch: 18 trial 8323 training loss: 0.04655373468995094\n",
      "epoch: 18 trial 8324 training loss: 0.05839885026216507\n",
      "epoch: 18 trial 8325 training loss: 0.009849276160821319\n",
      "epoch: 18 trial 8326 training loss: 0.08240143768489361\n",
      "epoch: 18 trial 8327 training loss: 0.02251060213893652\n",
      "epoch: 18 trial 8328 training loss: 0.048321143724024296\n",
      "epoch: 18 trial 8329 training loss: 0.029735802672803402\n",
      "epoch: 18 trial 8330 training loss: 0.08907044678926468\n",
      "epoch: 18 trial 8331 training loss: 0.04716044012457132\n",
      "epoch: 18 trial 8332 training loss: 0.02950273361057043\n",
      "epoch: 18 trial 8333 training loss: 0.23806973546743393\n",
      "epoch: 18 trial 8334 training loss: 0.0708856638520956\n",
      "epoch: 18 trial 8335 training loss: 0.03204711992293596\n",
      "epoch: 18 trial 8336 training loss: 0.14413793943822384\n",
      "epoch: 18 trial 8337 training loss: 0.08367151953279972\n",
      "epoch: 18 trial 8338 training loss: 0.025122419465333223\n",
      "epoch: 18 trial 8339 training loss: 0.024582738056778908\n",
      "epoch: 18 trial 8340 training loss: 0.022063633892685175\n",
      "epoch: 18 trial 8341 training loss: 0.014262465760111809\n",
      "epoch: 18 trial 8342 training loss: 0.010064194910228252\n",
      "epoch: 18 trial 8343 training loss: 0.05825030244886875\n",
      "epoch: 18 trial 8344 training loss: 0.01847027288749814\n",
      "epoch: 18 trial 8345 training loss: 0.023444401565939188\n",
      "epoch: 18 trial 8346 training loss: 0.04300105478614569\n",
      "epoch: 18 trial 8347 training loss: 0.0311376191675663\n",
      "epoch: 18 trial 8348 training loss: 0.029252368491142988\n",
      "epoch: 18 trial 8349 training loss: 0.023430026601999998\n",
      "epoch: 18 trial 8350 training loss: 0.0357216652482748\n",
      "epoch: 18 trial 8351 training loss: 0.0773013960570097\n",
      "epoch: 18 trial 8352 training loss: 0.03621521405875683\n",
      "epoch: 18 trial 8353 training loss: 0.009422295494005084\n",
      "epoch: 18 trial 8354 training loss: 0.022105125710368156\n",
      "epoch: 18 trial 8355 training loss: 0.023532491642981768\n",
      "epoch: 18 trial 8356 training loss: 0.024045536294579506\n",
      "epoch: 18 trial 8357 training loss: 0.13590997084975243\n",
      "epoch: 18 trial 8358 training loss: 0.02677215449512005\n",
      "epoch: 18 trial 8359 training loss: 0.04191865958273411\n",
      "epoch: 18 trial 8360 training loss: 0.02766715083271265\n",
      "epoch: 18 trial 8361 training loss: 0.03341611009091139\n",
      "epoch: 18 trial 8362 training loss: 0.04385360423475504\n",
      "epoch: 18 trial 8363 training loss: 0.01135789742693305\n",
      "epoch: 18 trial 8364 training loss: 0.04178509674966335\n",
      "epoch: 18 trial 8365 training loss: 0.013919597025960684\n",
      "epoch: 18 trial 8366 training loss: 0.11752002686262131\n",
      "epoch: 18 trial 8367 training loss: 0.02071301080286503\n",
      "epoch: 18 trial 8368 training loss: 0.06363217160105705\n",
      "epoch: 18 trial 8369 training loss: 0.031482525169849396\n",
      "epoch: 18 trial 8370 training loss: 0.03301780018955469\n",
      "epoch: 18 trial 8371 training loss: 0.027678467333316803\n",
      "epoch: 18 trial 8372 training loss: 0.12218580022454262\n",
      "epoch: 18 trial 8373 training loss: 0.009439472574740648\n",
      "epoch: 18 trial 8374 training loss: 0.01593448594212532\n",
      "epoch: 18 trial 8375 training loss: 0.0802515335381031\n",
      "epoch: 18 trial 8376 training loss: 0.0239093080163002\n",
      "epoch: 18 trial 8377 training loss: 0.0305750435218215\n",
      "epoch: 18 trial 8378 training loss: 0.0530448742210865\n",
      "epoch: 18 trial 8379 training loss: 0.027733638882637024\n",
      "epoch: 18 trial 8380 training loss: 0.019801395945250988\n",
      "epoch: 18 trial 8381 training loss: 0.016639737877994776\n",
      "epoch: 18 trial 8382 training loss: 0.03498946502804756\n",
      "epoch: 18 trial 8383 training loss: 0.013040584744885564\n",
      "epoch: 18 trial 8384 training loss: 0.010343964211642742\n",
      "epoch: 18 trial 8385 training loss: 0.031457241624593735\n",
      "epoch: 18 trial 8386 training loss: 0.00422083493322134\n",
      "epoch: 18 trial 8387 training loss: 0.011580877704545856\n",
      "epoch: 18 trial 8388 training loss: 0.018889030441641808\n",
      "epoch: 18 trial 8389 training loss: 0.009794611250981688\n",
      "epoch: 18 trial 8390 training loss: 0.055991293862462044\n",
      "epoch: 18 trial 8391 training loss: 0.0037894807173870504\n",
      "epoch: 18 trial 8392 training loss: 0.057413576170802116\n",
      "epoch: 18 trial 8393 training loss: 0.011822422500699759\n",
      "epoch: 18 trial 8394 training loss: 0.024484760127961636\n",
      "epoch: 18 trial 8395 training loss: 0.025255123618990183\n",
      "epoch: 18 trial 8396 training loss: 0.01322941342368722\n",
      "epoch: 18 trial 8397 training loss: 0.03432125598192215\n",
      "epoch: 18 trial 8398 training loss: 0.12001729011535645\n",
      "epoch: 18 trial 8399 training loss: 0.05331752821803093\n",
      "epoch: 18 trial 8400 training loss: 0.008688195375725627\n",
      "epoch: 18 trial 8401 training loss: 0.06565752997994423\n",
      "epoch: 18 trial 8402 training loss: 0.03290370665490627\n",
      "epoch: 18 trial 8403 training loss: 0.0256218696013093\n",
      "epoch: 18 trial 8404 training loss: 0.01849819626659155\n",
      "epoch: 18 trial 8405 training loss: 0.025746016297489405\n",
      "epoch: 18 trial 8406 training loss: 0.032079702243208885\n",
      "epoch: 18 trial 8407 training loss: 0.039050882682204247\n",
      "epoch: 18 trial 8408 training loss: 0.03912554122507572\n",
      "epoch: 18 trial 8409 training loss: 0.07261434383690357\n",
      "epoch: 18 trial 8410 training loss: 0.008008330361917615\n",
      "epoch: 18 trial 8411 training loss: 0.05201774276793003\n",
      "epoch: 18 trial 8412 training loss: 0.016556778457015753\n",
      "epoch: 18 trial 8413 training loss: 0.01760778808966279\n",
      "epoch: 18 trial 8414 training loss: 0.04237288888543844\n",
      "epoch: 18 trial 8415 training loss: 0.05149357579648495\n",
      "epoch: 18 trial 8416 training loss: 0.017326090950518847\n",
      "epoch: 18 trial 8417 training loss: 0.04136192984879017\n",
      "epoch: 18 trial 8418 training loss: 0.06911101378500462\n",
      "epoch: 18 trial 8419 training loss: 0.07038961164653301\n",
      "epoch: 18 trial 8420 training loss: 0.020081385504454374\n",
      "epoch: 18 trial 8421 training loss: 0.024002433754503727\n",
      "epoch: 18 trial 8422 training loss: 0.015130681917071342\n",
      "epoch: 18 trial 8423 training loss: 0.025782769545912743\n",
      "epoch: 18 trial 8424 training loss: 0.0359150692820549\n",
      "epoch: 18 trial 8425 training loss: 0.027961593121290207\n",
      "epoch: 18 trial 8426 training loss: 0.06284978147596121\n",
      "epoch: 18 trial 8427 training loss: 0.11228809133172035\n",
      "epoch: 18 trial 8428 training loss: 0.01714670890942216\n",
      "epoch: 18 trial 8429 training loss: 0.006108758272603154\n",
      "epoch: 18 trial 8430 training loss: 0.008061579195782542\n",
      "epoch: 18 trial 8431 training loss: 0.011667569866403937\n",
      "epoch: 18 trial 8432 training loss: 0.01459149643778801\n",
      "epoch: 18 trial 8433 training loss: 0.02249216102063656\n",
      "epoch: 18 trial 8434 training loss: 0.08156288228929043\n",
      "epoch: 18 trial 8435 training loss: 0.03647822421044111\n",
      "epoch: 18 trial 8436 training loss: 0.013757394626736641\n",
      "epoch: 18 trial 8437 training loss: 0.09260539710521698\n",
      "epoch: 18 trial 8438 training loss: 0.04140310827642679\n",
      "epoch: 18 trial 8439 training loss: 0.05647655017673969\n",
      "epoch: 18 trial 8440 training loss: 0.06437377817928791\n",
      "epoch: 18 trial 8441 training loss: 0.10525738075375557\n",
      "epoch: 18 trial 8442 training loss: 0.013115557609125972\n",
      "epoch: 18 trial 8443 training loss: 0.008094473509117961\n",
      "epoch: 18 trial 8444 training loss: 0.023435166105628014\n",
      "epoch: 18 trial 8445 training loss: 0.021543619222939014\n",
      "epoch: 18 trial 8446 training loss: 0.01868181023746729\n",
      "epoch: 18 trial 8447 training loss: 0.04894987307488918\n",
      "epoch: 18 trial 8448 training loss: 0.07206530496478081\n",
      "epoch: 18 trial 8449 training loss: 0.03148557897657156\n",
      "epoch: 18 trial 8450 training loss: 0.047800082713365555\n",
      "epoch: 18 trial 8451 training loss: 0.04016195237636566\n",
      "epoch: 18 trial 8452 training loss: 0.08831115439534187\n",
      "epoch: 18 trial 8453 training loss: 0.01263875956647098\n",
      "epoch: 18 trial 8454 training loss: 0.013077349867671728\n",
      "epoch: 18 trial 8455 training loss: 0.023508521728217602\n",
      "epoch: 18 trial 8456 training loss: 0.025737100280821323\n",
      "epoch: 18 trial 8457 training loss: 0.04777947627007961\n",
      "epoch: 18 trial 8458 training loss: 0.015231182798743248\n",
      "epoch: 18 trial 8459 training loss: 0.1676107719540596\n",
      "epoch: 18 trial 8460 training loss: 0.022846580017358065\n",
      "epoch: 18 trial 8461 training loss: 0.10596400499343872\n",
      "epoch: 18 trial 8462 training loss: 0.09957499243319035\n",
      "epoch: 18 trial 8463 training loss: 0.01545061101205647\n",
      "epoch: 18 trial 8464 training loss: 0.06403843313455582\n",
      "epoch: 18 trial 8465 training loss: 0.014291358646005392\n",
      "epoch: 18 trial 8466 training loss: 0.04688816238194704\n",
      "epoch: 18 trial 8467 training loss: 0.048312317579984665\n",
      "epoch: 18 trial 8468 training loss: 0.08161596581339836\n",
      "epoch: 18 trial 8469 training loss: 0.015034615993499756\n",
      "epoch: 18 trial 8470 training loss: 0.038729432970285416\n",
      "epoch: 18 trial 8471 training loss: 0.03599725477397442\n",
      "epoch: 18 trial 8472 training loss: 0.031729770824313164\n",
      "epoch: 18 trial 8473 training loss: 0.06186949834227562\n",
      "epoch: 18 trial 8474 training loss: 0.017481991555541754\n",
      "epoch: 18 trial 8475 training loss: 0.03269699774682522\n",
      "epoch: 18 trial 8476 training loss: 0.018538057804107666\n",
      "epoch: 18 trial 8477 training loss: 0.02499080030247569\n",
      "epoch: 18 trial 8478 training loss: 0.038278669118881226\n",
      "epoch: 18 trial 8479 training loss: 0.01120854215696454\n",
      "epoch: 18 trial 8480 training loss: 0.0071698392275720835\n",
      "epoch: 18 trial 8481 training loss: 0.03297953121364117\n",
      "epoch: 18 trial 8482 training loss: 0.08784789219498634\n",
      "epoch: 18 trial 8483 training loss: 0.02245946926996112\n",
      "epoch: 18 trial 8484 training loss: 0.08812570385634899\n",
      "epoch: 18 trial 8485 training loss: 0.04812810383737087\n",
      "epoch: 18 trial 8486 training loss: 0.02808120520785451\n",
      "epoch: 18 trial 8487 training loss: 0.1438828408718109\n",
      "epoch: 18 trial 8488 training loss: 0.012024631025269628\n",
      "epoch: 18 trial 8489 training loss: 0.011420216877013445\n",
      "epoch: 18 trial 8490 training loss: 0.011102105025202036\n",
      "epoch: 18 trial 8491 training loss: 0.004211309016682208\n",
      "epoch: 18 trial 8492 training loss: 0.03164448030292988\n",
      "epoch: 18 trial 8493 training loss: 0.03420951869338751\n",
      "epoch: 18 trial 8494 training loss: 0.04432322923094034\n",
      "epoch: 18 trial 8495 training loss: 0.02950581256300211\n",
      "epoch: 18 trial 8496 training loss: 0.04200310166925192\n",
      "epoch: 18 trial 8497 training loss: 0.021073524840176105\n",
      "epoch: 18 trial 8498 training loss: 0.09065617620944977\n",
      "epoch: 18 trial 8499 training loss: 0.017121021635830402\n",
      "epoch: 18 trial 8500 training loss: 0.008352086646482348\n",
      "epoch: 18 trial 8501 training loss: 0.024258687626570463\n",
      "epoch: 18 trial 8502 training loss: 0.00539490079972893\n",
      "epoch: 18 trial 8503 training loss: 0.07596826739609241\n",
      "epoch: 18 trial 8504 training loss: 0.0029328917735256255\n",
      "epoch: 18 trial 8505 training loss: 0.021731756161898375\n",
      "epoch: 18 trial 8506 training loss: 0.015242598252370954\n",
      "epoch: 18 trial 8507 training loss: 0.06006701476871967\n",
      "epoch: 18 trial 8508 training loss: 0.04614802449941635\n",
      "epoch: 18 trial 8509 training loss: 0.02284326171502471\n",
      "epoch: 18 trial 8510 training loss: 0.03108242154121399\n",
      "epoch: 18 trial 8511 training loss: 0.06219516880810261\n",
      "epoch: 18 trial 8512 training loss: 0.02514028735458851\n",
      "epoch: 18 trial 8513 training loss: 0.07011654041707516\n",
      "epoch: 18 trial 8514 training loss: 0.019579871091991663\n",
      "epoch: 18 trial 8515 training loss: 0.06370683945715427\n",
      "epoch: 18 trial 8516 training loss: 0.021576298400759697\n",
      "epoch: 18 trial 8517 training loss: 0.00786615302786231\n",
      "epoch: 18 trial 8518 training loss: 0.04893041029572487\n",
      "epoch: 18 trial 8519 training loss: 0.028780837543308735\n",
      "epoch: 18 trial 8520 training loss: 0.01709155412390828\n",
      "epoch: 18 trial 8521 training loss: 0.01841167639940977\n",
      "epoch: 18 trial 8522 training loss: 0.01039196690544486\n",
      "epoch: 18 trial 8523 training loss: 0.025821383111178875\n",
      "epoch: 18 trial 8524 training loss: 0.035103945061564445\n",
      "epoch: 18 trial 8525 training loss: 0.01681547239422798\n",
      "epoch: 18 trial 8526 training loss: 0.034232646226882935\n",
      "epoch: 18 trial 8527 training loss: 0.08683485351502895\n",
      "epoch: 18 trial 8528 training loss: 0.037801180966198444\n",
      "epoch: 18 trial 8529 training loss: 0.01327695045620203\n",
      "epoch: 18 trial 8530 training loss: 0.005014563677832484\n",
      "epoch: 18 trial 8531 training loss: 0.008005816722288728\n",
      "epoch: 18 trial 8532 training loss: 0.02707807905972004\n",
      "epoch: 18 trial 8533 training loss: 0.05160014517605305\n",
      "epoch: 18 trial 8534 training loss: 0.04445446189492941\n",
      "epoch: 18 trial 8535 training loss: 0.00403216719860211\n",
      "epoch: 18 trial 8536 training loss: 0.038821409456431866\n",
      "epoch: 18 trial 8537 training loss: 0.03448356967419386\n",
      "epoch: 18 trial 8538 training loss: 0.020732578355818987\n",
      "epoch: 18 trial 8539 training loss: 0.015106304083019495\n",
      "epoch: 18 trial 8540 training loss: 0.018089127261191607\n",
      "epoch: 18 trial 8541 training loss: 0.04235678631812334\n",
      "epoch: 18 trial 8542 training loss: 0.02289388980716467\n",
      "epoch: 18 trial 8543 training loss: 0.03355462197214365\n",
      "epoch: 18 trial 8544 training loss: 0.07630867883563042\n",
      "epoch: 18 trial 8545 training loss: 0.026508419774472713\n",
      "epoch: 18 trial 8546 training loss: 0.03241003677248955\n",
      "epoch: 18 trial 8547 training loss: 0.06474537961184978\n",
      "epoch: 18 trial 8548 training loss: 0.03222598833963275\n",
      "epoch: 18 trial 8549 training loss: 0.0631400141865015\n",
      "epoch: 18 trial 8550 training loss: 0.01704193279147148\n",
      "epoch: 18 trial 8551 training loss: 0.037023072596639395\n",
      "epoch: 18 trial 8552 training loss: 0.026060039177536964\n",
      "epoch: 18 trial 8553 training loss: 0.02205720078200102\n",
      "epoch: 18 trial 8554 training loss: 0.02641997952014208\n",
      "epoch: 18 trial 8555 training loss: 0.08676017075777054\n",
      "epoch: 18 trial 8556 training loss: 0.013649880420416594\n",
      "epoch: 18 trial 8557 training loss: 0.08659189194440842\n",
      "epoch: 18 trial 8558 training loss: 0.030346849001944065\n",
      "epoch: 18 trial 8559 training loss: 0.007205455098301172\n",
      "epoch: 18 trial 8560 training loss: 0.048906367272138596\n",
      "epoch: 18 trial 8561 training loss: 0.05990046635270119\n",
      "epoch: 18 trial 8562 training loss: 0.01069504558108747\n",
      "epoch: 18 trial 8563 training loss: 0.033646028488874435\n",
      "epoch: 18 trial 8564 training loss: 0.00961091835051775\n",
      "epoch: 18 trial 8565 training loss: 0.029352682642638683\n",
      "epoch: 18 trial 8566 training loss: 0.020839908625930548\n",
      "epoch: 18 trial 8567 training loss: 0.039293049834668636\n",
      "epoch: 18 trial 8568 training loss: 0.2635572552680969\n",
      "epoch: 18 trial 8569 training loss: 0.05641261488199234\n",
      "epoch: 18 trial 8570 training loss: 0.009321010089479387\n",
      "epoch: 18 trial 8571 training loss: 0.01735818712040782\n",
      "epoch: 18 trial 8572 training loss: 0.1337321139872074\n",
      "epoch: 18 trial 8573 training loss: 0.049247946590185165\n",
      "epoch: 18 trial 8574 training loss: 0.07663394883275032\n",
      "epoch: 18 trial 8575 training loss: 0.04961802437901497\n",
      "epoch: 18 trial 8576 training loss: 0.030690647661685944\n",
      "epoch: 18 trial 8577 training loss: 0.012362061534076929\n",
      "epoch: 18 trial 8578 training loss: 0.0730084590613842\n",
      "epoch: 18 trial 8579 training loss: 0.027932191733270884\n",
      "epoch: 18 trial 8580 training loss: 0.08688612654805183\n",
      "epoch: 18 trial 8581 training loss: 0.012130464194342494\n",
      "epoch: 18 trial 8582 training loss: 0.0060778678162023425\n",
      "epoch: 18 trial 8583 training loss: 0.029536458663642406\n",
      "epoch: 18 trial 8584 training loss: 0.006634624674916267\n",
      "epoch: 18 trial 8585 training loss: 0.013487701769918203\n",
      "epoch: 18 trial 8586 training loss: 0.09935617446899414\n",
      "epoch: 18 trial 8587 training loss: 0.07313716970384121\n",
      "epoch: 18 trial 8588 training loss: 0.02459491416811943\n",
      "epoch: 18 trial 8589 training loss: 0.1064571961760521\n",
      "epoch: 18 trial 8590 training loss: 0.08985763974487782\n",
      "epoch: 18 trial 8591 training loss: 0.028132596984505653\n",
      "epoch: 18 trial 8592 training loss: 0.06246786005795002\n",
      "epoch: 18 trial 8593 training loss: 0.034901538863778114\n",
      "epoch: 18 trial 8594 training loss: 0.011638896074146032\n",
      "epoch: 18 trial 8595 training loss: 0.005240019876509905\n",
      "epoch: 18 trial 8596 training loss: 0.12600852735340595\n",
      "epoch: 18 trial 8597 training loss: 0.06883661821484566\n",
      "epoch: 18 trial 8598 training loss: 0.043496591970324516\n",
      "epoch: 18 trial 8599 training loss: 0.05556097161024809\n",
      "epoch: 18 trial 8600 training loss: 0.023089514579623938\n",
      "epoch: 18 trial 8601 training loss: 0.03725885692983866\n",
      "epoch: 18 trial 8602 training loss: 0.014884031843394041\n",
      "epoch: 18 trial 8603 training loss: 0.04978090710937977\n",
      "epoch: 18 trial 8604 training loss: 0.026201162487268448\n",
      "epoch: 18 trial 8605 training loss: 0.037247550673782825\n",
      "epoch: 18 trial 8606 training loss: 0.08229404501616955\n",
      "epoch: 18 trial 8607 training loss: 0.022600051015615463\n",
      "epoch: 18 trial 8608 training loss: 0.009924859972670674\n",
      "epoch: 18 trial 8609 training loss: 0.034103672951459885\n",
      "epoch: 18 trial 8610 training loss: 0.004944543819874525\n",
      "epoch: 18 trial 8611 training loss: 0.037145535461604595\n",
      "epoch: 18 trial 8612 training loss: 0.04038931056857109\n",
      "epoch: 18 trial 8613 training loss: 0.011364444391801953\n",
      "epoch: 18 trial 8614 training loss: 0.0022437446750700474\n",
      "epoch: 18 trial 8615 training loss: 0.0043232961324974895\n",
      "epoch: 18 trial 8616 training loss: 0.017932545859366655\n",
      "epoch: 18 trial 8617 training loss: 0.03523929975926876\n",
      "epoch: 18 trial 8618 training loss: 0.004234043066389859\n",
      "epoch: 18 trial 8619 training loss: 0.004457202274352312\n",
      "epoch: 18 trial 8620 training loss: 0.006279924884438515\n",
      "epoch: 18 trial 8621 training loss: 0.008778564166277647\n",
      "epoch: 18 trial 8622 training loss: 0.010897418716922402\n",
      "epoch: 18 trial 8623 training loss: 0.0229708687402308\n",
      "epoch: 18 trial 8624 training loss: 0.10693483799695969\n",
      "epoch: 18 trial 8625 training loss: 0.019642028957605362\n",
      "epoch: 18 trial 8626 training loss: 0.018385318107903004\n",
      "epoch: 18 trial 8627 training loss: 0.027667334768921137\n",
      "epoch: 18 trial 8628 training loss: 0.03832041006535292\n",
      "epoch: 18 trial 8629 training loss: 0.03300888370722532\n",
      "epoch: 18 trial 8630 training loss: 0.029405861161649227\n",
      "epoch: 18 trial 8631 training loss: 0.03932033572345972\n",
      "epoch: 18 trial 8632 training loss: 0.07653813250362873\n",
      "epoch: 18 trial 8633 training loss: 0.10891895275563002\n",
      "epoch: 18 trial 8634 training loss: 0.018901048693805933\n",
      "epoch: 18 trial 8635 training loss: 0.02257508272305131\n",
      "epoch: 18 trial 8636 training loss: 0.08630489930510521\n",
      "epoch: 18 trial 8637 training loss: 0.016338839661329985\n",
      "epoch: 18 trial 8638 training loss: 0.05224754195660353\n",
      "epoch: 18 trial 8639 training loss: 0.09815873950719833\n",
      "epoch: 18 trial 8640 training loss: 0.09324471279978752\n",
      "epoch: 18 trial 8641 training loss: 0.04187842644751072\n",
      "epoch: 18 trial 8642 training loss: 0.10930559411644936\n",
      "epoch: 18 trial 8643 training loss: 0.018389520701020956\n",
      "epoch: 18 trial 8644 training loss: 0.05131847411394119\n",
      "epoch: 18 trial 8645 training loss: 0.034047503024339676\n",
      "epoch: 18 trial 8646 training loss: 0.09662602469325066\n",
      "epoch: 18 trial 8647 training loss: 0.008054207311943173\n",
      "epoch: 18 trial 8648 training loss: 0.0066165769239887595\n",
      "epoch: 18 trial 8649 training loss: 0.01860837684944272\n",
      "epoch: 18 trial 8650 training loss: 0.026975907385349274\n",
      "epoch: 18 trial 8651 training loss: 0.05169845186173916\n",
      "epoch: 18 trial 8652 training loss: 0.05360267963260412\n",
      "epoch: 18 trial 8653 training loss: 0.015593655873090029\n",
      "epoch: 18 trial 8654 training loss: 0.054784512147307396\n",
      "epoch: 18 trial 8655 training loss: 0.01684740511700511\n",
      "epoch: 18 trial 8656 training loss: 0.01618816750124097\n",
      "epoch: 18 trial 8657 training loss: 0.02026794571429491\n",
      "epoch: 18 trial 8658 training loss: 0.012333518825471401\n",
      "epoch: 18 trial 8659 training loss: 0.018241541925817728\n",
      "epoch: 18 trial 8660 training loss: 0.03897431958466768\n",
      "epoch: 18 trial 8661 training loss: 0.02757993619889021\n",
      "epoch: 18 trial 8662 training loss: 0.024723428767174482\n",
      "epoch: 18 trial 8663 training loss: 0.022260763216763735\n",
      "epoch: 18 trial 8664 training loss: 0.016975820064544678\n",
      "epoch: 18 trial 8665 training loss: 0.004851986654102802\n",
      "epoch: 18 trial 8666 training loss: 0.012532301247119904\n",
      "epoch: 18 trial 8667 training loss: 0.016484785359352827\n",
      "epoch: 18 trial 8668 training loss: 0.014195994008332491\n",
      "epoch: 18 trial 8669 training loss: 0.006129453657194972\n",
      "epoch: 18 trial 8670 training loss: 0.013075381983071566\n",
      "epoch: 18 trial 8671 training loss: 0.0777229368686676\n",
      "epoch: 18 trial 8672 training loss: 0.010761779267340899\n",
      "epoch: 18 trial 8673 training loss: 0.031243370845913887\n",
      "epoch: 18 trial 8674 training loss: 0.013887939974665642\n",
      "epoch: 18 trial 8675 training loss: 0.04745145607739687\n",
      "epoch: 18 trial 8676 training loss: 0.045353750698268414\n",
      "epoch: 18 trial 8677 training loss: 0.03997214138507843\n",
      "epoch: 18 trial 8678 training loss: 0.027433372102677822\n",
      "epoch: 18 trial 8679 training loss: 0.0201303125359118\n",
      "epoch: 18 trial 8680 training loss: 0.1037057414650917\n",
      "epoch: 18 trial 8681 training loss: 0.005487766698934138\n",
      "epoch: 18 trial 8682 training loss: 0.017986894119530916\n",
      "epoch: 18 trial 8683 training loss: 0.1302301175892353\n",
      "epoch: 18 trial 8684 training loss: 0.026139930821955204\n",
      "epoch: 18 trial 8685 training loss: 0.02153384080156684\n",
      "epoch: 18 trial 8686 training loss: 0.01767931948415935\n",
      "epoch: 18 trial 8687 training loss: 0.012924416922032833\n",
      "epoch: 18 trial 8688 training loss: 0.026881558820605278\n",
      "epoch: 18 trial 8689 training loss: 0.014356385450810194\n",
      "epoch: 18 trial 8690 training loss: 0.051442502066493034\n",
      "epoch: 18 trial 8691 training loss: 0.1582941971719265\n",
      "epoch: 18 trial 8692 training loss: 0.06988016329705715\n",
      "epoch: 18 trial 8693 training loss: 0.027362198568880558\n",
      "epoch: 18 trial 8694 training loss: 0.07149436511099339\n",
      "epoch: 18 trial 8695 training loss: 0.10263798013329506\n",
      "epoch: 18 trial 8696 training loss: 0.024387741927057505\n",
      "epoch: 18 trial 8697 training loss: 0.07164667546749115\n",
      "epoch: 18 trial 8698 training loss: 0.007028489897493273\n",
      "epoch: 18 trial 8699 training loss: 0.04073532484471798\n",
      "epoch: 18 trial 8700 training loss: 0.013072393834590912\n",
      "epoch: 18 trial 8701 training loss: 0.02780291438102722\n",
      "epoch: 18 trial 8702 training loss: 0.11323093622922897\n",
      "epoch: 18 trial 8703 training loss: 0.04631137661635876\n",
      "epoch: 18 trial 8704 training loss: 0.019570416305214167\n",
      "epoch: 18 trial 8705 training loss: 0.021246248856186867\n",
      "epoch: 18 trial 8706 training loss: 0.08519364520907402\n",
      "epoch: 18 trial 8707 training loss: 0.01532010268419981\n",
      "epoch: 18 trial 8708 training loss: 0.05738081783056259\n",
      "epoch: 18 trial 8709 training loss: 0.03643554914742708\n",
      "epoch: 18 trial 8710 training loss: 0.04748301673680544\n",
      "epoch: 18 trial 8711 training loss: 0.014491705223917961\n",
      "epoch: 18 trial 8712 training loss: 0.0583154521882534\n",
      "epoch: 19 trial 8713 training loss: 0.014536510687321424\n",
      "epoch: 19 trial 8714 training loss: 0.012844353914260864\n",
      "epoch: 19 trial 8715 training loss: 0.010756247444078326\n",
      "epoch: 19 trial 8716 training loss: 0.06388887390494347\n",
      "epoch: 19 trial 8717 training loss: 0.0044706957414746284\n",
      "epoch: 19 trial 8718 training loss: 0.00527385005261749\n",
      "epoch: 19 trial 8719 training loss: 0.02675690036267042\n",
      "epoch: 19 trial 8720 training loss: 0.03025953471660614\n",
      "epoch: 19 trial 8721 training loss: 0.0060695119900628924\n",
      "epoch: 19 trial 8722 training loss: 0.01328619266860187\n",
      "epoch: 19 trial 8723 training loss: 0.05485636182129383\n",
      "epoch: 19 trial 8724 training loss: 0.03301683720201254\n",
      "epoch: 19 trial 8725 training loss: 0.01607728097587824\n",
      "epoch: 19 trial 8726 training loss: 0.017786644864827394\n",
      "epoch: 19 trial 8727 training loss: 0.020684625022113323\n",
      "epoch: 19 trial 8728 training loss: 0.024040550459176302\n",
      "epoch: 19 trial 8729 training loss: 0.10646336898207664\n",
      "epoch: 19 trial 8730 training loss: 0.04981381818652153\n",
      "epoch: 19 trial 8731 training loss: 0.017208264209330082\n",
      "epoch: 19 trial 8732 training loss: 0.020033792592585087\n",
      "epoch: 19 trial 8733 training loss: 0.01850591879338026\n",
      "epoch: 19 trial 8734 training loss: 0.07823114283382893\n",
      "epoch: 19 trial 8735 training loss: 0.012335481587797403\n",
      "epoch: 19 trial 8736 training loss: 0.003273230860941112\n",
      "epoch: 19 trial 8737 training loss: 0.1343553513288498\n",
      "epoch: 19 trial 8738 training loss: 0.026311670430004597\n",
      "epoch: 19 trial 8739 training loss: 0.010650120209902525\n",
      "epoch: 19 trial 8740 training loss: 0.10517601110041142\n",
      "epoch: 19 trial 8741 training loss: 0.012800561729818583\n",
      "epoch: 19 trial 8742 training loss: 0.02057328773662448\n",
      "epoch: 19 trial 8743 training loss: 0.025089776143431664\n",
      "epoch: 19 trial 8744 training loss: 0.0130555285140872\n",
      "epoch: 19 trial 8745 training loss: 0.030082921497523785\n",
      "epoch: 19 trial 8746 training loss: 0.04665016941726208\n",
      "epoch: 19 trial 8747 training loss: 0.016371082980185747\n",
      "epoch: 19 trial 8748 training loss: 0.06082824990153313\n",
      "epoch: 19 trial 8749 training loss: 0.15424393489956856\n",
      "epoch: 19 trial 8750 training loss: 0.3032364994287491\n",
      "epoch: 19 trial 8751 training loss: 0.04447157308459282\n",
      "epoch: 19 trial 8752 training loss: 0.08952698111534119\n",
      "epoch: 19 trial 8753 training loss: 0.09572686441242695\n",
      "epoch: 19 trial 8754 training loss: 0.011410261038690805\n",
      "epoch: 19 trial 8755 training loss: 0.07552855275571346\n",
      "epoch: 19 trial 8756 training loss: 0.03751328121870756\n",
      "epoch: 19 trial 8757 training loss: 0.026921135373413563\n",
      "epoch: 19 trial 8758 training loss: 0.05762503854930401\n",
      "epoch: 19 trial 8759 training loss: 0.010787306819111109\n",
      "epoch: 19 trial 8760 training loss: 0.026792421005666256\n",
      "epoch: 19 trial 8761 training loss: 0.01561375753954053\n",
      "epoch: 19 trial 8762 training loss: 0.0984722338616848\n",
      "epoch: 19 trial 8763 training loss: 0.0366695960983634\n",
      "epoch: 19 trial 8764 training loss: 0.12229225784540176\n",
      "epoch: 19 trial 8765 training loss: 0.034437600523233414\n",
      "epoch: 19 trial 8766 training loss: 0.04373475443571806\n",
      "epoch: 19 trial 8767 training loss: 0.034568190574645996\n",
      "epoch: 19 trial 8768 training loss: 0.054839568212628365\n",
      "epoch: 19 trial 8769 training loss: 0.026999304071068764\n",
      "epoch: 19 trial 8770 training loss: 0.017262430861592293\n",
      "epoch: 19 trial 8771 training loss: 0.029169632121920586\n",
      "epoch: 19 trial 8772 training loss: 0.08124799653887749\n",
      "epoch: 19 trial 8773 training loss: 0.023874106351286173\n",
      "epoch: 19 trial 8774 training loss: 0.12272489070892334\n",
      "epoch: 19 trial 8775 training loss: 0.019123990088701248\n",
      "epoch: 19 trial 8776 training loss: 0.003241696918848902\n",
      "epoch: 19 trial 8777 training loss: 0.04774313699454069\n",
      "epoch: 19 trial 8778 training loss: 0.05564643256366253\n",
      "epoch: 19 trial 8779 training loss: 0.02133372938260436\n",
      "epoch: 19 trial 8780 training loss: 0.021371242124587297\n",
      "epoch: 19 trial 8781 training loss: 0.05660763382911682\n",
      "epoch: 19 trial 8782 training loss: 0.03990674950182438\n",
      "epoch: 19 trial 8783 training loss: 0.043987625278532505\n",
      "epoch: 19 trial 8784 training loss: 0.03337293770164251\n",
      "epoch: 19 trial 8785 training loss: 0.014383487869054079\n",
      "epoch: 19 trial 8786 training loss: 0.06527530588209629\n",
      "epoch: 19 trial 8787 training loss: 0.01403710967861116\n",
      "epoch: 19 trial 8788 training loss: 0.02426040545105934\n",
      "epoch: 19 trial 8789 training loss: 0.02960353624075651\n",
      "epoch: 19 trial 8790 training loss: 0.07925618812441826\n",
      "epoch: 19 trial 8791 training loss: 0.01919881347566843\n",
      "epoch: 19 trial 8792 training loss: 0.05353093333542347\n",
      "epoch: 19 trial 8793 training loss: 0.027907209936529398\n",
      "epoch: 19 trial 8794 training loss: 0.043997797183692455\n",
      "epoch: 19 trial 8795 training loss: 0.04387823026627302\n",
      "epoch: 19 trial 8796 training loss: 0.02169650187715888\n",
      "epoch: 19 trial 8797 training loss: 0.06749697402119637\n",
      "epoch: 19 trial 8798 training loss: 0.01667933608405292\n",
      "epoch: 19 trial 8799 training loss: 0.029300890862941742\n",
      "epoch: 19 trial 8800 training loss: 0.04972750414162874\n",
      "epoch: 19 trial 8801 training loss: 0.053653694689273834\n",
      "epoch: 19 trial 8802 training loss: 0.0054993509547784925\n",
      "epoch: 19 trial 8803 training loss: 0.041403280571103096\n",
      "epoch: 19 trial 8804 training loss: 0.025510345585644245\n",
      "epoch: 19 trial 8805 training loss: 0.02105932799167931\n",
      "epoch: 19 trial 8806 training loss: 0.07039618864655495\n",
      "epoch: 19 trial 8807 training loss: 0.04854197520762682\n",
      "epoch: 19 trial 8808 training loss: 0.05313275475054979\n",
      "epoch: 19 trial 8809 training loss: 0.009459490654990077\n",
      "epoch: 19 trial 8810 training loss: 0.07987977936863899\n",
      "epoch: 19 trial 8811 training loss: 0.022235105745494366\n",
      "epoch: 19 trial 8812 training loss: 0.04391804337501526\n",
      "epoch: 19 trial 8813 training loss: 0.030520852655172348\n",
      "epoch: 19 trial 8814 training loss: 0.09834934771060944\n",
      "epoch: 19 trial 8815 training loss: 0.0502219432964921\n",
      "epoch: 19 trial 8816 training loss: 0.027705642394721508\n",
      "epoch: 19 trial 8817 training loss: 0.22875744104385376\n",
      "epoch: 19 trial 8818 training loss: 0.07679691724479198\n",
      "epoch: 19 trial 8819 training loss: 0.025977826211601496\n",
      "epoch: 19 trial 8820 training loss: 0.16999604552984238\n",
      "epoch: 19 trial 8821 training loss: 0.07575724646449089\n",
      "epoch: 19 trial 8822 training loss: 0.026315024122595787\n",
      "epoch: 19 trial 8823 training loss: 0.024585658218711615\n",
      "epoch: 19 trial 8824 training loss: 0.025988230481743813\n",
      "epoch: 19 trial 8825 training loss: 0.015072576701641083\n",
      "epoch: 19 trial 8826 training loss: 0.009910986758768559\n",
      "epoch: 19 trial 8827 training loss: 0.057801212184131145\n",
      "epoch: 19 trial 8828 training loss: 0.01863562176004052\n",
      "epoch: 19 trial 8829 training loss: 0.023859787732362747\n",
      "epoch: 19 trial 8830 training loss: 0.03610457666218281\n",
      "epoch: 19 trial 8831 training loss: 0.03147038258612156\n",
      "epoch: 19 trial 8832 training loss: 0.02949967421591282\n",
      "epoch: 19 trial 8833 training loss: 0.022761006839573383\n",
      "epoch: 19 trial 8834 training loss: 0.035841917619109154\n",
      "epoch: 19 trial 8835 training loss: 0.08217490650713444\n",
      "epoch: 19 trial 8836 training loss: 0.036490959115326405\n",
      "epoch: 19 trial 8837 training loss: 0.008429929497651756\n",
      "epoch: 19 trial 8838 training loss: 0.02129696263000369\n",
      "epoch: 19 trial 8839 training loss: 0.021329266019165516\n",
      "epoch: 19 trial 8840 training loss: 0.02496365550905466\n",
      "epoch: 19 trial 8841 training loss: 0.14107762277126312\n",
      "epoch: 19 trial 8842 training loss: 0.02764170989394188\n",
      "epoch: 19 trial 8843 training loss: 0.04045214410871267\n",
      "epoch: 19 trial 8844 training loss: 0.027792741544544697\n",
      "epoch: 19 trial 8845 training loss: 0.032184588722884655\n",
      "epoch: 19 trial 8846 training loss: 0.043428871780633926\n",
      "epoch: 19 trial 8847 training loss: 0.010165323037654161\n",
      "epoch: 19 trial 8848 training loss: 0.04195926710963249\n",
      "epoch: 19 trial 8849 training loss: 0.013643966056406498\n",
      "epoch: 19 trial 8850 training loss: 0.1139519177377224\n",
      "epoch: 19 trial 8851 training loss: 0.019842554815113544\n",
      "epoch: 19 trial 8852 training loss: 0.06057882774621248\n",
      "epoch: 19 trial 8853 training loss: 0.03328618500381708\n",
      "epoch: 19 trial 8854 training loss: 0.03324169106781483\n",
      "epoch: 19 trial 8855 training loss: 0.028027398511767387\n",
      "epoch: 19 trial 8856 training loss: 0.12025152146816254\n",
      "epoch: 19 trial 8857 training loss: 0.009336733724921942\n",
      "epoch: 19 trial 8858 training loss: 0.015569853596389294\n",
      "epoch: 19 trial 8859 training loss: 0.08381668850779533\n",
      "epoch: 19 trial 8860 training loss: 0.023878312669694424\n",
      "epoch: 19 trial 8861 training loss: 0.028588080313056707\n",
      "epoch: 19 trial 8862 training loss: 0.053600702434778214\n",
      "epoch: 19 trial 8863 training loss: 0.02574076224118471\n",
      "epoch: 19 trial 8864 training loss: 0.022562432568520308\n",
      "epoch: 19 trial 8865 training loss: 0.014604857889935374\n",
      "epoch: 19 trial 8866 training loss: 0.035830093547701836\n",
      "epoch: 19 trial 8867 training loss: 0.013123207027092576\n",
      "epoch: 19 trial 8868 training loss: 0.009059005184099078\n",
      "epoch: 19 trial 8869 training loss: 0.03229777794331312\n",
      "epoch: 19 trial 8870 training loss: 0.003924189950339496\n",
      "epoch: 19 trial 8871 training loss: 0.010699011152610183\n",
      "epoch: 19 trial 8872 training loss: 0.01714622275903821\n",
      "epoch: 19 trial 8873 training loss: 0.010409804992377758\n",
      "epoch: 19 trial 8874 training loss: 0.05602390691637993\n",
      "epoch: 19 trial 8875 training loss: 0.004008483956567943\n",
      "epoch: 19 trial 8876 training loss: 0.05188905354589224\n",
      "epoch: 19 trial 8877 training loss: 0.012136292178183794\n",
      "epoch: 19 trial 8878 training loss: 0.022677637171000242\n",
      "epoch: 19 trial 8879 training loss: 0.02517124777659774\n",
      "epoch: 19 trial 8880 training loss: 0.013444446958601475\n",
      "epoch: 19 trial 8881 training loss: 0.03173325955867767\n",
      "epoch: 19 trial 8882 training loss: 0.12884574756026268\n",
      "epoch: 19 trial 8883 training loss: 0.05073361471295357\n",
      "epoch: 19 trial 8884 training loss: 0.009107154794037342\n",
      "epoch: 19 trial 8885 training loss: 0.06353944540023804\n",
      "epoch: 19 trial 8886 training loss: 0.033579301089048386\n",
      "epoch: 19 trial 8887 training loss: 0.02523468155413866\n",
      "epoch: 19 trial 8888 training loss: 0.01662528677843511\n",
      "epoch: 19 trial 8889 training loss: 0.026047254912555218\n",
      "epoch: 19 trial 8890 training loss: 0.035455779172480106\n",
      "epoch: 19 trial 8891 training loss: 0.038864990696311\n",
      "epoch: 19 trial 8892 training loss: 0.03640729561448097\n",
      "epoch: 19 trial 8893 training loss: 0.0752954799681902\n",
      "epoch: 19 trial 8894 training loss: 0.008370679686777294\n",
      "epoch: 19 trial 8895 training loss: 0.05598266050219536\n",
      "epoch: 19 trial 8896 training loss: 0.020266239531338215\n",
      "epoch: 19 trial 8897 training loss: 0.01796108717098832\n",
      "epoch: 19 trial 8898 training loss: 0.04832614213228226\n",
      "epoch: 19 trial 8899 training loss: 0.0520341694355011\n",
      "epoch: 19 trial 8900 training loss: 0.01792028034105897\n",
      "epoch: 19 trial 8901 training loss: 0.036959351040422916\n",
      "epoch: 19 trial 8902 training loss: 0.07472835667431355\n",
      "epoch: 19 trial 8903 training loss: 0.07197001948952675\n",
      "epoch: 19 trial 8904 training loss: 0.0162355974316597\n",
      "epoch: 19 trial 8905 training loss: 0.022771958261728287\n",
      "epoch: 19 trial 8906 training loss: 0.014110664837062359\n",
      "epoch: 19 trial 8907 training loss: 0.022884386125952005\n",
      "epoch: 19 trial 8908 training loss: 0.03509931452572346\n",
      "epoch: 19 trial 8909 training loss: 0.02919118059799075\n",
      "epoch: 19 trial 8910 training loss: 0.06459394469857216\n",
      "epoch: 19 trial 8911 training loss: 0.11324608698487282\n",
      "epoch: 19 trial 8912 training loss: 0.01863593654707074\n",
      "epoch: 19 trial 8913 training loss: 0.005827210494317114\n",
      "epoch: 19 trial 8914 training loss: 0.008241201285272837\n",
      "epoch: 19 trial 8915 training loss: 0.011378365568816662\n",
      "epoch: 19 trial 8916 training loss: 0.015006655361503363\n",
      "epoch: 19 trial 8917 training loss: 0.02377133397385478\n",
      "epoch: 19 trial 8918 training loss: 0.09030435793101788\n",
      "epoch: 19 trial 8919 training loss: 0.03586236480623484\n",
      "epoch: 19 trial 8920 training loss: 0.013659017393365502\n",
      "epoch: 19 trial 8921 training loss: 0.09809194132685661\n",
      "epoch: 19 trial 8922 training loss: 0.042711700312793255\n",
      "epoch: 19 trial 8923 training loss: 0.059209566563367844\n",
      "epoch: 19 trial 8924 training loss: 0.061905866488814354\n",
      "epoch: 19 trial 8925 training loss: 0.10454651527106762\n",
      "epoch: 19 trial 8926 training loss: 0.013860576320439577\n",
      "epoch: 19 trial 8927 training loss: 0.008361963788047433\n",
      "epoch: 19 trial 8928 training loss: 0.022751567885279655\n",
      "epoch: 19 trial 8929 training loss: 0.020521567668765783\n",
      "epoch: 19 trial 8930 training loss: 0.019775547552853823\n",
      "epoch: 19 trial 8931 training loss: 0.05008475389331579\n",
      "epoch: 19 trial 8932 training loss: 0.07482481189072132\n",
      "epoch: 19 trial 8933 training loss: 0.03182004299014807\n",
      "epoch: 19 trial 8934 training loss: 0.047383093275129795\n",
      "epoch: 19 trial 8935 training loss: 0.041964342817664146\n",
      "epoch: 19 trial 8936 training loss: 0.09335766173899174\n",
      "epoch: 19 trial 8937 training loss: 0.01298552518710494\n",
      "epoch: 19 trial 8938 training loss: 0.011722648283466697\n",
      "epoch: 19 trial 8939 training loss: 0.023407389409840107\n",
      "epoch: 19 trial 8940 training loss: 0.0252089761197567\n",
      "epoch: 19 trial 8941 training loss: 0.049641371704638004\n",
      "epoch: 19 trial 8942 training loss: 0.015837943647056818\n",
      "epoch: 19 trial 8943 training loss: 0.1676160916686058\n",
      "epoch: 19 trial 8944 training loss: 0.01969366194680333\n",
      "epoch: 19 trial 8945 training loss: 0.1098952479660511\n",
      "epoch: 19 trial 8946 training loss: 0.09641164541244507\n",
      "epoch: 19 trial 8947 training loss: 0.013854142278432846\n",
      "epoch: 19 trial 8948 training loss: 0.06622689962387085\n",
      "epoch: 19 trial 8949 training loss: 0.014703845139592886\n",
      "epoch: 19 trial 8950 training loss: 0.047982039861381054\n",
      "epoch: 19 trial 8951 training loss: 0.0457210224121809\n",
      "epoch: 19 trial 8952 training loss: 0.07839022949337959\n",
      "epoch: 19 trial 8953 training loss: 0.01649081241339445\n",
      "epoch: 19 trial 8954 training loss: 0.04044455010443926\n",
      "epoch: 19 trial 8955 training loss: 0.032838910818099976\n",
      "epoch: 19 trial 8956 training loss: 0.03229153994470835\n",
      "epoch: 19 trial 8957 training loss: 0.05865796096622944\n",
      "epoch: 19 trial 8958 training loss: 0.018082076217979193\n",
      "epoch: 19 trial 8959 training loss: 0.034440649673342705\n",
      "epoch: 19 trial 8960 training loss: 0.020823421888053417\n",
      "epoch: 19 trial 8961 training loss: 0.0234645651653409\n",
      "epoch: 19 trial 8962 training loss: 0.03894846420735121\n",
      "epoch: 19 trial 8963 training loss: 0.011110014747828245\n",
      "epoch: 19 trial 8964 training loss: 0.007156431209295988\n",
      "epoch: 19 trial 8965 training loss: 0.03439783025532961\n",
      "epoch: 19 trial 8966 training loss: 0.08466772735118866\n",
      "epoch: 19 trial 8967 training loss: 0.021115224808454514\n",
      "epoch: 19 trial 8968 training loss: 0.0948885940015316\n",
      "epoch: 19 trial 8969 training loss: 0.05193748790770769\n",
      "epoch: 19 trial 8970 training loss: 0.02687559323385358\n",
      "epoch: 19 trial 8971 training loss: 0.1430068016052246\n",
      "epoch: 19 trial 8972 training loss: 0.012564846314489841\n",
      "epoch: 19 trial 8973 training loss: 0.010915344348177314\n",
      "epoch: 19 trial 8974 training loss: 0.01109493081457913\n",
      "epoch: 19 trial 8975 training loss: 0.003973513841629028\n",
      "epoch: 19 trial 8976 training loss: 0.029744248371571302\n",
      "epoch: 19 trial 8977 training loss: 0.03460582811385393\n",
      "epoch: 19 trial 8978 training loss: 0.049289862625300884\n",
      "epoch: 19 trial 8979 training loss: 0.03114276472479105\n",
      "epoch: 19 trial 8980 training loss: 0.0454993499442935\n",
      "epoch: 19 trial 8981 training loss: 0.019452459178864956\n",
      "epoch: 19 trial 8982 training loss: 0.089110616594553\n",
      "epoch: 19 trial 8983 training loss: 0.017458895687013865\n",
      "epoch: 19 trial 8984 training loss: 0.008802836062386632\n",
      "epoch: 19 trial 8985 training loss: 0.02292477060109377\n",
      "epoch: 19 trial 8986 training loss: 0.005513970274478197\n",
      "epoch: 19 trial 8987 training loss: 0.07017576694488525\n",
      "epoch: 19 trial 8988 training loss: 0.0031827620696276426\n",
      "epoch: 19 trial 8989 training loss: 0.02359679387882352\n",
      "epoch: 19 trial 8990 training loss: 0.01747393887490034\n",
      "epoch: 19 trial 8991 training loss: 0.06168975122272968\n",
      "epoch: 19 trial 8992 training loss: 0.04758378863334656\n",
      "epoch: 19 trial 8993 training loss: 0.022719244472682476\n",
      "epoch: 19 trial 8994 training loss: 0.03457497153431177\n",
      "epoch: 19 trial 8995 training loss: 0.06456420943140984\n",
      "epoch: 19 trial 8996 training loss: 0.02635684423148632\n",
      "epoch: 19 trial 8997 training loss: 0.07818594947457314\n",
      "epoch: 19 trial 8998 training loss: 0.021567860152572393\n",
      "epoch: 19 trial 8999 training loss: 0.06889938190579414\n",
      "epoch: 19 trial 9000 training loss: 0.020336799789220095\n",
      "epoch: 19 trial 9001 training loss: 0.008320705033838749\n",
      "epoch: 19 trial 9002 training loss: 0.05317102558910847\n",
      "epoch: 19 trial 9003 training loss: 0.02940289955586195\n",
      "epoch: 19 trial 9004 training loss: 0.01751727587543428\n",
      "epoch: 19 trial 9005 training loss: 0.017606619047001004\n",
      "epoch: 19 trial 9006 training loss: 0.009844161570072174\n",
      "epoch: 19 trial 9007 training loss: 0.02771834284067154\n",
      "epoch: 19 trial 9008 training loss: 0.03843772318214178\n",
      "epoch: 19 trial 9009 training loss: 0.016448639798909426\n",
      "epoch: 19 trial 9010 training loss: 0.032088929787278175\n",
      "epoch: 19 trial 9011 training loss: 0.09337599948048592\n",
      "epoch: 19 trial 9012 training loss: 0.039806684479117393\n",
      "epoch: 19 trial 9013 training loss: 0.012922359397634864\n",
      "epoch: 19 trial 9014 training loss: 0.0048670596443116665\n",
      "epoch: 19 trial 9015 training loss: 0.007477404782548547\n",
      "epoch: 19 trial 9016 training loss: 0.02690406609326601\n",
      "epoch: 19 trial 9017 training loss: 0.051233273930847645\n",
      "epoch: 19 trial 9018 training loss: 0.04552052542567253\n",
      "epoch: 19 trial 9019 training loss: 0.003318097733426839\n",
      "epoch: 19 trial 9020 training loss: 0.033114793710410595\n",
      "epoch: 19 trial 9021 training loss: 0.031170670874416828\n",
      "epoch: 19 trial 9022 training loss: 0.021348595153540373\n",
      "epoch: 19 trial 9023 training loss: 0.01470900559797883\n",
      "epoch: 19 trial 9024 training loss: 0.017337012570351362\n",
      "epoch: 19 trial 9025 training loss: 0.04001852683722973\n",
      "epoch: 19 trial 9026 training loss: 0.024467292707413435\n",
      "epoch: 19 trial 9027 training loss: 0.03377808257937431\n",
      "epoch: 19 trial 9028 training loss: 0.07668196596205235\n",
      "epoch: 19 trial 9029 training loss: 0.022280164062976837\n",
      "epoch: 19 trial 9030 training loss: 0.03126348555088043\n",
      "epoch: 19 trial 9031 training loss: 0.0616404227912426\n",
      "epoch: 19 trial 9032 training loss: 0.03397222515195608\n",
      "epoch: 19 trial 9033 training loss: 0.059659192338585854\n",
      "epoch: 19 trial 9034 training loss: 0.017693682573735714\n",
      "epoch: 19 trial 9035 training loss: 0.03194220969453454\n",
      "epoch: 19 trial 9036 training loss: 0.02576387207955122\n",
      "epoch: 19 trial 9037 training loss: 0.022516322322189808\n",
      "epoch: 19 trial 9038 training loss: 0.02500369120389223\n",
      "epoch: 19 trial 9039 training loss: 0.08776549994945526\n",
      "epoch: 19 trial 9040 training loss: 0.012960424646735191\n",
      "epoch: 19 trial 9041 training loss: 0.0874409880489111\n",
      "epoch: 19 trial 9042 training loss: 0.027418870478868484\n",
      "epoch: 19 trial 9043 training loss: 0.006781593896448612\n",
      "epoch: 19 trial 9044 training loss: 0.04914248175919056\n",
      "epoch: 19 trial 9045 training loss: 0.060337288305163383\n",
      "epoch: 19 trial 9046 training loss: 0.01054917206056416\n",
      "epoch: 19 trial 9047 training loss: 0.034517690539360046\n",
      "epoch: 19 trial 9048 training loss: 0.009698299691081047\n",
      "epoch: 19 trial 9049 training loss: 0.03267197124660015\n",
      "epoch: 19 trial 9050 training loss: 0.021050160750746727\n",
      "epoch: 19 trial 9051 training loss: 0.035404457710683346\n",
      "epoch: 19 trial 9052 training loss: 0.24961533024907112\n",
      "epoch: 19 trial 9053 training loss: 0.05596695467829704\n",
      "epoch: 19 trial 9054 training loss: 0.00922388443723321\n",
      "epoch: 19 trial 9055 training loss: 0.017194777727127075\n",
      "epoch: 19 trial 9056 training loss: 0.1256169080734253\n",
      "epoch: 19 trial 9057 training loss: 0.047189608216285706\n",
      "epoch: 19 trial 9058 training loss: 0.07527676224708557\n",
      "epoch: 19 trial 9059 training loss: 0.04797921609133482\n",
      "epoch: 19 trial 9060 training loss: 0.031207431107759476\n",
      "epoch: 19 trial 9061 training loss: 0.011398336617276073\n",
      "epoch: 19 trial 9062 training loss: 0.06815423257648945\n",
      "epoch: 19 trial 9063 training loss: 0.026088641956448555\n",
      "epoch: 19 trial 9064 training loss: 0.08880012296140194\n",
      "epoch: 19 trial 9065 training loss: 0.012771655106917024\n",
      "epoch: 19 trial 9066 training loss: 0.005689271609298885\n",
      "epoch: 19 trial 9067 training loss: 0.028774110600352287\n",
      "epoch: 19 trial 9068 training loss: 0.005908053368330002\n",
      "epoch: 19 trial 9069 training loss: 0.015140913892537355\n",
      "epoch: 19 trial 9070 training loss: 0.09984752722084522\n",
      "epoch: 19 trial 9071 training loss: 0.07243584096431732\n",
      "epoch: 19 trial 9072 training loss: 0.026610534638166428\n",
      "epoch: 19 trial 9073 training loss: 0.11141020432114601\n",
      "epoch: 19 trial 9074 training loss: 0.0915456973016262\n",
      "epoch: 19 trial 9075 training loss: 0.026433894876390696\n",
      "epoch: 19 trial 9076 training loss: 0.059121438302099705\n",
      "epoch: 19 trial 9077 training loss: 0.035680923610925674\n",
      "epoch: 19 trial 9078 training loss: 0.01313723437488079\n",
      "epoch: 19 trial 9079 training loss: 0.004945839638821781\n",
      "epoch: 19 trial 9080 training loss: 0.12951551005244255\n",
      "epoch: 19 trial 9081 training loss: 0.06912733055651188\n",
      "epoch: 19 trial 9082 training loss: 0.04964517895132303\n",
      "epoch: 19 trial 9083 training loss: 0.05724626034498215\n",
      "epoch: 19 trial 9084 training loss: 0.024795715231448412\n",
      "epoch: 19 trial 9085 training loss: 0.03682104218751192\n",
      "epoch: 19 trial 9086 training loss: 0.014552716631442308\n",
      "epoch: 19 trial 9087 training loss: 0.04683640133589506\n",
      "epoch: 19 trial 9088 training loss: 0.031108377501368523\n",
      "epoch: 19 trial 9089 training loss: 0.037171948701143265\n",
      "epoch: 19 trial 9090 training loss: 0.08405902609229088\n",
      "epoch: 19 trial 9091 training loss: 0.025660183746367693\n",
      "epoch: 19 trial 9092 training loss: 0.010265582241117954\n",
      "epoch: 19 trial 9093 training loss: 0.03140203841030598\n",
      "epoch: 19 trial 9094 training loss: 0.004173139692284167\n",
      "epoch: 19 trial 9095 training loss: 0.03488217294216156\n",
      "epoch: 19 trial 9096 training loss: 0.03843609616160393\n",
      "epoch: 19 trial 9097 training loss: 0.01309495372697711\n",
      "epoch: 19 trial 9098 training loss: 0.002291058364789933\n",
      "epoch: 19 trial 9099 training loss: 0.0038570525939576328\n",
      "epoch: 19 trial 9100 training loss: 0.01726199360564351\n",
      "epoch: 19 trial 9101 training loss: 0.03390246722847223\n",
      "epoch: 19 trial 9102 training loss: 0.004502248833887279\n",
      "epoch: 19 trial 9103 training loss: 0.004537166794762015\n",
      "epoch: 19 trial 9104 training loss: 0.006070030154660344\n",
      "epoch: 19 trial 9105 training loss: 0.008449454791843891\n",
      "epoch: 19 trial 9106 training loss: 0.011024984298273921\n",
      "epoch: 19 trial 9107 training loss: 0.023758013732731342\n",
      "epoch: 19 trial 9108 training loss: 0.11087517440319061\n",
      "epoch: 19 trial 9109 training loss: 0.01882956875488162\n",
      "epoch: 19 trial 9110 training loss: 0.019051704090088606\n",
      "epoch: 19 trial 9111 training loss: 0.02685292763635516\n",
      "epoch: 19 trial 9112 training loss: 0.03806019574403763\n",
      "epoch: 19 trial 9113 training loss: 0.03566661290824413\n",
      "epoch: 19 trial 9114 training loss: 0.03170534037053585\n",
      "epoch: 19 trial 9115 training loss: 0.04346337262541056\n",
      "epoch: 19 trial 9116 training loss: 0.07953068055212498\n",
      "epoch: 19 trial 9117 training loss: 0.1087824646383524\n",
      "epoch: 19 trial 9118 training loss: 0.021470142528414726\n",
      "epoch: 19 trial 9119 training loss: 0.02206320222467184\n",
      "epoch: 19 trial 9120 training loss: 0.0824490375816822\n",
      "epoch: 19 trial 9121 training loss: 0.018004130572080612\n",
      "epoch: 19 trial 9122 training loss: 0.055189719423651695\n",
      "epoch: 19 trial 9123 training loss: 0.10394272953271866\n",
      "epoch: 19 trial 9124 training loss: 0.09580853208899498\n",
      "epoch: 19 trial 9125 training loss: 0.03874581400305033\n",
      "epoch: 19 trial 9126 training loss: 0.10387197136878967\n",
      "epoch: 19 trial 9127 training loss: 0.021223858930170536\n",
      "epoch: 19 trial 9128 training loss: 0.050312966108322144\n",
      "epoch: 19 trial 9129 training loss: 0.034018343314528465\n",
      "epoch: 19 trial 9130 training loss: 0.10217892192304134\n",
      "epoch: 19 trial 9131 training loss: 0.0076569486409425735\n",
      "epoch: 19 trial 9132 training loss: 0.007263755309395492\n",
      "epoch: 19 trial 9133 training loss: 0.01656111958436668\n",
      "epoch: 19 trial 9134 training loss: 0.029136978089809418\n",
      "epoch: 19 trial 9135 training loss: 0.056335125118494034\n",
      "epoch: 19 trial 9136 training loss: 0.05025693215429783\n",
      "epoch: 19 trial 9137 training loss: 0.015155232977122068\n",
      "epoch: 19 trial 9138 training loss: 0.058842793107032776\n",
      "epoch: 19 trial 9139 training loss: 0.01496836170554161\n",
      "epoch: 19 trial 9140 training loss: 0.01699853129684925\n",
      "epoch: 19 trial 9141 training loss: 0.020768790040165186\n",
      "epoch: 19 trial 9142 training loss: 0.012472297064960003\n",
      "epoch: 19 trial 9143 training loss: 0.019750695675611496\n",
      "epoch: 19 trial 9144 training loss: 0.041200281120836735\n",
      "epoch: 19 trial 9145 training loss: 0.027180716395378113\n",
      "epoch: 19 trial 9146 training loss: 0.02199390809983015\n",
      "epoch: 19 trial 9147 training loss: 0.021126139909029007\n",
      "epoch: 19 trial 9148 training loss: 0.016137951985001564\n",
      "epoch: 19 trial 9149 training loss: 0.004891815595328808\n",
      "epoch: 19 trial 9150 training loss: 0.011337633710354567\n",
      "epoch: 19 trial 9151 training loss: 0.014665313065052032\n",
      "epoch: 19 trial 9152 training loss: 0.015182407107204199\n",
      "epoch: 19 trial 9153 training loss: 0.005412200465798378\n",
      "epoch: 19 trial 9154 training loss: 0.012938599102199078\n",
      "epoch: 19 trial 9155 training loss: 0.08601150289177895\n",
      "epoch: 19 trial 9156 training loss: 0.009967074729502201\n",
      "epoch: 19 trial 9157 training loss: 0.03381385188549757\n",
      "epoch: 19 trial 9158 training loss: 0.014108508359640837\n",
      "epoch: 19 trial 9159 training loss: 0.046231609769165516\n",
      "epoch: 19 trial 9160 training loss: 0.040617285296320915\n",
      "epoch: 19 trial 9161 training loss: 0.041740880347788334\n",
      "epoch: 19 trial 9162 training loss: 0.026405584532767534\n",
      "epoch: 19 trial 9163 training loss: 0.020369038451462984\n",
      "epoch: 19 trial 9164 training loss: 0.10394953936338425\n",
      "epoch: 19 trial 9165 training loss: 0.0054299295879900455\n",
      "epoch: 19 trial 9166 training loss: 0.017438197508454323\n",
      "epoch: 19 trial 9167 training loss: 0.13149326294660568\n",
      "epoch: 19 trial 9168 training loss: 0.025885642506182194\n",
      "epoch: 19 trial 9169 training loss: 0.020133123267441988\n",
      "epoch: 19 trial 9170 training loss: 0.018135258927941322\n",
      "epoch: 19 trial 9171 training loss: 0.01294117933139205\n",
      "epoch: 19 trial 9172 training loss: 0.025703578256070614\n",
      "epoch: 19 trial 9173 training loss: 0.013536602491512895\n",
      "epoch: 19 trial 9174 training loss: 0.055114151909947395\n",
      "epoch: 19 trial 9175 training loss: 0.1540183462202549\n",
      "epoch: 19 trial 9176 training loss: 0.07142936065793037\n",
      "epoch: 19 trial 9177 training loss: 0.0278082387521863\n",
      "epoch: 19 trial 9178 training loss: 0.06392828561365604\n",
      "epoch: 19 trial 9179 training loss: 0.09223408810794353\n",
      "epoch: 19 trial 9180 training loss: 0.026357056573033333\n",
      "epoch: 19 trial 9181 training loss: 0.07253255695104599\n",
      "epoch: 19 trial 9182 training loss: 0.007336829439736903\n",
      "epoch: 19 trial 9183 training loss: 0.038073232397437096\n",
      "epoch: 19 trial 9184 training loss: 0.012038966175168753\n",
      "epoch: 19 trial 9185 training loss: 0.028022661339491606\n",
      "epoch: 19 trial 9186 training loss: 0.10785765759646893\n",
      "epoch: 19 trial 9187 training loss: 0.04687725007534027\n",
      "epoch: 19 trial 9188 training loss: 0.020596665795892477\n",
      "epoch: 19 trial 9189 training loss: 0.01948656653985381\n",
      "epoch: 19 trial 9190 training loss: 0.07655370980501175\n",
      "epoch: 19 trial 9191 training loss: 0.01513680350035429\n",
      "epoch: 19 trial 9192 training loss: 0.056210026144981384\n",
      "epoch: 19 trial 9193 training loss: 0.03769874572753906\n",
      "epoch: 19 trial 9194 training loss: 0.04339977726340294\n",
      "epoch: 19 trial 9195 training loss: 0.01323280087672174\n",
      "epoch: 19 trial 9196 training loss: 0.052632746286690235\n",
      "epoch: 20 trial 9197 training loss: 0.013547804905101657\n",
      "epoch: 20 trial 9198 training loss: 0.01310269208624959\n",
      "epoch: 20 trial 9199 training loss: 0.011043125297874212\n",
      "epoch: 20 trial 9200 training loss: 0.06214526854455471\n",
      "epoch: 20 trial 9201 training loss: 0.004270103177987039\n",
      "epoch: 20 trial 9202 training loss: 0.005281342659145594\n",
      "epoch: 20 trial 9203 training loss: 0.024445575661957264\n",
      "epoch: 20 trial 9204 training loss: 0.030145134776830673\n",
      "epoch: 20 trial 9205 training loss: 0.004931858624331653\n",
      "epoch: 20 trial 9206 training loss: 0.014068445190787315\n",
      "epoch: 20 trial 9207 training loss: 0.05080618988722563\n",
      "epoch: 20 trial 9208 training loss: 0.03210076130926609\n",
      "epoch: 20 trial 9209 training loss: 0.014663724228739738\n",
      "epoch: 20 trial 9210 training loss: 0.017840625252574682\n",
      "epoch: 20 trial 9211 training loss: 0.02100800024345517\n",
      "epoch: 20 trial 9212 training loss: 0.022753868252038956\n",
      "epoch: 20 trial 9213 training loss: 0.1087623443454504\n",
      "epoch: 20 trial 9214 training loss: 0.04818164091557264\n",
      "epoch: 20 trial 9215 training loss: 0.016254854388535023\n",
      "epoch: 20 trial 9216 training loss: 0.01944612432271242\n",
      "epoch: 20 trial 9217 training loss: 0.017061824910342693\n",
      "epoch: 20 trial 9218 training loss: 0.07744698412716389\n",
      "epoch: 20 trial 9219 training loss: 0.012044503353536129\n",
      "epoch: 20 trial 9220 training loss: 0.0034001244930550456\n",
      "epoch: 20 trial 9221 training loss: 0.13412901014089584\n",
      "epoch: 20 trial 9222 training loss: 0.02820927370339632\n",
      "epoch: 20 trial 9223 training loss: 0.011043640552088618\n",
      "epoch: 20 trial 9224 training loss: 0.09715554118156433\n",
      "epoch: 20 trial 9225 training loss: 0.011655914131551981\n",
      "epoch: 20 trial 9226 training loss: 0.02048343885689974\n",
      "epoch: 20 trial 9227 training loss: 0.025074530858546495\n",
      "epoch: 20 trial 9228 training loss: 0.012592885410413146\n",
      "epoch: 20 trial 9229 training loss: 0.02799328789114952\n",
      "epoch: 20 trial 9230 training loss: 0.04142915224656463\n",
      "epoch: 20 trial 9231 training loss: 0.015793498139828444\n",
      "epoch: 20 trial 9232 training loss: 0.06551874056458473\n",
      "epoch: 20 trial 9233 training loss: 0.14146927371621132\n",
      "epoch: 20 trial 9234 training loss: 0.296326220035553\n",
      "epoch: 20 trial 9235 training loss: 0.046292755752801895\n",
      "epoch: 20 trial 9236 training loss: 0.08103739842772484\n",
      "epoch: 20 trial 9237 training loss: 0.09784360229969025\n",
      "epoch: 20 trial 9238 training loss: 0.011641107266768813\n",
      "epoch: 20 trial 9239 training loss: 0.07608604244887829\n",
      "epoch: 20 trial 9240 training loss: 0.036096914671361446\n",
      "epoch: 20 trial 9241 training loss: 0.025904903188347816\n",
      "epoch: 20 trial 9242 training loss: 0.05719165876507759\n",
      "epoch: 20 trial 9243 training loss: 0.01108963368460536\n",
      "epoch: 20 trial 9244 training loss: 0.02845052443444729\n",
      "epoch: 20 trial 9245 training loss: 0.01545263803564012\n",
      "epoch: 20 trial 9246 training loss: 0.09620505012571812\n",
      "epoch: 20 trial 9247 training loss: 0.03177445661276579\n",
      "epoch: 20 trial 9248 training loss: 0.11822380684316158\n",
      "epoch: 20 trial 9249 training loss: 0.03571247775107622\n",
      "epoch: 20 trial 9250 training loss: 0.040484409779310226\n",
      "epoch: 20 trial 9251 training loss: 0.03439923748373985\n",
      "epoch: 20 trial 9252 training loss: 0.059089330956339836\n",
      "epoch: 20 trial 9253 training loss: 0.026313296519219875\n",
      "epoch: 20 trial 9254 training loss: 0.0172636890783906\n",
      "epoch: 20 trial 9255 training loss: 0.02822686918079853\n",
      "epoch: 20 trial 9256 training loss: 0.08568798191845417\n",
      "epoch: 20 trial 9257 training loss: 0.02575154695659876\n",
      "epoch: 20 trial 9258 training loss: 0.12359471991658211\n",
      "epoch: 20 trial 9259 training loss: 0.017627635970711708\n",
      "epoch: 20 trial 9260 training loss: 0.003380699723493308\n",
      "epoch: 20 trial 9261 training loss: 0.05032328702509403\n",
      "epoch: 20 trial 9262 training loss: 0.052806345745921135\n",
      "epoch: 20 trial 9263 training loss: 0.02196314139291644\n",
      "epoch: 20 trial 9264 training loss: 0.021477322559803724\n",
      "epoch: 20 trial 9265 training loss: 0.05129762552678585\n",
      "epoch: 20 trial 9266 training loss: 0.04038189444690943\n",
      "epoch: 20 trial 9267 training loss: 0.042324382811784744\n",
      "epoch: 20 trial 9268 training loss: 0.037044549360871315\n",
      "epoch: 20 trial 9269 training loss: 0.01435915450565517\n",
      "epoch: 20 trial 9270 training loss: 0.06836124882102013\n",
      "epoch: 20 trial 9271 training loss: 0.01284407265484333\n",
      "epoch: 20 trial 9272 training loss: 0.024244718719273806\n",
      "epoch: 20 trial 9273 training loss: 0.03093406092375517\n",
      "epoch: 20 trial 9274 training loss: 0.08233793079853058\n",
      "epoch: 20 trial 9275 training loss: 0.017310629598796368\n",
      "epoch: 20 trial 9276 training loss: 0.05089770071208477\n",
      "epoch: 20 trial 9277 training loss: 0.02688174694776535\n",
      "epoch: 20 trial 9278 training loss: 0.04258606117218733\n",
      "epoch: 20 trial 9279 training loss: 0.04770132713019848\n",
      "epoch: 20 trial 9280 training loss: 0.02253456972539425\n",
      "epoch: 20 trial 9281 training loss: 0.06654687970876694\n",
      "epoch: 20 trial 9282 training loss: 0.015338156837970018\n",
      "epoch: 20 trial 9283 training loss: 0.027980318758636713\n",
      "epoch: 20 trial 9284 training loss: 0.05547022446990013\n",
      "epoch: 20 trial 9285 training loss: 0.055726331658661366\n",
      "epoch: 20 trial 9286 training loss: 0.005553035298362374\n",
      "epoch: 20 trial 9287 training loss: 0.038349506445229053\n",
      "epoch: 20 trial 9288 training loss: 0.02643656823784113\n",
      "epoch: 20 trial 9289 training loss: 0.02374969655647874\n",
      "epoch: 20 trial 9290 training loss: 0.0660233311355114\n",
      "epoch: 20 trial 9291 training loss: 0.04618699289858341\n",
      "epoch: 20 trial 9292 training loss: 0.05269086826592684\n",
      "epoch: 20 trial 9293 training loss: 0.009678389644250274\n",
      "epoch: 20 trial 9294 training loss: 0.083580382168293\n",
      "epoch: 20 trial 9295 training loss: 0.021353562828153372\n",
      "epoch: 20 trial 9296 training loss: 0.04874244146049023\n",
      "epoch: 20 trial 9297 training loss: 0.03066890873014927\n",
      "epoch: 20 trial 9298 training loss: 0.08864780329167843\n",
      "epoch: 20 trial 9299 training loss: 0.04646140895783901\n",
      "epoch: 20 trial 9300 training loss: 0.029346227645874023\n",
      "epoch: 20 trial 9301 training loss: 0.24353056401014328\n",
      "epoch: 20 trial 9302 training loss: 0.07216864824295044\n",
      "epoch: 20 trial 9303 training loss: 0.02961339708417654\n",
      "epoch: 20 trial 9304 training loss: 0.15122730284929276\n",
      "epoch: 20 trial 9305 training loss: 0.07472809590399265\n",
      "epoch: 20 trial 9306 training loss: 0.025313685182482004\n",
      "epoch: 20 trial 9307 training loss: 0.024555555544793606\n",
      "epoch: 20 trial 9308 training loss: 0.024952384177595377\n",
      "epoch: 20 trial 9309 training loss: 0.014496911549940705\n",
      "epoch: 20 trial 9310 training loss: 0.011181259993463755\n",
      "epoch: 20 trial 9311 training loss: 0.05812769569456577\n",
      "epoch: 20 trial 9312 training loss: 0.017566283233463764\n",
      "epoch: 20 trial 9313 training loss: 0.023710888344794512\n",
      "epoch: 20 trial 9314 training loss: 0.03665641508996487\n",
      "epoch: 20 trial 9315 training loss: 0.03041902557015419\n",
      "epoch: 20 trial 9316 training loss: 0.02987806685268879\n",
      "epoch: 20 trial 9317 training loss: 0.021175013855099678\n",
      "epoch: 20 trial 9318 training loss: 0.034795661456882954\n",
      "epoch: 20 trial 9319 training loss: 0.07767842151224613\n",
      "epoch: 20 trial 9320 training loss: 0.04049082286655903\n",
      "epoch: 20 trial 9321 training loss: 0.009914633817970753\n",
      "epoch: 20 trial 9322 training loss: 0.022597084287554026\n",
      "epoch: 20 trial 9323 training loss: 0.02290974697098136\n",
      "epoch: 20 trial 9324 training loss: 0.02558755222707987\n",
      "epoch: 20 trial 9325 training loss: 0.13787858933210373\n",
      "epoch: 20 trial 9326 training loss: 0.024557598866522312\n",
      "epoch: 20 trial 9327 training loss: 0.04038660507649183\n",
      "epoch: 20 trial 9328 training loss: 0.028961331583559513\n",
      "epoch: 20 trial 9329 training loss: 0.03108829353004694\n",
      "epoch: 20 trial 9330 training loss: 0.04387176316231489\n",
      "epoch: 20 trial 9331 training loss: 0.010454258881509304\n",
      "epoch: 20 trial 9332 training loss: 0.04564332216978073\n",
      "epoch: 20 trial 9333 training loss: 0.013617636170238256\n",
      "epoch: 20 trial 9334 training loss: 0.11256830580532551\n",
      "epoch: 20 trial 9335 training loss: 0.01917621400207281\n",
      "epoch: 20 trial 9336 training loss: 0.06420322693884373\n",
      "epoch: 20 trial 9337 training loss: 0.033121829852461815\n",
      "epoch: 20 trial 9338 training loss: 0.033727264031767845\n",
      "epoch: 20 trial 9339 training loss: 0.025913278106600046\n",
      "epoch: 20 trial 9340 training loss: 0.12539386004209518\n",
      "epoch: 20 trial 9341 training loss: 0.008373798569664359\n",
      "epoch: 20 trial 9342 training loss: 0.015648665372282267\n",
      "epoch: 20 trial 9343 training loss: 0.08193160220980644\n",
      "epoch: 20 trial 9344 training loss: 0.021063861902803183\n",
      "epoch: 20 trial 9345 training loss: 0.030052187852561474\n",
      "epoch: 20 trial 9346 training loss: 0.05211466923356056\n",
      "epoch: 20 trial 9347 training loss: 0.026840823236852884\n",
      "epoch: 20 trial 9348 training loss: 0.0194660066626966\n",
      "epoch: 20 trial 9349 training loss: 0.017677204217761755\n",
      "epoch: 20 trial 9350 training loss: 0.035374339669942856\n",
      "epoch: 20 trial 9351 training loss: 0.014435994438827038\n",
      "epoch: 20 trial 9352 training loss: 0.008325978880748153\n",
      "epoch: 20 trial 9353 training loss: 0.027399097569286823\n",
      "epoch: 20 trial 9354 training loss: 0.004345923545770347\n",
      "epoch: 20 trial 9355 training loss: 0.012117084115743637\n",
      "epoch: 20 trial 9356 training loss: 0.01922554662451148\n",
      "epoch: 20 trial 9357 training loss: 0.010673877783119678\n",
      "epoch: 20 trial 9358 training loss: 0.058047955855727196\n",
      "epoch: 20 trial 9359 training loss: 0.004585093760397285\n",
      "epoch: 20 trial 9360 training loss: 0.052997968159615993\n",
      "epoch: 20 trial 9361 training loss: 0.009902399266138673\n",
      "epoch: 20 trial 9362 training loss: 0.02163486974313855\n",
      "epoch: 20 trial 9363 training loss: 0.022729245480149984\n",
      "epoch: 20 trial 9364 training loss: 0.013809071853756905\n",
      "epoch: 20 trial 9365 training loss: 0.033700164407491684\n",
      "epoch: 20 trial 9366 training loss: 0.11902618408203125\n",
      "epoch: 20 trial 9367 training loss: 0.049867862835526466\n",
      "epoch: 20 trial 9368 training loss: 0.00812050118111074\n",
      "epoch: 20 trial 9369 training loss: 0.06781543791294098\n",
      "epoch: 20 trial 9370 training loss: 0.03202705550938845\n",
      "epoch: 20 trial 9371 training loss: 0.024164543487131596\n",
      "epoch: 20 trial 9372 training loss: 0.016122137545607984\n",
      "epoch: 20 trial 9373 training loss: 0.025469019543379545\n",
      "epoch: 20 trial 9374 training loss: 0.03303094021975994\n",
      "epoch: 20 trial 9375 training loss: 0.03830243553966284\n",
      "epoch: 20 trial 9376 training loss: 0.03374201664701104\n",
      "epoch: 20 trial 9377 training loss: 0.07700075022876263\n",
      "epoch: 20 trial 9378 training loss: 0.00775187136605382\n",
      "epoch: 20 trial 9379 training loss: 0.05736857280135155\n",
      "epoch: 20 trial 9380 training loss: 0.019625897984951735\n",
      "epoch: 20 trial 9381 training loss: 0.018150828778743744\n",
      "epoch: 20 trial 9382 training loss: 0.04453799407929182\n",
      "epoch: 20 trial 9383 training loss: 0.04963099118322134\n",
      "epoch: 20 trial 9384 training loss: 0.01644901931285858\n",
      "epoch: 20 trial 9385 training loss: 0.03739297762513161\n",
      "epoch: 20 trial 9386 training loss: 0.07599652744829655\n",
      "epoch: 20 trial 9387 training loss: 0.07219000160694122\n",
      "epoch: 20 trial 9388 training loss: 0.015973117668181658\n",
      "epoch: 20 trial 9389 training loss: 0.021437549963593483\n",
      "epoch: 20 trial 9390 training loss: 0.014428268885239959\n",
      "epoch: 20 trial 9391 training loss: 0.023381761740893126\n",
      "epoch: 20 trial 9392 training loss: 0.03813735023140907\n",
      "epoch: 20 trial 9393 training loss: 0.027742599369958043\n",
      "epoch: 20 trial 9394 training loss: 0.07041944190859795\n",
      "epoch: 20 trial 9395 training loss: 0.11056465655565262\n",
      "epoch: 20 trial 9396 training loss: 0.015484023839235306\n",
      "epoch: 20 trial 9397 training loss: 0.006034990074113011\n",
      "epoch: 20 trial 9398 training loss: 0.008203472010791302\n",
      "epoch: 20 trial 9399 training loss: 0.011934711132198572\n",
      "epoch: 20 trial 9400 training loss: 0.01527770422399044\n",
      "epoch: 20 trial 9401 training loss: 0.021497483365237713\n",
      "epoch: 20 trial 9402 training loss: 0.09380743280053139\n",
      "epoch: 20 trial 9403 training loss: 0.032083242200315\n",
      "epoch: 20 trial 9404 training loss: 0.014391975942999125\n",
      "epoch: 20 trial 9405 training loss: 0.08809549175202847\n",
      "epoch: 20 trial 9406 training loss: 0.04502240661531687\n",
      "epoch: 20 trial 9407 training loss: 0.05982143338769674\n",
      "epoch: 20 trial 9408 training loss: 0.06442351266741753\n",
      "epoch: 20 trial 9409 training loss: 0.09672389551997185\n",
      "epoch: 20 trial 9410 training loss: 0.01397137250751257\n",
      "epoch: 20 trial 9411 training loss: 0.00749185704626143\n",
      "epoch: 20 trial 9412 training loss: 0.025199844036251307\n",
      "epoch: 20 trial 9413 training loss: 0.022394363768398762\n",
      "epoch: 20 trial 9414 training loss: 0.02057094732299447\n",
      "epoch: 20 trial 9415 training loss: 0.052997684106230736\n",
      "epoch: 20 trial 9416 training loss: 0.07944680750370026\n",
      "epoch: 20 trial 9417 training loss: 0.03110236395150423\n",
      "epoch: 20 trial 9418 training loss: 0.043593031354248524\n",
      "epoch: 20 trial 9419 training loss: 0.04059999156743288\n",
      "epoch: 20 trial 9420 training loss: 0.09118596464395523\n",
      "epoch: 20 trial 9421 training loss: 0.013224317459389567\n",
      "epoch: 20 trial 9422 training loss: 0.013845863519236445\n",
      "epoch: 20 trial 9423 training loss: 0.02222289564087987\n",
      "epoch: 20 trial 9424 training loss: 0.022771520540118217\n",
      "epoch: 20 trial 9425 training loss: 0.04575160704553127\n",
      "epoch: 20 trial 9426 training loss: 0.014491960406303406\n",
      "epoch: 20 trial 9427 training loss: 0.1652005836367607\n",
      "epoch: 20 trial 9428 training loss: 0.01996342372149229\n",
      "epoch: 20 trial 9429 training loss: 0.10151083394885063\n",
      "epoch: 20 trial 9430 training loss: 0.09093916974961758\n",
      "epoch: 20 trial 9431 training loss: 0.013702326687052846\n",
      "epoch: 20 trial 9432 training loss: 0.06392976269125938\n",
      "epoch: 20 trial 9433 training loss: 0.014855376444756985\n",
      "epoch: 20 trial 9434 training loss: 0.04471589718014002\n",
      "epoch: 20 trial 9435 training loss: 0.04431919474154711\n",
      "epoch: 20 trial 9436 training loss: 0.07255140878260136\n",
      "epoch: 20 trial 9437 training loss: 0.01587270712479949\n",
      "epoch: 20 trial 9438 training loss: 0.039000438526272774\n",
      "epoch: 20 trial 9439 training loss: 0.03502580337226391\n",
      "epoch: 20 trial 9440 training loss: 0.029384420718997717\n",
      "epoch: 20 trial 9441 training loss: 0.06016327068209648\n",
      "epoch: 20 trial 9442 training loss: 0.017851976677775383\n",
      "epoch: 20 trial 9443 training loss: 0.03140780795365572\n",
      "epoch: 20 trial 9444 training loss: 0.01952762668952346\n",
      "epoch: 20 trial 9445 training loss: 0.023415525909513235\n",
      "epoch: 20 trial 9446 training loss: 0.04067326430231333\n",
      "epoch: 20 trial 9447 training loss: 0.01159117161296308\n",
      "epoch: 20 trial 9448 training loss: 0.007014561677351594\n",
      "epoch: 20 trial 9449 training loss: 0.03382169734686613\n",
      "epoch: 20 trial 9450 training loss: 0.08530650660395622\n",
      "epoch: 20 trial 9451 training loss: 0.020644107833504677\n",
      "epoch: 20 trial 9452 training loss: 0.08816755563020706\n",
      "epoch: 20 trial 9453 training loss: 0.05076041165739298\n",
      "epoch: 20 trial 9454 training loss: 0.02923422772437334\n",
      "epoch: 20 trial 9455 training loss: 0.1322711780667305\n",
      "epoch: 20 trial 9456 training loss: 0.012597380438819528\n",
      "epoch: 20 trial 9457 training loss: 0.012257687281817198\n",
      "epoch: 20 trial 9458 training loss: 0.009744344046339393\n",
      "epoch: 20 trial 9459 training loss: 0.003824423416517675\n",
      "epoch: 20 trial 9460 training loss: 0.031239059753715992\n",
      "epoch: 20 trial 9461 training loss: 0.0313986437395215\n",
      "epoch: 20 trial 9462 training loss: 0.05190852005034685\n",
      "epoch: 20 trial 9463 training loss: 0.03134756814688444\n",
      "epoch: 20 trial 9464 training loss: 0.042228356935083866\n",
      "epoch: 20 trial 9465 training loss: 0.019693768583238125\n",
      "epoch: 20 trial 9466 training loss: 0.07828155532479286\n",
      "epoch: 20 trial 9467 training loss: 0.017280970700085163\n",
      "epoch: 20 trial 9468 training loss: 0.007696409709751606\n",
      "epoch: 20 trial 9469 training loss: 0.020988248754292727\n",
      "epoch: 20 trial 9470 training loss: 0.005200836341828108\n",
      "epoch: 20 trial 9471 training loss: 0.0685105798766017\n",
      "epoch: 20 trial 9472 training loss: 0.0027777364011853933\n",
      "epoch: 20 trial 9473 training loss: 0.022348144557327032\n",
      "epoch: 20 trial 9474 training loss: 0.016865455079823732\n",
      "epoch: 20 trial 9475 training loss: 0.05972994677722454\n",
      "epoch: 20 trial 9476 training loss: 0.040206875652074814\n",
      "epoch: 20 trial 9477 training loss: 0.022183708380907774\n",
      "epoch: 20 trial 9478 training loss: 0.03073391178622842\n",
      "epoch: 20 trial 9479 training loss: 0.06297051534056664\n",
      "epoch: 20 trial 9480 training loss: 0.026473840698599815\n",
      "epoch: 20 trial 9481 training loss: 0.07380375266075134\n",
      "epoch: 20 trial 9482 training loss: 0.017674287781119347\n",
      "epoch: 20 trial 9483 training loss: 0.06400488503277302\n",
      "epoch: 20 trial 9484 training loss: 0.020378060173243284\n",
      "epoch: 20 trial 9485 training loss: 0.007853828137740493\n",
      "epoch: 20 trial 9486 training loss: 0.04782242048531771\n",
      "epoch: 20 trial 9487 training loss: 0.02798618096858263\n",
      "epoch: 20 trial 9488 training loss: 0.01639376813545823\n",
      "epoch: 20 trial 9489 training loss: 0.01946090767160058\n",
      "epoch: 20 trial 9490 training loss: 0.010229568928480148\n",
      "epoch: 20 trial 9491 training loss: 0.026951679959893227\n",
      "epoch: 20 trial 9492 training loss: 0.03800857253372669\n",
      "epoch: 20 trial 9493 training loss: 0.01517878519371152\n",
      "epoch: 20 trial 9494 training loss: 0.030157777946442366\n",
      "epoch: 20 trial 9495 training loss: 0.08598638884723186\n",
      "epoch: 20 trial 9496 training loss: 0.03935924358665943\n",
      "epoch: 20 trial 9497 training loss: 0.012356026796624064\n",
      "epoch: 20 trial 9498 training loss: 0.004934502299875021\n",
      "epoch: 20 trial 9499 training loss: 0.00737376045435667\n",
      "epoch: 20 trial 9500 training loss: 0.028671301901340485\n",
      "epoch: 20 trial 9501 training loss: 0.04813501890748739\n",
      "epoch: 20 trial 9502 training loss: 0.0451020672917366\n",
      "epoch: 20 trial 9503 training loss: 0.0032825584057718515\n",
      "epoch: 20 trial 9504 training loss: 0.033663393929600716\n",
      "epoch: 20 trial 9505 training loss: 0.0289226146414876\n",
      "epoch: 20 trial 9506 training loss: 0.019939680583775043\n",
      "epoch: 20 trial 9507 training loss: 0.014307847479358315\n",
      "epoch: 20 trial 9508 training loss: 0.019045439548790455\n",
      "epoch: 20 trial 9509 training loss: 0.037829562090337276\n",
      "epoch: 20 trial 9510 training loss: 0.025389614515006542\n",
      "epoch: 20 trial 9511 training loss: 0.03213173896074295\n",
      "epoch: 20 trial 9512 training loss: 0.07122869975864887\n",
      "epoch: 20 trial 9513 training loss: 0.020617011934518814\n",
      "epoch: 20 trial 9514 training loss: 0.032290187664330006\n",
      "epoch: 20 trial 9515 training loss: 0.06259028427302837\n",
      "epoch: 20 trial 9516 training loss: 0.031577552668750286\n",
      "epoch: 20 trial 9517 training loss: 0.06294819340109825\n",
      "epoch: 20 trial 9518 training loss: 0.0177867547608912\n",
      "epoch: 20 trial 9519 training loss: 0.033681388944387436\n",
      "epoch: 20 trial 9520 training loss: 0.025970507878810167\n",
      "epoch: 20 trial 9521 training loss: 0.022155754268169403\n",
      "epoch: 20 trial 9522 training loss: 0.026526832953095436\n",
      "epoch: 20 trial 9523 training loss: 0.09194942563772202\n",
      "epoch: 20 trial 9524 training loss: 0.013579349964857101\n",
      "epoch: 20 trial 9525 training loss: 0.08096328563988209\n",
      "epoch: 20 trial 9526 training loss: 0.031164972111582756\n",
      "epoch: 20 trial 9527 training loss: 0.007226291578263044\n",
      "epoch: 20 trial 9528 training loss: 0.05338269658386707\n",
      "epoch: 20 trial 9529 training loss: 0.06066649779677391\n",
      "epoch: 20 trial 9530 training loss: 0.01127905142493546\n",
      "epoch: 20 trial 9531 training loss: 0.0363996597006917\n",
      "epoch: 20 trial 9532 training loss: 0.010314374463632703\n",
      "epoch: 20 trial 9533 training loss: 0.03142662066966295\n",
      "epoch: 20 trial 9534 training loss: 0.022306109312921762\n",
      "epoch: 20 trial 9535 training loss: 0.042873699218034744\n",
      "epoch: 20 trial 9536 training loss: 0.2741207927465439\n",
      "epoch: 20 trial 9537 training loss: 0.05614624358713627\n",
      "epoch: 20 trial 9538 training loss: 0.009576242882758379\n",
      "epoch: 20 trial 9539 training loss: 0.015458845999091864\n",
      "epoch: 20 trial 9540 training loss: 0.1266789324581623\n",
      "epoch: 20 trial 9541 training loss: 0.048717692494392395\n",
      "epoch: 20 trial 9542 training loss: 0.08110571093857288\n",
      "epoch: 20 trial 9543 training loss: 0.05445439554750919\n",
      "epoch: 20 trial 9544 training loss: 0.030758613720536232\n",
      "epoch: 20 trial 9545 training loss: 0.010048317722976208\n",
      "epoch: 20 trial 9546 training loss: 0.06845077313482761\n",
      "epoch: 20 trial 9547 training loss: 0.03318419028073549\n",
      "epoch: 20 trial 9548 training loss: 0.09220198355615139\n",
      "epoch: 20 trial 9549 training loss: 0.011213239515200257\n",
      "epoch: 20 trial 9550 training loss: 0.005355756147764623\n",
      "epoch: 20 trial 9551 training loss: 0.02565074898302555\n",
      "epoch: 20 trial 9552 training loss: 0.006385233486071229\n",
      "epoch: 20 trial 9553 training loss: 0.014002681476995349\n",
      "epoch: 20 trial 9554 training loss: 0.09596684947609901\n",
      "epoch: 20 trial 9555 training loss: 0.07388564385473728\n",
      "epoch: 20 trial 9556 training loss: 0.028368069790303707\n",
      "epoch: 20 trial 9557 training loss: 0.11649910360574722\n",
      "epoch: 20 trial 9558 training loss: 0.08439013548195362\n",
      "epoch: 20 trial 9559 training loss: 0.02352676074951887\n",
      "epoch: 20 trial 9560 training loss: 0.060095289722085\n",
      "epoch: 20 trial 9561 training loss: 0.036566756665706635\n",
      "epoch: 20 trial 9562 training loss: 0.01443422632291913\n",
      "epoch: 20 trial 9563 training loss: 0.00521224329713732\n",
      "epoch: 20 trial 9564 training loss: 0.12180178612470627\n",
      "epoch: 20 trial 9565 training loss: 0.06443552114069462\n",
      "epoch: 20 trial 9566 training loss: 0.05055177677422762\n",
      "epoch: 20 trial 9567 training loss: 0.054950153455138206\n",
      "epoch: 20 trial 9568 training loss: 0.024431154131889343\n",
      "epoch: 20 trial 9569 training loss: 0.036251298151910305\n",
      "epoch: 20 trial 9570 training loss: 0.014523658901453018\n",
      "epoch: 20 trial 9571 training loss: 0.04786996729671955\n",
      "epoch: 20 trial 9572 training loss: 0.026877397671341896\n",
      "epoch: 20 trial 9573 training loss: 0.035375322215259075\n",
      "epoch: 20 trial 9574 training loss: 0.08253825269639492\n",
      "epoch: 20 trial 9575 training loss: 0.023008843418210745\n",
      "epoch: 20 trial 9576 training loss: 0.011463176226243377\n",
      "epoch: 20 trial 9577 training loss: 0.033715139143168926\n",
      "epoch: 20 trial 9578 training loss: 0.005031323293223977\n",
      "epoch: 20 trial 9579 training loss: 0.03342671040445566\n",
      "epoch: 20 trial 9580 training loss: 0.038699109107255936\n",
      "epoch: 20 trial 9581 training loss: 0.012491995003074408\n",
      "epoch: 20 trial 9582 training loss: 0.0022242373088374734\n",
      "epoch: 20 trial 9583 training loss: 0.0032355700968764722\n",
      "epoch: 20 trial 9584 training loss: 0.0176190291531384\n",
      "epoch: 20 trial 9585 training loss: 0.0351385734975338\n",
      "epoch: 20 trial 9586 training loss: 0.0037489731330424547\n",
      "epoch: 20 trial 9587 training loss: 0.004748689127154648\n",
      "epoch: 20 trial 9588 training loss: 0.006845062482170761\n",
      "epoch: 20 trial 9589 training loss: 0.008296907180920243\n",
      "epoch: 20 trial 9590 training loss: 0.010851128492504358\n",
      "epoch: 20 trial 9591 training loss: 0.02333975350484252\n",
      "epoch: 20 trial 9592 training loss: 0.10705598443746567\n",
      "epoch: 20 trial 9593 training loss: 0.017692540306597948\n",
      "epoch: 20 trial 9594 training loss: 0.017013613134622574\n",
      "epoch: 20 trial 9595 training loss: 0.026584405917674303\n",
      "epoch: 20 trial 9596 training loss: 0.038309856317937374\n",
      "epoch: 20 trial 9597 training loss: 0.036021362990140915\n",
      "epoch: 20 trial 9598 training loss: 0.03158691478893161\n",
      "epoch: 20 trial 9599 training loss: 0.04463315289467573\n",
      "epoch: 20 trial 9600 training loss: 0.07350525073707104\n",
      "epoch: 20 trial 9601 training loss: 0.11042746156454086\n",
      "epoch: 20 trial 9602 training loss: 0.021486039739102125\n",
      "epoch: 20 trial 9603 training loss: 0.02657685847952962\n",
      "epoch: 20 trial 9604 training loss: 0.08297953940927982\n",
      "epoch: 20 trial 9605 training loss: 0.019761479925364256\n",
      "epoch: 20 trial 9606 training loss: 0.05215654522180557\n",
      "epoch: 20 trial 9607 training loss: 0.09532908163964748\n",
      "epoch: 20 trial 9608 training loss: 0.09381838142871857\n",
      "epoch: 20 trial 9609 training loss: 0.04180884175002575\n",
      "epoch: 20 trial 9610 training loss: 0.10663173161447048\n",
      "epoch: 20 trial 9611 training loss: 0.020575128961354494\n",
      "epoch: 20 trial 9612 training loss: 0.04696807451546192\n",
      "epoch: 20 trial 9613 training loss: 0.03108133003115654\n",
      "epoch: 20 trial 9614 training loss: 0.10722481459379196\n",
      "epoch: 20 trial 9615 training loss: 0.008060054387897253\n",
      "epoch: 20 trial 9616 training loss: 0.00722947483882308\n",
      "epoch: 20 trial 9617 training loss: 0.018982368987053633\n",
      "epoch: 20 trial 9618 training loss: 0.027263562195003033\n",
      "epoch: 20 trial 9619 training loss: 0.05471871793270111\n",
      "epoch: 20 trial 9620 training loss: 0.05638897232711315\n",
      "epoch: 20 trial 9621 training loss: 0.016086373943835497\n",
      "epoch: 20 trial 9622 training loss: 0.05548481643199921\n",
      "epoch: 20 trial 9623 training loss: 0.014146655332297087\n",
      "epoch: 20 trial 9624 training loss: 0.01570125063881278\n",
      "epoch: 20 trial 9625 training loss: 0.02203361364081502\n",
      "epoch: 20 trial 9626 training loss: 0.014271064661443233\n",
      "epoch: 20 trial 9627 training loss: 0.01967337680980563\n",
      "epoch: 20 trial 9628 training loss: 0.04108468350023031\n",
      "epoch: 20 trial 9629 training loss: 0.02610307466238737\n",
      "epoch: 20 trial 9630 training loss: 0.023724630940705538\n",
      "epoch: 20 trial 9631 training loss: 0.022566167172044516\n",
      "epoch: 20 trial 9632 training loss: 0.016582980751991272\n",
      "epoch: 20 trial 9633 training loss: 0.004768895101733506\n",
      "epoch: 20 trial 9634 training loss: 0.011058833682909608\n",
      "epoch: 20 trial 9635 training loss: 0.015130744315683842\n",
      "epoch: 20 trial 9636 training loss: 0.01441933261230588\n",
      "epoch: 20 trial 9637 training loss: 0.006096455035731196\n",
      "epoch: 20 trial 9638 training loss: 0.013112387619912624\n",
      "epoch: 20 trial 9639 training loss: 0.08482430130243301\n",
      "epoch: 20 trial 9640 training loss: 0.010401508072391152\n",
      "epoch: 20 trial 9641 training loss: 0.03498878888785839\n",
      "epoch: 20 trial 9642 training loss: 0.015010862611234188\n",
      "epoch: 20 trial 9643 training loss: 0.04712163470685482\n",
      "epoch: 20 trial 9644 training loss: 0.04384350311011076\n",
      "epoch: 20 trial 9645 training loss: 0.038900951854884624\n",
      "epoch: 20 trial 9646 training loss: 0.027108557522296906\n",
      "epoch: 20 trial 9647 training loss: 0.020999227184802294\n",
      "epoch: 20 trial 9648 training loss: 0.09461396560072899\n",
      "epoch: 20 trial 9649 training loss: 0.005006787250749767\n",
      "epoch: 20 trial 9650 training loss: 0.018285099882632494\n",
      "epoch: 20 trial 9651 training loss: 0.12110988795757294\n",
      "epoch: 20 trial 9652 training loss: 0.025199665687978268\n",
      "epoch: 20 trial 9653 training loss: 0.020205683540552855\n",
      "epoch: 20 trial 9654 training loss: 0.018541309516876936\n",
      "epoch: 20 trial 9655 training loss: 0.01241212384775281\n",
      "epoch: 20 trial 9656 training loss: 0.028550698421895504\n",
      "epoch: 20 trial 9657 training loss: 0.01397949131205678\n",
      "epoch: 20 trial 9658 training loss: 0.0581704955548048\n",
      "epoch: 20 trial 9659 training loss: 0.15447105467319489\n",
      "epoch: 20 trial 9660 training loss: 0.06905635446310043\n",
      "epoch: 20 trial 9661 training loss: 0.0284212501719594\n",
      "epoch: 20 trial 9662 training loss: 0.06356359831988811\n",
      "epoch: 20 trial 9663 training loss: 0.09724773094058037\n",
      "epoch: 20 trial 9664 training loss: 0.02570003690198064\n",
      "epoch: 20 trial 9665 training loss: 0.07132381945848465\n",
      "epoch: 20 trial 9666 training loss: 0.007779760635457933\n",
      "epoch: 20 trial 9667 training loss: 0.040935798548161983\n",
      "epoch: 20 trial 9668 training loss: 0.012750549707561731\n",
      "epoch: 20 trial 9669 training loss: 0.028484522365033627\n",
      "epoch: 20 trial 9670 training loss: 0.11119746789336205\n",
      "epoch: 20 trial 9671 training loss: 0.04689277522265911\n",
      "epoch: 20 trial 9672 training loss: 0.01892157760448754\n",
      "epoch: 20 trial 9673 training loss: 0.01996123418211937\n",
      "epoch: 20 trial 9674 training loss: 0.07065148092806339\n",
      "epoch: 20 trial 9675 training loss: 0.015255360398441553\n",
      "epoch: 20 trial 9676 training loss: 0.05433553829789162\n",
      "epoch: 20 trial 9677 training loss: 0.03432950284332037\n",
      "epoch: 20 trial 9678 training loss: 0.04324045591056347\n",
      "epoch: 20 trial 9679 training loss: 0.01378147816285491\n",
      "epoch: 20 trial 9680 training loss: 0.05135359521955252\n",
      "epoch: 21 trial 9681 training loss: 0.014166933484375477\n",
      "epoch: 21 trial 9682 training loss: 0.012317660730332136\n",
      "epoch: 21 trial 9683 training loss: 0.010973835363984108\n",
      "epoch: 21 trial 9684 training loss: 0.05532685108482838\n",
      "epoch: 21 trial 9685 training loss: 0.004382908809930086\n",
      "epoch: 21 trial 9686 training loss: 0.005540485959500074\n",
      "epoch: 21 trial 9687 training loss: 0.026058193296194077\n",
      "epoch: 21 trial 9688 training loss: 0.03055385034531355\n",
      "epoch: 21 trial 9689 training loss: 0.006006882060319185\n",
      "epoch: 21 trial 9690 training loss: 0.013701495714485645\n",
      "epoch: 21 trial 9691 training loss: 0.0523454574868083\n",
      "epoch: 21 trial 9692 training loss: 0.03151353448629379\n",
      "epoch: 21 trial 9693 training loss: 0.016807264648377895\n",
      "epoch: 21 trial 9694 training loss: 0.01764557557180524\n",
      "epoch: 21 trial 9695 training loss: 0.021284725982695818\n",
      "epoch: 21 trial 9696 training loss: 0.02226895373314619\n",
      "epoch: 21 trial 9697 training loss: 0.11101946607232094\n",
      "epoch: 21 trial 9698 training loss: 0.0500178886577487\n",
      "epoch: 21 trial 9699 training loss: 0.016938365064561367\n",
      "epoch: 21 trial 9700 training loss: 0.020702404901385307\n",
      "epoch: 21 trial 9701 training loss: 0.016574968118220568\n",
      "epoch: 21 trial 9702 training loss: 0.0732386875897646\n",
      "epoch: 21 trial 9703 training loss: 0.012752631213515997\n",
      "epoch: 21 trial 9704 training loss: 0.0034564801026135683\n",
      "epoch: 21 trial 9705 training loss: 0.1375348251312971\n",
      "epoch: 21 trial 9706 training loss: 0.02585717849433422\n",
      "epoch: 21 trial 9707 training loss: 0.01199240330606699\n",
      "epoch: 21 trial 9708 training loss: 0.10297186858952045\n",
      "epoch: 21 trial 9709 training loss: 0.012787038926035166\n",
      "epoch: 21 trial 9710 training loss: 0.01794012077152729\n",
      "epoch: 21 trial 9711 training loss: 0.025719987228512764\n",
      "epoch: 21 trial 9712 training loss: 0.012060079257935286\n",
      "epoch: 21 trial 9713 training loss: 0.028425509110093117\n",
      "epoch: 21 trial 9714 training loss: 0.04626394249498844\n",
      "epoch: 21 trial 9715 training loss: 0.015450186561793089\n",
      "epoch: 21 trial 9716 training loss: 0.06294323690235615\n",
      "epoch: 21 trial 9717 training loss: 0.1525096334517002\n",
      "epoch: 21 trial 9718 training loss: 0.28736869990825653\n",
      "epoch: 21 trial 9719 training loss: 0.044781274162232876\n",
      "epoch: 21 trial 9720 training loss: 0.08701964095234871\n",
      "epoch: 21 trial 9721 training loss: 0.1010183859616518\n",
      "epoch: 21 trial 9722 training loss: 0.01107315463013947\n",
      "epoch: 21 trial 9723 training loss: 0.07699992693960667\n",
      "epoch: 21 trial 9724 training loss: 0.03533331956714392\n",
      "epoch: 21 trial 9725 training loss: 0.023176762275397778\n",
      "epoch: 21 trial 9726 training loss: 0.061684293672442436\n",
      "epoch: 21 trial 9727 training loss: 0.010362562257796526\n",
      "epoch: 21 trial 9728 training loss: 0.028228825889527798\n",
      "epoch: 21 trial 9729 training loss: 0.01563905831426382\n",
      "epoch: 21 trial 9730 training loss: 0.10199666768312454\n",
      "epoch: 21 trial 9731 training loss: 0.03527480736374855\n",
      "epoch: 21 trial 9732 training loss: 0.10669538378715515\n",
      "epoch: 21 trial 9733 training loss: 0.04128870088607073\n",
      "epoch: 21 trial 9734 training loss: 0.04264400992542505\n",
      "epoch: 21 trial 9735 training loss: 0.03540345933288336\n",
      "epoch: 21 trial 9736 training loss: 0.062064697965979576\n",
      "epoch: 21 trial 9737 training loss: 0.024420898873358965\n",
      "epoch: 21 trial 9738 training loss: 0.016786969266831875\n",
      "epoch: 21 trial 9739 training loss: 0.030140025541186333\n",
      "epoch: 21 trial 9740 training loss: 0.08623369224369526\n",
      "epoch: 21 trial 9741 training loss: 0.02527415519580245\n",
      "epoch: 21 trial 9742 training loss: 0.11994431167840958\n",
      "epoch: 21 trial 9743 training loss: 0.01586459344252944\n",
      "epoch: 21 trial 9744 training loss: 0.0024839954858180135\n",
      "epoch: 21 trial 9745 training loss: 0.051483793184161186\n",
      "epoch: 21 trial 9746 training loss: 0.05531403236091137\n",
      "epoch: 21 trial 9747 training loss: 0.02314471360296011\n",
      "epoch: 21 trial 9748 training loss: 0.02152694296091795\n",
      "epoch: 21 trial 9749 training loss: 0.052172351628541946\n",
      "epoch: 21 trial 9750 training loss: 0.039672836661338806\n",
      "epoch: 21 trial 9751 training loss: 0.04544129595160484\n",
      "epoch: 21 trial 9752 training loss: 0.03403969015926123\n",
      "epoch: 21 trial 9753 training loss: 0.01322369952686131\n",
      "epoch: 21 trial 9754 training loss: 0.06680143624544144\n",
      "epoch: 21 trial 9755 training loss: 0.014282193966209888\n",
      "epoch: 21 trial 9756 training loss: 0.02347793849185109\n",
      "epoch: 21 trial 9757 training loss: 0.0329428119584918\n",
      "epoch: 21 trial 9758 training loss: 0.07610583864152431\n",
      "epoch: 21 trial 9759 training loss: 0.018209558445960283\n",
      "epoch: 21 trial 9760 training loss: 0.05196397565305233\n",
      "epoch: 21 trial 9761 training loss: 0.02776758885011077\n",
      "epoch: 21 trial 9762 training loss: 0.045384328812360764\n",
      "epoch: 21 trial 9763 training loss: 0.04156462009996176\n",
      "epoch: 21 trial 9764 training loss: 0.028970735613256693\n",
      "epoch: 21 trial 9765 training loss: 0.06887627951800823\n",
      "epoch: 21 trial 9766 training loss: 0.015553676523268223\n",
      "epoch: 21 trial 9767 training loss: 0.027373474091291428\n",
      "epoch: 21 trial 9768 training loss: 0.056027865037322044\n",
      "epoch: 21 trial 9769 training loss: 0.056865522637963295\n",
      "epoch: 21 trial 9770 training loss: 0.006609054165892303\n",
      "epoch: 21 trial 9771 training loss: 0.04175195936113596\n",
      "epoch: 21 trial 9772 training loss: 0.02493647113442421\n",
      "epoch: 21 trial 9773 training loss: 0.02149318065494299\n",
      "epoch: 21 trial 9774 training loss: 0.06831629015505314\n",
      "epoch: 21 trial 9775 training loss: 0.04995447024703026\n",
      "epoch: 21 trial 9776 training loss: 0.05535076092928648\n",
      "epoch: 21 trial 9777 training loss: 0.009996301028877497\n",
      "epoch: 21 trial 9778 training loss: 0.08401111327111721\n",
      "epoch: 21 trial 9779 training loss: 0.023330665659159422\n",
      "epoch: 21 trial 9780 training loss: 0.04525469709187746\n",
      "epoch: 21 trial 9781 training loss: 0.029365023598074913\n",
      "epoch: 21 trial 9782 training loss: 0.09790977835655212\n",
      "epoch: 21 trial 9783 training loss: 0.04763445630669594\n",
      "epoch: 21 trial 9784 training loss: 0.03068697452545166\n",
      "epoch: 21 trial 9785 training loss: 0.23001597076654434\n",
      "epoch: 21 trial 9786 training loss: 0.07739493437111378\n",
      "epoch: 21 trial 9787 training loss: 0.02617140579968691\n",
      "epoch: 21 trial 9788 training loss: 0.15435753762722015\n",
      "epoch: 21 trial 9789 training loss: 0.07838524878025055\n",
      "epoch: 21 trial 9790 training loss: 0.02705263812094927\n",
      "epoch: 21 trial 9791 training loss: 0.02287867432460189\n",
      "epoch: 21 trial 9792 training loss: 0.024351959582418203\n",
      "epoch: 21 trial 9793 training loss: 0.015310805290937424\n",
      "epoch: 21 trial 9794 training loss: 0.010084080742672086\n",
      "epoch: 21 trial 9795 training loss: 0.05915582366287708\n",
      "epoch: 21 trial 9796 training loss: 0.018447631504386663\n",
      "epoch: 21 trial 9797 training loss: 0.026461678557097912\n",
      "epoch: 21 trial 9798 training loss: 0.036468169651925564\n",
      "epoch: 21 trial 9799 training loss: 0.03227097541093826\n",
      "epoch: 21 trial 9800 training loss: 0.029891932383179665\n",
      "epoch: 21 trial 9801 training loss: 0.020534187089651823\n",
      "epoch: 21 trial 9802 training loss: 0.037038302049040794\n",
      "epoch: 21 trial 9803 training loss: 0.07761471159756184\n",
      "epoch: 21 trial 9804 training loss: 0.03551872819662094\n",
      "epoch: 21 trial 9805 training loss: 0.00943829258903861\n",
      "epoch: 21 trial 9806 training loss: 0.02099450957030058\n",
      "epoch: 21 trial 9807 training loss: 0.02376176370307803\n",
      "epoch: 21 trial 9808 training loss: 0.02593074645847082\n",
      "epoch: 21 trial 9809 training loss: 0.1371854953467846\n",
      "epoch: 21 trial 9810 training loss: 0.024598867166787386\n",
      "epoch: 21 trial 9811 training loss: 0.03762504644691944\n",
      "epoch: 21 trial 9812 training loss: 0.030473717488348484\n",
      "epoch: 21 trial 9813 training loss: 0.03373188525438309\n",
      "epoch: 21 trial 9814 training loss: 0.040744535624980927\n",
      "epoch: 21 trial 9815 training loss: 0.01129117188975215\n",
      "epoch: 21 trial 9816 training loss: 0.04481831472367048\n",
      "epoch: 21 trial 9817 training loss: 0.012695577694103122\n",
      "epoch: 21 trial 9818 training loss: 0.11550438031554222\n",
      "epoch: 21 trial 9819 training loss: 0.017717123497277498\n",
      "epoch: 21 trial 9820 training loss: 0.06293919030576944\n",
      "epoch: 21 trial 9821 training loss: 0.03641171660274267\n",
      "epoch: 21 trial 9822 training loss: 0.032453808933496475\n",
      "epoch: 21 trial 9823 training loss: 0.02654624404385686\n",
      "epoch: 21 trial 9824 training loss: 0.11610463261604309\n",
      "epoch: 21 trial 9825 training loss: 0.007644633995369077\n",
      "epoch: 21 trial 9826 training loss: 0.015474390238523483\n",
      "epoch: 21 trial 9827 training loss: 0.08372462913393974\n",
      "epoch: 21 trial 9828 training loss: 0.024207082577049732\n",
      "epoch: 21 trial 9829 training loss: 0.032770368270576\n",
      "epoch: 21 trial 9830 training loss: 0.05434824340045452\n",
      "epoch: 21 trial 9831 training loss: 0.03032524837180972\n",
      "epoch: 21 trial 9832 training loss: 0.021956515032798052\n",
      "epoch: 21 trial 9833 training loss: 0.016268324572592974\n",
      "epoch: 21 trial 9834 training loss: 0.03518488071858883\n",
      "epoch: 21 trial 9835 training loss: 0.012139370199292898\n",
      "epoch: 21 trial 9836 training loss: 0.009426844073459506\n",
      "epoch: 21 trial 9837 training loss: 0.028200328815728426\n",
      "epoch: 21 trial 9838 training loss: 0.004068116541020572\n",
      "epoch: 21 trial 9839 training loss: 0.012148002628237009\n",
      "epoch: 21 trial 9840 training loss: 0.01755330292508006\n",
      "epoch: 21 trial 9841 training loss: 0.010448698187246919\n",
      "epoch: 21 trial 9842 training loss: 0.05244775768369436\n",
      "epoch: 21 trial 9843 training loss: 0.004420475568622351\n",
      "epoch: 21 trial 9844 training loss: 0.054017892107367516\n",
      "epoch: 21 trial 9845 training loss: 0.01222773501649499\n",
      "epoch: 21 trial 9846 training loss: 0.022883005905896425\n",
      "epoch: 21 trial 9847 training loss: 0.026982222218066454\n",
      "epoch: 21 trial 9848 training loss: 0.01480783661827445\n",
      "epoch: 21 trial 9849 training loss: 0.03444923274219036\n",
      "epoch: 21 trial 9850 training loss: 0.118479885160923\n",
      "epoch: 21 trial 9851 training loss: 0.054645998403429985\n",
      "epoch: 21 trial 9852 training loss: 0.009490636410191655\n",
      "epoch: 21 trial 9853 training loss: 0.06787040643393993\n",
      "epoch: 21 trial 9854 training loss: 0.030985785648226738\n",
      "epoch: 21 trial 9855 training loss: 0.022756023798137903\n",
      "epoch: 21 trial 9856 training loss: 0.01699606142938137\n",
      "epoch: 21 trial 9857 training loss: 0.025864295661449432\n",
      "epoch: 21 trial 9858 training loss: 0.02789417328312993\n",
      "epoch: 21 trial 9859 training loss: 0.03883612807840109\n",
      "epoch: 21 trial 9860 training loss: 0.03577874414622784\n",
      "epoch: 21 trial 9861 training loss: 0.07657506130635738\n",
      "epoch: 21 trial 9862 training loss: 0.008224567165598273\n",
      "epoch: 21 trial 9863 training loss: 0.061369771137833595\n",
      "epoch: 21 trial 9864 training loss: 0.019097987562417984\n",
      "epoch: 21 trial 9865 training loss: 0.01747567392885685\n",
      "epoch: 21 trial 9866 training loss: 0.046212879940867424\n",
      "epoch: 21 trial 9867 training loss: 0.04871030244976282\n",
      "epoch: 21 trial 9868 training loss: 0.015396723523736\n",
      "epoch: 21 trial 9869 training loss: 0.03724699467420578\n",
      "epoch: 21 trial 9870 training loss: 0.07729639299213886\n",
      "epoch: 21 trial 9871 training loss: 0.07522705756127834\n",
      "epoch: 21 trial 9872 training loss: 0.016701208893209696\n",
      "epoch: 21 trial 9873 training loss: 0.020929320249706507\n",
      "epoch: 21 trial 9874 training loss: 0.015265570022165775\n",
      "epoch: 21 trial 9875 training loss: 0.022775588557124138\n",
      "epoch: 21 trial 9876 training loss: 0.04062748700380325\n",
      "epoch: 21 trial 9877 training loss: 0.031261906027793884\n",
      "epoch: 21 trial 9878 training loss: 0.07282623089849949\n",
      "epoch: 21 trial 9879 training loss: 0.11307015642523766\n",
      "epoch: 21 trial 9880 training loss: 0.016317643225193024\n",
      "epoch: 21 trial 9881 training loss: 0.006453568581491709\n",
      "epoch: 21 trial 9882 training loss: 0.00848413584753871\n",
      "epoch: 21 trial 9883 training loss: 0.0109405352268368\n",
      "epoch: 21 trial 9884 training loss: 0.016108775977045298\n",
      "epoch: 21 trial 9885 training loss: 0.021753227338194847\n",
      "epoch: 21 trial 9886 training loss: 0.09884695708751678\n",
      "epoch: 21 trial 9887 training loss: 0.03428847715258598\n",
      "epoch: 21 trial 9888 training loss: 0.013615503907203674\n",
      "epoch: 21 trial 9889 training loss: 0.0943410973995924\n",
      "epoch: 21 trial 9890 training loss: 0.0421073492616415\n",
      "epoch: 21 trial 9891 training loss: 0.059898942708969116\n",
      "epoch: 21 trial 9892 training loss: 0.0655441852286458\n",
      "epoch: 21 trial 9893 training loss: 0.11316893808543682\n",
      "epoch: 21 trial 9894 training loss: 0.014217800693586469\n",
      "epoch: 21 trial 9895 training loss: 0.009161093970760703\n",
      "epoch: 21 trial 9896 training loss: 0.021002778317779303\n",
      "epoch: 21 trial 9897 training loss: 0.022420627996325493\n",
      "epoch: 21 trial 9898 training loss: 0.021382728591561317\n",
      "epoch: 21 trial 9899 training loss: 0.0473542008548975\n",
      "epoch: 21 trial 9900 training loss: 0.07289944775402546\n",
      "epoch: 21 trial 9901 training loss: 0.03176267445087433\n",
      "epoch: 21 trial 9902 training loss: 0.04884433280676603\n",
      "epoch: 21 trial 9903 training loss: 0.039452819153666496\n",
      "epoch: 21 trial 9904 training loss: 0.09114095941185951\n",
      "epoch: 21 trial 9905 training loss: 0.01301125786267221\n",
      "epoch: 21 trial 9906 training loss: 0.011904898565262556\n",
      "epoch: 21 trial 9907 training loss: 0.025001270696520805\n",
      "epoch: 21 trial 9908 training loss: 0.029521997086703777\n",
      "epoch: 21 trial 9909 training loss: 0.04548292048275471\n",
      "epoch: 21 trial 9910 training loss: 0.014811580069363117\n",
      "epoch: 21 trial 9911 training loss: 0.17118968069553375\n",
      "epoch: 21 trial 9912 training loss: 0.021214174572378397\n",
      "epoch: 21 trial 9913 training loss: 0.09858054295182228\n",
      "epoch: 21 trial 9914 training loss: 0.09083512984216213\n",
      "epoch: 21 trial 9915 training loss: 0.013587472029030323\n",
      "epoch: 21 trial 9916 training loss: 0.06107233930379152\n",
      "epoch: 21 trial 9917 training loss: 0.015379884745925665\n",
      "epoch: 21 trial 9918 training loss: 0.048577334731817245\n",
      "epoch: 21 trial 9919 training loss: 0.04904376808553934\n",
      "epoch: 21 trial 9920 training loss: 0.06613854691386223\n",
      "epoch: 21 trial 9921 training loss: 0.015313873533159494\n",
      "epoch: 21 trial 9922 training loss: 0.039133621379733086\n",
      "epoch: 21 trial 9923 training loss: 0.03376973047852516\n",
      "epoch: 21 trial 9924 training loss: 0.031256585381925106\n",
      "epoch: 21 trial 9925 training loss: 0.060440365225076675\n",
      "epoch: 21 trial 9926 training loss: 0.017541428562253714\n",
      "epoch: 21 trial 9927 training loss: 0.029031083919107914\n",
      "epoch: 21 trial 9928 training loss: 0.01912163896486163\n",
      "epoch: 21 trial 9929 training loss: 0.0227851876989007\n",
      "epoch: 21 trial 9930 training loss: 0.03986992686986923\n",
      "epoch: 21 trial 9931 training loss: 0.010964268585667014\n",
      "epoch: 21 trial 9932 training loss: 0.006799343973398209\n",
      "epoch: 21 trial 9933 training loss: 0.033395927399396896\n",
      "epoch: 21 trial 9934 training loss: 0.0897244606167078\n",
      "epoch: 21 trial 9935 training loss: 0.020709479693323374\n",
      "epoch: 21 trial 9936 training loss: 0.09158444963395596\n",
      "epoch: 21 trial 9937 training loss: 0.055346143431961536\n",
      "epoch: 21 trial 9938 training loss: 0.027832286432385445\n",
      "epoch: 21 trial 9939 training loss: 0.1422683149576187\n",
      "epoch: 21 trial 9940 training loss: 0.012596283107995987\n",
      "epoch: 21 trial 9941 training loss: 0.011499993968755007\n",
      "epoch: 21 trial 9942 training loss: 0.010100306244567037\n",
      "epoch: 21 trial 9943 training loss: 0.0039402099791914225\n",
      "epoch: 21 trial 9944 training loss: 0.03166750445961952\n",
      "epoch: 21 trial 9945 training loss: 0.035757691599428654\n",
      "epoch: 21 trial 9946 training loss: 0.0530651556327939\n",
      "epoch: 21 trial 9947 training loss: 0.030466554686427116\n",
      "epoch: 21 trial 9948 training loss: 0.04899564012885094\n",
      "epoch: 21 trial 9949 training loss: 0.01837070658802986\n",
      "epoch: 21 trial 9950 training loss: 0.08345110714435577\n",
      "epoch: 21 trial 9951 training loss: 0.017847123090177774\n",
      "epoch: 21 trial 9952 training loss: 0.0077226716093719006\n",
      "epoch: 21 trial 9953 training loss: 0.022046566009521484\n",
      "epoch: 21 trial 9954 training loss: 0.0056224382715299726\n",
      "epoch: 21 trial 9955 training loss: 0.07363811694085598\n",
      "epoch: 21 trial 9956 training loss: 0.003207308007404208\n",
      "epoch: 21 trial 9957 training loss: 0.02395471278578043\n",
      "epoch: 21 trial 9958 training loss: 0.018232112750411034\n",
      "epoch: 21 trial 9959 training loss: 0.060068154707551\n",
      "epoch: 21 trial 9960 training loss: 0.04329647030681372\n",
      "epoch: 21 trial 9961 training loss: 0.021668650209903717\n",
      "epoch: 21 trial 9962 training loss: 0.03234361205250025\n",
      "epoch: 21 trial 9963 training loss: 0.06429731659591198\n",
      "epoch: 21 trial 9964 training loss: 0.026133625768125057\n",
      "epoch: 21 trial 9965 training loss: 0.07101328484714031\n",
      "epoch: 21 trial 9966 training loss: 0.019357176963239908\n",
      "epoch: 21 trial 9967 training loss: 0.0599060095846653\n",
      "epoch: 21 trial 9968 training loss: 0.019395566545426846\n",
      "epoch: 21 trial 9969 training loss: 0.007517172954976559\n",
      "epoch: 21 trial 9970 training loss: 0.05024570319801569\n",
      "epoch: 21 trial 9971 training loss: 0.02984572295099497\n",
      "epoch: 21 trial 9972 training loss: 0.01615995168685913\n",
      "epoch: 21 trial 9973 training loss: 0.01978043047711253\n",
      "epoch: 21 trial 9974 training loss: 0.009267461718991399\n",
      "epoch: 21 trial 9975 training loss: 0.023211585823446512\n",
      "epoch: 21 trial 9976 training loss: 0.03640650864690542\n",
      "epoch: 21 trial 9977 training loss: 0.01476308610290289\n",
      "epoch: 21 trial 9978 training loss: 0.03322468884289265\n",
      "epoch: 21 trial 9979 training loss: 0.08782367035746574\n",
      "epoch: 21 trial 9980 training loss: 0.04140098765492439\n",
      "epoch: 21 trial 9981 training loss: 0.013129503931850195\n",
      "epoch: 21 trial 9982 training loss: 0.004615770769305527\n",
      "epoch: 21 trial 9983 training loss: 0.00739321717992425\n",
      "epoch: 21 trial 9984 training loss: 0.026595267932862043\n",
      "epoch: 21 trial 9985 training loss: 0.04835629928857088\n",
      "epoch: 21 trial 9986 training loss: 0.0434985039755702\n",
      "epoch: 21 trial 9987 training loss: 0.0038904869579710066\n",
      "epoch: 21 trial 9988 training loss: 0.03316752798855305\n",
      "epoch: 21 trial 9989 training loss: 0.028608883265405893\n",
      "epoch: 21 trial 9990 training loss: 0.018625476164743304\n",
      "epoch: 21 trial 9991 training loss: 0.01638208655640483\n",
      "epoch: 21 trial 9992 training loss: 0.018162342254072428\n",
      "epoch: 21 trial 9993 training loss: 0.042933935299515724\n",
      "epoch: 21 trial 9994 training loss: 0.022939044050872326\n",
      "epoch: 21 trial 9995 training loss: 0.03257247433066368\n",
      "epoch: 21 trial 9996 training loss: 0.07333904877305031\n",
      "epoch: 21 trial 9997 training loss: 0.022376507055014372\n",
      "epoch: 21 trial 9998 training loss: 0.03333619888871908\n",
      "epoch: 21 trial 9999 training loss: 0.06441051699221134\n",
      "epoch: 21 trial 10000 training loss: 0.03323219623416662\n",
      "epoch: 21 trial 10001 training loss: 0.06650771759450436\n",
      "epoch: 21 trial 10002 training loss: 0.017406092025339603\n",
      "epoch: 21 trial 10003 training loss: 0.030750966630876064\n",
      "epoch: 21 trial 10004 training loss: 0.026425243355333805\n",
      "epoch: 21 trial 10005 training loss: 0.02219452615827322\n",
      "epoch: 21 trial 10006 training loss: 0.02440211223438382\n",
      "epoch: 21 trial 10007 training loss: 0.09293071553111076\n",
      "epoch: 21 trial 10008 training loss: 0.014855234185233712\n",
      "epoch: 21 trial 10009 training loss: 0.0817959550768137\n",
      "epoch: 21 trial 10010 training loss: 0.03225530777126551\n",
      "epoch: 21 trial 10011 training loss: 0.007231931667774916\n",
      "epoch: 21 trial 10012 training loss: 0.046326179057359695\n",
      "epoch: 21 trial 10013 training loss: 0.05649389419704676\n",
      "epoch: 21 trial 10014 training loss: 0.011799430008977652\n",
      "epoch: 21 trial 10015 training loss: 0.033982833847403526\n",
      "epoch: 21 trial 10016 training loss: 0.009753340389579535\n",
      "epoch: 21 trial 10017 training loss: 0.033943917602300644\n",
      "epoch: 21 trial 10018 training loss: 0.021786849480122328\n",
      "epoch: 21 trial 10019 training loss: 0.040440880693495274\n",
      "epoch: 21 trial 10020 training loss: 0.26940319687128067\n",
      "epoch: 21 trial 10021 training loss: 0.053356777876615524\n",
      "epoch: 21 trial 10022 training loss: 0.008964624255895615\n",
      "epoch: 21 trial 10023 training loss: 0.016825168393552303\n",
      "epoch: 21 trial 10024 training loss: 0.12581027671694756\n",
      "epoch: 21 trial 10025 training loss: 0.04708354081958532\n",
      "epoch: 21 trial 10026 training loss: 0.07692401111125946\n",
      "epoch: 21 trial 10027 training loss: 0.050815315917134285\n",
      "epoch: 21 trial 10028 training loss: 0.032752531580626965\n",
      "epoch: 21 trial 10029 training loss: 0.009949344675987959\n",
      "epoch: 21 trial 10030 training loss: 0.06824419274926186\n",
      "epoch: 21 trial 10031 training loss: 0.03024117834866047\n",
      "epoch: 21 trial 10032 training loss: 0.0873738694936037\n",
      "epoch: 21 trial 10033 training loss: 0.011913790134713054\n",
      "epoch: 21 trial 10034 training loss: 0.005337920563761145\n",
      "epoch: 21 trial 10035 training loss: 0.02700652927160263\n",
      "epoch: 21 trial 10036 training loss: 0.005306275095790625\n",
      "epoch: 21 trial 10037 training loss: 0.014252766501158476\n",
      "epoch: 21 trial 10038 training loss: 0.09663383290171623\n",
      "epoch: 21 trial 10039 training loss: 0.07080096378922462\n",
      "epoch: 21 trial 10040 training loss: 0.025310765486210585\n",
      "epoch: 21 trial 10041 training loss: 0.09838440455496311\n",
      "epoch: 21 trial 10042 training loss: 0.09191948175430298\n",
      "epoch: 21 trial 10043 training loss: 0.025173015892505646\n",
      "epoch: 21 trial 10044 training loss: 0.06812665052711964\n",
      "epoch: 21 trial 10045 training loss: 0.03191506862640381\n",
      "epoch: 21 trial 10046 training loss: 0.017765888012945652\n",
      "epoch: 21 trial 10047 training loss: 0.005652574123814702\n",
      "epoch: 21 trial 10048 training loss: 0.13004492968320847\n",
      "epoch: 21 trial 10049 training loss: 0.056245525367558\n",
      "epoch: 21 trial 10050 training loss: 0.04985595773905516\n",
      "epoch: 21 trial 10051 training loss: 0.055364709347486496\n",
      "epoch: 21 trial 10052 training loss: 0.026460389606654644\n",
      "epoch: 21 trial 10053 training loss: 0.038689940236508846\n",
      "epoch: 21 trial 10054 training loss: 0.015045919455587864\n",
      "epoch: 21 trial 10055 training loss: 0.047371338121593\n",
      "epoch: 21 trial 10056 training loss: 0.026259408332407475\n",
      "epoch: 21 trial 10057 training loss: 0.03783279098570347\n",
      "epoch: 21 trial 10058 training loss: 0.08806028962135315\n",
      "epoch: 21 trial 10059 training loss: 0.024376821471378207\n",
      "epoch: 21 trial 10060 training loss: 0.010415820172056556\n",
      "epoch: 21 trial 10061 training loss: 0.029166557360440493\n",
      "epoch: 21 trial 10062 training loss: 0.004230898921377957\n",
      "epoch: 21 trial 10063 training loss: 0.034614257514476776\n",
      "epoch: 21 trial 10064 training loss: 0.03776079323142767\n",
      "epoch: 21 trial 10065 training loss: 0.01283620367757976\n",
      "epoch: 21 trial 10066 training loss: 0.0022538701887242496\n",
      "epoch: 21 trial 10067 training loss: 0.0033896108507178724\n",
      "epoch: 21 trial 10068 training loss: 0.018287900369614363\n",
      "epoch: 21 trial 10069 training loss: 0.036313108168542385\n",
      "epoch: 21 trial 10070 training loss: 0.005036342889070511\n",
      "epoch: 21 trial 10071 training loss: 0.00507912936154753\n",
      "epoch: 21 trial 10072 training loss: 0.006327123963274062\n",
      "epoch: 21 trial 10073 training loss: 0.008808566955849528\n",
      "epoch: 21 trial 10074 training loss: 0.011438684538006783\n",
      "epoch: 21 trial 10075 training loss: 0.025418108329176903\n",
      "epoch: 21 trial 10076 training loss: 0.107595294713974\n",
      "epoch: 21 trial 10077 training loss: 0.02123296493664384\n",
      "epoch: 21 trial 10078 training loss: 0.0181037369184196\n",
      "epoch: 21 trial 10079 training loss: 0.029243369586765766\n",
      "epoch: 21 trial 10080 training loss: 0.038344104774296284\n",
      "epoch: 21 trial 10081 training loss: 0.03437701053917408\n",
      "epoch: 21 trial 10082 training loss: 0.030000480357557535\n",
      "epoch: 21 trial 10083 training loss: 0.0431742649525404\n",
      "epoch: 21 trial 10084 training loss: 0.06846620701253414\n",
      "epoch: 21 trial 10085 training loss: 0.10349256917834282\n",
      "epoch: 21 trial 10086 training loss: 0.020480481907725334\n",
      "epoch: 21 trial 10087 training loss: 0.02241395739838481\n",
      "epoch: 21 trial 10088 training loss: 0.08918271400034428\n",
      "epoch: 21 trial 10089 training loss: 0.015670273918658495\n",
      "epoch: 21 trial 10090 training loss: 0.0490975147113204\n",
      "epoch: 21 trial 10091 training loss: 0.09044358879327774\n",
      "epoch: 21 trial 10092 training loss: 0.0920439213514328\n",
      "epoch: 21 trial 10093 training loss: 0.042898885905742645\n",
      "epoch: 21 trial 10094 training loss: 0.11350733786821365\n",
      "epoch: 21 trial 10095 training loss: 0.0183333782479167\n",
      "epoch: 21 trial 10096 training loss: 0.04007436241954565\n",
      "epoch: 21 trial 10097 training loss: 0.030384080484509468\n",
      "epoch: 21 trial 10098 training loss: 0.0982367992401123\n",
      "epoch: 21 trial 10099 training loss: 0.008726550033316016\n",
      "epoch: 21 trial 10100 training loss: 0.006277403794229031\n",
      "epoch: 21 trial 10101 training loss: 0.020048278849571943\n",
      "epoch: 21 trial 10102 training loss: 0.02711837412789464\n",
      "epoch: 21 trial 10103 training loss: 0.05102737620472908\n",
      "epoch: 21 trial 10104 training loss: 0.05260338447988033\n",
      "epoch: 21 trial 10105 training loss: 0.01623975159600377\n",
      "epoch: 21 trial 10106 training loss: 0.05650843679904938\n",
      "epoch: 21 trial 10107 training loss: 0.01511839684098959\n",
      "epoch: 21 trial 10108 training loss: 0.017132175620645285\n",
      "epoch: 21 trial 10109 training loss: 0.02084786305204034\n",
      "epoch: 21 trial 10110 training loss: 0.013679648749530315\n",
      "epoch: 21 trial 10111 training loss: 0.01801815675571561\n",
      "epoch: 21 trial 10112 training loss: 0.038088093511760235\n",
      "epoch: 21 trial 10113 training loss: 0.028164069168269634\n",
      "epoch: 21 trial 10114 training loss: 0.0237993779592216\n",
      "epoch: 21 trial 10115 training loss: 0.021038226783275604\n",
      "epoch: 21 trial 10116 training loss: 0.01549778413027525\n",
      "epoch: 21 trial 10117 training loss: 0.004806595970876515\n",
      "epoch: 21 trial 10118 training loss: 0.011055458104237914\n",
      "epoch: 21 trial 10119 training loss: 0.0152547312900424\n",
      "epoch: 21 trial 10120 training loss: 0.013363624224439263\n",
      "epoch: 21 trial 10121 training loss: 0.005102777271531522\n",
      "epoch: 21 trial 10122 training loss: 0.012839776929467916\n",
      "epoch: 21 trial 10123 training loss: 0.0853084921836853\n",
      "epoch: 21 trial 10124 training loss: 0.010112582938745618\n",
      "epoch: 21 trial 10125 training loss: 0.038602517917752266\n",
      "epoch: 21 trial 10126 training loss: 0.015496530570089817\n",
      "epoch: 21 trial 10127 training loss: 0.04940575361251831\n",
      "epoch: 21 trial 10128 training loss: 0.04540022183209658\n",
      "epoch: 21 trial 10129 training loss: 0.04145012702792883\n",
      "epoch: 21 trial 10130 training loss: 0.025367832276970148\n",
      "epoch: 21 trial 10131 training loss: 0.021254797000437975\n",
      "epoch: 21 trial 10132 training loss: 0.0956792589277029\n",
      "epoch: 21 trial 10133 training loss: 0.006011232268065214\n",
      "epoch: 21 trial 10134 training loss: 0.01760957110673189\n",
      "epoch: 21 trial 10135 training loss: 0.12624584510922432\n",
      "epoch: 21 trial 10136 training loss: 0.023792149499058723\n",
      "epoch: 21 trial 10137 training loss: 0.01973609672859311\n",
      "epoch: 21 trial 10138 training loss: 0.017715003341436386\n",
      "epoch: 21 trial 10139 training loss: 0.012931327801197767\n",
      "epoch: 21 trial 10140 training loss: 0.026895553804934025\n",
      "epoch: 21 trial 10141 training loss: 0.013492083875462413\n",
      "epoch: 21 trial 10142 training loss: 0.05374041851609945\n",
      "epoch: 21 trial 10143 training loss: 0.15523739159107208\n",
      "epoch: 21 trial 10144 training loss: 0.07123495079576969\n",
      "epoch: 21 trial 10145 training loss: 0.02908088732510805\n",
      "epoch: 21 trial 10146 training loss: 0.06268811039626598\n",
      "epoch: 21 trial 10147 training loss: 0.09665867127478123\n",
      "epoch: 21 trial 10148 training loss: 0.024727863259613514\n",
      "epoch: 21 trial 10149 training loss: 0.06626698281615973\n",
      "epoch: 21 trial 10150 training loss: 0.008027548203244805\n",
      "epoch: 21 trial 10151 training loss: 0.038288614712655544\n",
      "epoch: 21 trial 10152 training loss: 0.012417866848409176\n",
      "epoch: 21 trial 10153 training loss: 0.02786901593208313\n",
      "epoch: 21 trial 10154 training loss: 0.1130792498588562\n",
      "epoch: 21 trial 10155 training loss: 0.04715748690068722\n",
      "epoch: 21 trial 10156 training loss: 0.01975606521591544\n",
      "epoch: 21 trial 10157 training loss: 0.020403002854436636\n",
      "epoch: 21 trial 10158 training loss: 0.07294734194874763\n",
      "epoch: 21 trial 10159 training loss: 0.015425463207066059\n",
      "epoch: 21 trial 10160 training loss: 0.05195544566959143\n",
      "epoch: 21 trial 10161 training loss: 0.03792923875153065\n",
      "epoch: 21 trial 10162 training loss: 0.042851800099015236\n",
      "epoch: 21 trial 10163 training loss: 0.013015494449064136\n",
      "epoch: 21 trial 10164 training loss: 0.055882249027490616\n",
      "epoch: 22 trial 10165 training loss: 0.013107785023748875\n",
      "epoch: 22 trial 10166 training loss: 0.013521256390959024\n",
      "epoch: 22 trial 10167 training loss: 0.009611723478883505\n",
      "epoch: 22 trial 10168 training loss: 0.05836549587547779\n",
      "epoch: 22 trial 10169 training loss: 0.004489556769840419\n",
      "epoch: 22 trial 10170 training loss: 0.005352790001779795\n",
      "epoch: 22 trial 10171 training loss: 0.026472946628928185\n",
      "epoch: 22 trial 10172 training loss: 0.034235878847539425\n",
      "epoch: 22 trial 10173 training loss: 0.0057845538249239326\n",
      "epoch: 22 trial 10174 training loss: 0.014280336908996105\n",
      "epoch: 22 trial 10175 training loss: 0.057822929695248604\n",
      "epoch: 22 trial 10176 training loss: 0.033089688047766685\n",
      "epoch: 22 trial 10177 training loss: 0.01522960513830185\n",
      "epoch: 22 trial 10178 training loss: 0.017501063644886017\n",
      "epoch: 22 trial 10179 training loss: 0.019733355846256018\n",
      "epoch: 22 trial 10180 training loss: 0.022572388406842947\n",
      "epoch: 22 trial 10181 training loss: 0.10952707007527351\n",
      "epoch: 22 trial 10182 training loss: 0.04846248775720596\n",
      "epoch: 22 trial 10183 training loss: 0.018361613620072603\n",
      "epoch: 22 trial 10184 training loss: 0.020245183259248734\n",
      "epoch: 22 trial 10185 training loss: 0.015977973118424416\n",
      "epoch: 22 trial 10186 training loss: 0.08352544903755188\n",
      "epoch: 22 trial 10187 training loss: 0.013447653269395232\n",
      "epoch: 22 trial 10188 training loss: 0.0038986774161458015\n",
      "epoch: 22 trial 10189 training loss: 0.13426827266812325\n",
      "epoch: 22 trial 10190 training loss: 0.026887691114097834\n",
      "epoch: 22 trial 10191 training loss: 0.011346917366608977\n",
      "epoch: 22 trial 10192 training loss: 0.10273509286344051\n",
      "epoch: 22 trial 10193 training loss: 0.014934508828446269\n",
      "epoch: 22 trial 10194 training loss: 0.019483324605971575\n",
      "epoch: 22 trial 10195 training loss: 0.024760314263403416\n",
      "epoch: 22 trial 10196 training loss: 0.01198826334439218\n",
      "epoch: 22 trial 10197 training loss: 0.0297507643699646\n",
      "epoch: 22 trial 10198 training loss: 0.04942254163324833\n",
      "epoch: 22 trial 10199 training loss: 0.01797831803560257\n",
      "epoch: 22 trial 10200 training loss: 0.06458218395709991\n",
      "epoch: 22 trial 10201 training loss: 0.1383914202451706\n",
      "epoch: 22 trial 10202 training loss: 0.2682625651359558\n",
      "epoch: 22 trial 10203 training loss: 0.04494293127208948\n",
      "epoch: 22 trial 10204 training loss: 0.08732768334448338\n",
      "epoch: 22 trial 10205 training loss: 0.09271220676600933\n",
      "epoch: 22 trial 10206 training loss: 0.011288878740742803\n",
      "epoch: 22 trial 10207 training loss: 0.08217436075210571\n",
      "epoch: 22 trial 10208 training loss: 0.035886190831661224\n",
      "epoch: 22 trial 10209 training loss: 0.023326819762587547\n",
      "epoch: 22 trial 10210 training loss: 0.061090435832738876\n",
      "epoch: 22 trial 10211 training loss: 0.010585654294118285\n",
      "epoch: 22 trial 10212 training loss: 0.02697572112083435\n",
      "epoch: 22 trial 10213 training loss: 0.015058138407766819\n",
      "epoch: 22 trial 10214 training loss: 0.10545005463063717\n",
      "epoch: 22 trial 10215 training loss: 0.03165987506508827\n",
      "epoch: 22 trial 10216 training loss: 0.10967898555099964\n",
      "epoch: 22 trial 10217 training loss: 0.03766287211328745\n",
      "epoch: 22 trial 10218 training loss: 0.04169603809714317\n",
      "epoch: 22 trial 10219 training loss: 0.03479546122252941\n",
      "epoch: 22 trial 10220 training loss: 0.05960024707019329\n",
      "epoch: 22 trial 10221 training loss: 0.026084566488862038\n",
      "epoch: 22 trial 10222 training loss: 0.01722979824990034\n",
      "epoch: 22 trial 10223 training loss: 0.028371242806315422\n",
      "epoch: 22 trial 10224 training loss: 0.0877055674791336\n",
      "epoch: 22 trial 10225 training loss: 0.026339301373809576\n",
      "epoch: 22 trial 10226 training loss: 0.12112538143992424\n",
      "epoch: 22 trial 10227 training loss: 0.017921434715390205\n",
      "epoch: 22 trial 10228 training loss: 0.0029208874912001193\n",
      "epoch: 22 trial 10229 training loss: 0.04818734806030989\n",
      "epoch: 22 trial 10230 training loss: 0.05274207144975662\n",
      "epoch: 22 trial 10231 training loss: 0.02230340661481023\n",
      "epoch: 22 trial 10232 training loss: 0.024002281948924065\n",
      "epoch: 22 trial 10233 training loss: 0.051895846612751484\n",
      "epoch: 22 trial 10234 training loss: 0.03922246117144823\n",
      "epoch: 22 trial 10235 training loss: 0.04382779821753502\n",
      "epoch: 22 trial 10236 training loss: 0.03584189806133509\n",
      "epoch: 22 trial 10237 training loss: 0.01592296687886119\n",
      "epoch: 22 trial 10238 training loss: 0.06721400283277035\n",
      "epoch: 22 trial 10239 training loss: 0.013573135249316692\n",
      "epoch: 22 trial 10240 training loss: 0.025349132250994444\n",
      "epoch: 22 trial 10241 training loss: 0.031355016864836216\n",
      "epoch: 22 trial 10242 training loss: 0.07851211912930012\n",
      "epoch: 22 trial 10243 training loss: 0.018342385068535805\n",
      "epoch: 22 trial 10244 training loss: 0.050755592063069344\n",
      "epoch: 22 trial 10245 training loss: 0.026774778496474028\n",
      "epoch: 22 trial 10246 training loss: 0.0444007134065032\n",
      "epoch: 22 trial 10247 training loss: 0.04604644514620304\n",
      "epoch: 22 trial 10248 training loss: 0.02399991685524583\n",
      "epoch: 22 trial 10249 training loss: 0.06852477602660656\n",
      "epoch: 22 trial 10250 training loss: 0.016202198341488838\n",
      "epoch: 22 trial 10251 training loss: 0.02757604606449604\n",
      "epoch: 22 trial 10252 training loss: 0.05238333344459534\n",
      "epoch: 22 trial 10253 training loss: 0.05742763914167881\n",
      "epoch: 22 trial 10254 training loss: 0.005951229715719819\n",
      "epoch: 22 trial 10255 training loss: 0.03963484521955252\n",
      "epoch: 22 trial 10256 training loss: 0.027184386737644672\n",
      "epoch: 22 trial 10257 training loss: 0.021540487185120583\n",
      "epoch: 22 trial 10258 training loss: 0.06417244113981724\n",
      "epoch: 22 trial 10259 training loss: 0.04428750183433294\n",
      "epoch: 22 trial 10260 training loss: 0.053934166207909584\n",
      "epoch: 22 trial 10261 training loss: 0.010439956560730934\n",
      "epoch: 22 trial 10262 training loss: 0.08398240990936756\n",
      "epoch: 22 trial 10263 training loss: 0.02121281484141946\n",
      "epoch: 22 trial 10264 training loss: 0.053627245128154755\n",
      "epoch: 22 trial 10265 training loss: 0.029543951153755188\n",
      "epoch: 22 trial 10266 training loss: 0.0883325207978487\n",
      "epoch: 22 trial 10267 training loss: 0.04580151289701462\n",
      "epoch: 22 trial 10268 training loss: 0.030152445659041405\n",
      "epoch: 22 trial 10269 training loss: 0.23252323269844055\n",
      "epoch: 22 trial 10270 training loss: 0.07248497009277344\n",
      "epoch: 22 trial 10271 training loss: 0.031243990175426006\n",
      "epoch: 22 trial 10272 training loss: 0.16532692685723305\n",
      "epoch: 22 trial 10273 training loss: 0.07862113229930401\n",
      "epoch: 22 trial 10274 training loss: 0.025713086128234863\n",
      "epoch: 22 trial 10275 training loss: 0.021471571642905474\n",
      "epoch: 22 trial 10276 training loss: 0.028369415551424026\n",
      "epoch: 22 trial 10277 training loss: 0.013998472830280662\n",
      "epoch: 22 trial 10278 training loss: 0.009751895675435662\n",
      "epoch: 22 trial 10279 training loss: 0.056957781314849854\n",
      "epoch: 22 trial 10280 training loss: 0.01778829749673605\n",
      "epoch: 22 trial 10281 training loss: 0.028145632706582546\n",
      "epoch: 22 trial 10282 training loss: 0.12090910412371159\n",
      "epoch: 22 trial 10283 training loss: 0.0271018473431468\n",
      "epoch: 22 trial 10284 training loss: 0.02145921904593706\n",
      "epoch: 22 trial 10285 training loss: 0.013045613188296556\n",
      "epoch: 22 trial 10286 training loss: 0.04199375119060278\n",
      "epoch: 22 trial 10287 training loss: 0.09449794702231884\n",
      "epoch: 22 trial 10288 training loss: 0.02864934131503105\n",
      "epoch: 22 trial 10289 training loss: 0.026432190090417862\n",
      "epoch: 22 trial 10290 training loss: 0.03968025650829077\n",
      "epoch: 22 trial 10291 training loss: 0.02653109608218074\n",
      "epoch: 22 trial 10292 training loss: 0.025717942975461483\n",
      "epoch: 22 trial 10293 training loss: 0.15910585969686508\n",
      "epoch: 22 trial 10294 training loss: 0.024657343979924917\n",
      "epoch: 22 trial 10295 training loss: 0.05402854643762112\n",
      "epoch: 22 trial 10296 training loss: 0.028841514140367508\n",
      "epoch: 22 trial 10297 training loss: 0.034857272170484066\n",
      "epoch: 22 trial 10298 training loss: 0.046426854096353054\n",
      "epoch: 22 trial 10299 training loss: 0.010620368411764503\n",
      "epoch: 22 trial 10300 training loss: 0.041320472955703735\n",
      "epoch: 22 trial 10301 training loss: 0.01124969869852066\n",
      "epoch: 22 trial 10302 training loss: 0.12018833309412003\n",
      "epoch: 22 trial 10303 training loss: 0.02123281918466091\n",
      "epoch: 22 trial 10304 training loss: 0.056568616069853306\n",
      "epoch: 22 trial 10305 training loss: 0.02201305888593197\n",
      "epoch: 22 trial 10306 training loss: 0.038823097944259644\n",
      "epoch: 22 trial 10307 training loss: 0.019365839194506407\n",
      "epoch: 22 trial 10308 training loss: 0.12264144793152809\n",
      "epoch: 22 trial 10309 training loss: 0.009222685592249036\n",
      "epoch: 22 trial 10310 training loss: 0.016568665392696857\n",
      "epoch: 22 trial 10311 training loss: 0.06814764812588692\n",
      "epoch: 22 trial 10312 training loss: 0.014985524583607912\n",
      "epoch: 22 trial 10313 training loss: 0.031247984617948532\n",
      "epoch: 22 trial 10314 training loss: 0.05227775499224663\n",
      "epoch: 22 trial 10315 training loss: 0.026032998226583004\n",
      "epoch: 22 trial 10316 training loss: 0.017960534431040287\n",
      "epoch: 22 trial 10317 training loss: 0.01689972635358572\n",
      "epoch: 22 trial 10318 training loss: 0.03596723359078169\n",
      "epoch: 22 trial 10319 training loss: 0.013614763040095568\n",
      "epoch: 22 trial 10320 training loss: 0.00943773752078414\n",
      "epoch: 22 trial 10321 training loss: 0.03134157136082649\n",
      "epoch: 22 trial 10322 training loss: 0.004158121068030596\n",
      "epoch: 22 trial 10323 training loss: 0.013474511448293924\n",
      "epoch: 22 trial 10324 training loss: 0.01962094148620963\n",
      "epoch: 22 trial 10325 training loss: 0.010276631684973836\n",
      "epoch: 22 trial 10326 training loss: 0.05441336706280708\n",
      "epoch: 22 trial 10327 training loss: 0.004464260535314679\n",
      "epoch: 22 trial 10328 training loss: 0.055862173438072205\n",
      "epoch: 22 trial 10329 training loss: 0.010237355018034577\n",
      "epoch: 22 trial 10330 training loss: 0.022427197080105543\n",
      "epoch: 22 trial 10331 training loss: 0.02522101253271103\n",
      "epoch: 22 trial 10332 training loss: 0.01268743653781712\n",
      "epoch: 22 trial 10333 training loss: 0.0346851721405983\n",
      "epoch: 22 trial 10334 training loss: 0.12157206609845161\n",
      "epoch: 22 trial 10335 training loss: 0.05230823718011379\n",
      "epoch: 22 trial 10336 training loss: 0.010138335172086954\n",
      "epoch: 22 trial 10337 training loss: 0.06268984451889992\n",
      "epoch: 22 trial 10338 training loss: 0.03256514575332403\n",
      "epoch: 22 trial 10339 training loss: 0.025527510326355696\n",
      "epoch: 22 trial 10340 training loss: 0.020045666489750147\n",
      "epoch: 22 trial 10341 training loss: 0.021646284963935614\n",
      "epoch: 22 trial 10342 training loss: 0.03677478805184364\n",
      "epoch: 22 trial 10343 training loss: 0.04179577250033617\n",
      "epoch: 22 trial 10344 training loss: 0.04043630696833134\n",
      "epoch: 22 trial 10345 training loss: 0.07371912896633148\n",
      "epoch: 22 trial 10346 training loss: 0.007992292055860162\n",
      "epoch: 22 trial 10347 training loss: 0.054843295365571976\n",
      "epoch: 22 trial 10348 training loss: 0.01968838507309556\n",
      "epoch: 22 trial 10349 training loss: 0.01807110896334052\n",
      "epoch: 22 trial 10350 training loss: 0.04167551454156637\n",
      "epoch: 22 trial 10351 training loss: 0.05323428474366665\n",
      "epoch: 22 trial 10352 training loss: 0.017650470603257418\n",
      "epoch: 22 trial 10353 training loss: 0.039970869198441505\n",
      "epoch: 22 trial 10354 training loss: 0.06795414164662361\n",
      "epoch: 22 trial 10355 training loss: 0.07132724486291409\n",
      "epoch: 22 trial 10356 training loss: 0.015403271652758121\n",
      "epoch: 22 trial 10357 training loss: 0.02011702209711075\n",
      "epoch: 22 trial 10358 training loss: 0.013017015065997839\n",
      "epoch: 22 trial 10359 training loss: 0.023608979769051075\n",
      "epoch: 22 trial 10360 training loss: 0.035495505668222904\n",
      "epoch: 22 trial 10361 training loss: 0.03011896274983883\n",
      "epoch: 22 trial 10362 training loss: 0.06775808148086071\n",
      "epoch: 22 trial 10363 training loss: 0.11441253125667572\n",
      "epoch: 22 trial 10364 training loss: 0.021070446819067\n",
      "epoch: 22 trial 10365 training loss: 0.0066257615108042955\n",
      "epoch: 22 trial 10366 training loss: 0.008133037714287639\n",
      "epoch: 22 trial 10367 training loss: 0.012316271429881454\n",
      "epoch: 22 trial 10368 training loss: 0.015263017732650042\n",
      "epoch: 22 trial 10369 training loss: 0.02543028863146901\n",
      "epoch: 22 trial 10370 training loss: 0.08539697714149952\n",
      "epoch: 22 trial 10371 training loss: 0.031398515217006207\n",
      "epoch: 22 trial 10372 training loss: 0.012957199243828654\n",
      "epoch: 22 trial 10373 training loss: 0.09339224733412266\n",
      "epoch: 22 trial 10374 training loss: 0.04935826361179352\n",
      "epoch: 22 trial 10375 training loss: 0.05747462064027786\n",
      "epoch: 22 trial 10376 training loss: 0.06212868355214596\n",
      "epoch: 22 trial 10377 training loss: 0.10524991154670715\n",
      "epoch: 22 trial 10378 training loss: 0.013379683950915933\n",
      "epoch: 22 trial 10379 training loss: 0.00925154099240899\n",
      "epoch: 22 trial 10380 training loss: 0.022704523988068104\n",
      "epoch: 22 trial 10381 training loss: 0.02053836267441511\n",
      "epoch: 22 trial 10382 training loss: 0.019764195661991835\n",
      "epoch: 22 trial 10383 training loss: 0.04470840934664011\n",
      "epoch: 22 trial 10384 training loss: 0.0707881711423397\n",
      "epoch: 22 trial 10385 training loss: 0.029950995463877916\n",
      "epoch: 22 trial 10386 training loss: 0.04833757132291794\n",
      "epoch: 22 trial 10387 training loss: 0.03873561415821314\n",
      "epoch: 22 trial 10388 training loss: 0.09036453254520893\n",
      "epoch: 22 trial 10389 training loss: 0.013313952134922147\n",
      "epoch: 22 trial 10390 training loss: 0.01169867580756545\n",
      "epoch: 22 trial 10391 training loss: 0.023161678109318018\n",
      "epoch: 22 trial 10392 training loss: 0.026410683058202267\n",
      "epoch: 22 trial 10393 training loss: 0.048142362385988235\n",
      "epoch: 22 trial 10394 training loss: 0.01538430037908256\n",
      "epoch: 22 trial 10395 training loss: 0.17433320358395576\n",
      "epoch: 22 trial 10396 training loss: 0.019504820927977562\n",
      "epoch: 22 trial 10397 training loss: 0.10527443140745163\n",
      "epoch: 22 trial 10398 training loss: 0.09496950544416904\n",
      "epoch: 22 trial 10399 training loss: 0.014565040823072195\n",
      "epoch: 22 trial 10400 training loss: 0.06214207876473665\n",
      "epoch: 22 trial 10401 training loss: 0.014974011108279228\n",
      "epoch: 22 trial 10402 training loss: 0.049563053995370865\n",
      "epoch: 22 trial 10403 training loss: 0.046342724934220314\n",
      "epoch: 22 trial 10404 training loss: 0.07452818565070629\n",
      "epoch: 22 trial 10405 training loss: 0.014779710676521063\n",
      "epoch: 22 trial 10406 training loss: 0.038906145840883255\n",
      "epoch: 22 trial 10407 training loss: 0.0354140680283308\n",
      "epoch: 22 trial 10408 training loss: 0.0278851049952209\n",
      "epoch: 22 trial 10409 training loss: 0.06288651563227177\n",
      "epoch: 22 trial 10410 training loss: 0.0175670119933784\n",
      "epoch: 22 trial 10411 training loss: 0.02877275925129652\n",
      "epoch: 22 trial 10412 training loss: 0.019603352528065443\n",
      "epoch: 22 trial 10413 training loss: 0.02336110407486558\n",
      "epoch: 22 trial 10414 training loss: 0.03581639425829053\n",
      "epoch: 22 trial 10415 training loss: 0.011399314273148775\n",
      "epoch: 22 trial 10416 training loss: 0.0072616583202034235\n",
      "epoch: 22 trial 10417 training loss: 0.03318989276885986\n",
      "epoch: 22 trial 10418 training loss: 0.08289284072816372\n",
      "epoch: 22 trial 10419 training loss: 0.022070693783462048\n",
      "epoch: 22 trial 10420 training loss: 0.1014039944857359\n",
      "epoch: 22 trial 10421 training loss: 0.050845387391746044\n",
      "epoch: 22 trial 10422 training loss: 0.025962691754102707\n",
      "epoch: 22 trial 10423 training loss: 0.1433432698249817\n",
      "epoch: 22 trial 10424 training loss: 0.01264083618298173\n",
      "epoch: 22 trial 10425 training loss: 0.011845387518405914\n",
      "epoch: 22 trial 10426 training loss: 0.009510715841315687\n",
      "epoch: 22 trial 10427 training loss: 0.004152348730713129\n",
      "epoch: 22 trial 10428 training loss: 0.03063017688691616\n",
      "epoch: 22 trial 10429 training loss: 0.032319268211722374\n",
      "epoch: 22 trial 10430 training loss: 0.05297764949500561\n",
      "epoch: 22 trial 10431 training loss: 0.031597621738910675\n",
      "epoch: 22 trial 10432 training loss: 0.0465746633708477\n",
      "epoch: 22 trial 10433 training loss: 0.019464585464447737\n",
      "epoch: 22 trial 10434 training loss: 0.08144401758909225\n",
      "epoch: 22 trial 10435 training loss: 0.01753880176693201\n",
      "epoch: 22 trial 10436 training loss: 0.007934733061119914\n",
      "epoch: 22 trial 10437 training loss: 0.023540893103927374\n",
      "epoch: 22 trial 10438 training loss: 0.005397426430135965\n",
      "epoch: 22 trial 10439 training loss: 0.07270032912492752\n",
      "epoch: 22 trial 10440 training loss: 0.003047240083105862\n",
      "epoch: 22 trial 10441 training loss: 0.022339551243931055\n",
      "epoch: 22 trial 10442 training loss: 0.01661567622795701\n",
      "epoch: 22 trial 10443 training loss: 0.06099795922636986\n",
      "epoch: 22 trial 10444 training loss: 0.0421109963208437\n",
      "epoch: 22 trial 10445 training loss: 0.020984903443604708\n",
      "epoch: 22 trial 10446 training loss: 0.03317663539201021\n",
      "epoch: 22 trial 10447 training loss: 0.06020955927670002\n",
      "epoch: 22 trial 10448 training loss: 0.026261976920068264\n",
      "epoch: 22 trial 10449 training loss: 0.07414409704506397\n",
      "epoch: 22 trial 10450 training loss: 0.019399047829210758\n",
      "epoch: 22 trial 10451 training loss: 0.06325079873204231\n",
      "epoch: 22 trial 10452 training loss: 0.02120881248265505\n",
      "epoch: 22 trial 10453 training loss: 0.0073565468192100525\n",
      "epoch: 22 trial 10454 training loss: 0.05032740347087383\n",
      "epoch: 22 trial 10455 training loss: 0.02944435365498066\n",
      "epoch: 22 trial 10456 training loss: 0.01673844689503312\n",
      "epoch: 22 trial 10457 training loss: 0.018168033100664616\n",
      "epoch: 22 trial 10458 training loss: 0.009481356479227543\n",
      "epoch: 22 trial 10459 training loss: 0.02529849298298359\n",
      "epoch: 22 trial 10460 training loss: 0.03743722755461931\n",
      "epoch: 22 trial 10461 training loss: 0.01586611196398735\n",
      "epoch: 22 trial 10462 training loss: 0.031037843320518732\n",
      "epoch: 22 trial 10463 training loss: 0.08913507498800755\n",
      "epoch: 22 trial 10464 training loss: 0.038778129033744335\n",
      "epoch: 22 trial 10465 training loss: 0.014406259637326002\n",
      "epoch: 22 trial 10466 training loss: 0.004757046466693282\n",
      "epoch: 22 trial 10467 training loss: 0.007153367041610181\n",
      "epoch: 22 trial 10468 training loss: 0.028081153519451618\n",
      "epoch: 22 trial 10469 training loss: 0.0505811870098114\n",
      "epoch: 22 trial 10470 training loss: 0.04646157566457987\n",
      "epoch: 22 trial 10471 training loss: 0.0034057056182064116\n",
      "epoch: 22 trial 10472 training loss: 0.030133944936096668\n",
      "epoch: 22 trial 10473 training loss: 0.024282422848045826\n",
      "epoch: 22 trial 10474 training loss: 0.021192776504904032\n",
      "epoch: 22 trial 10475 training loss: 0.014810573309659958\n",
      "epoch: 22 trial 10476 training loss: 0.017308988142758608\n",
      "epoch: 22 trial 10477 training loss: 0.04172072373330593\n",
      "epoch: 22 trial 10478 training loss: 0.023740575183182955\n",
      "epoch: 22 trial 10479 training loss: 0.030414961278438568\n",
      "epoch: 22 trial 10480 training loss: 0.06721088849008083\n",
      "epoch: 22 trial 10481 training loss: 0.018452467396855354\n",
      "epoch: 22 trial 10482 training loss: 0.03193001262843609\n",
      "epoch: 22 trial 10483 training loss: 0.06262603402137756\n",
      "epoch: 22 trial 10484 training loss: 0.03340356517583132\n",
      "epoch: 22 trial 10485 training loss: 0.06469227559864521\n",
      "epoch: 22 trial 10486 training loss: 0.017592994961887598\n",
      "epoch: 22 trial 10487 training loss: 0.028001696802675724\n",
      "epoch: 22 trial 10488 training loss: 0.025980204343795776\n",
      "epoch: 22 trial 10489 training loss: 0.022489323746412992\n",
      "epoch: 22 trial 10490 training loss: 0.022696112748235464\n",
      "epoch: 22 trial 10491 training loss: 0.08827725239098072\n",
      "epoch: 22 trial 10492 training loss: 0.014231575652956963\n",
      "epoch: 22 trial 10493 training loss: 0.08267208002507687\n",
      "epoch: 22 trial 10494 training loss: 0.03168470971286297\n",
      "epoch: 22 trial 10495 training loss: 0.007090539671480656\n",
      "epoch: 22 trial 10496 training loss: 0.0456945588812232\n",
      "epoch: 22 trial 10497 training loss: 0.057638635858893394\n",
      "epoch: 22 trial 10498 training loss: 0.011744103394448757\n",
      "epoch: 22 trial 10499 training loss: 0.036188787780702114\n",
      "epoch: 22 trial 10500 training loss: 0.010390727315098047\n",
      "epoch: 22 trial 10501 training loss: 0.031731508672237396\n",
      "epoch: 22 trial 10502 training loss: 0.020905588753521442\n",
      "epoch: 22 trial 10503 training loss: 0.03685724921524525\n",
      "epoch: 22 trial 10504 training loss: 0.2643623799085617\n",
      "epoch: 22 trial 10505 training loss: 0.058812616392970085\n",
      "epoch: 22 trial 10506 training loss: 0.009437432046979666\n",
      "epoch: 22 trial 10507 training loss: 0.0182401267811656\n",
      "epoch: 22 trial 10508 training loss: 0.1273171417415142\n",
      "epoch: 22 trial 10509 training loss: 0.04898918140679598\n",
      "epoch: 22 trial 10510 training loss: 0.0725451186299324\n",
      "epoch: 22 trial 10511 training loss: 0.0509840939193964\n",
      "epoch: 22 trial 10512 training loss: 0.0307391919195652\n",
      "epoch: 22 trial 10513 training loss: 0.008829060476273298\n",
      "epoch: 22 trial 10514 training loss: 0.07574013620615005\n",
      "epoch: 22 trial 10515 training loss: 0.029574450105428696\n",
      "epoch: 22 trial 10516 training loss: 0.09383857995271683\n",
      "epoch: 22 trial 10517 training loss: 0.01360309706069529\n",
      "epoch: 22 trial 10518 training loss: 0.005895920738112181\n",
      "epoch: 22 trial 10519 training loss: 0.029997454956173897\n",
      "epoch: 22 trial 10520 training loss: 0.00517565303016454\n",
      "epoch: 22 trial 10521 training loss: 0.013576966244727373\n",
      "epoch: 22 trial 10522 training loss: 0.10438395105302334\n",
      "epoch: 22 trial 10523 training loss: 0.07125971652567387\n",
      "epoch: 22 trial 10524 training loss: 0.027448976412415504\n",
      "epoch: 22 trial 10525 training loss: 0.10827494040131569\n",
      "epoch: 22 trial 10526 training loss: 0.09437867626547813\n",
      "epoch: 22 trial 10527 training loss: 0.024427164811640978\n",
      "epoch: 22 trial 10528 training loss: 0.057470109313726425\n",
      "epoch: 22 trial 10529 training loss: 0.033610605634748936\n",
      "epoch: 22 trial 10530 training loss: 0.014467427041381598\n",
      "epoch: 22 trial 10531 training loss: 0.005064586293883622\n",
      "epoch: 22 trial 10532 training loss: 0.12183879688382149\n",
      "epoch: 22 trial 10533 training loss: 0.0705542080104351\n",
      "epoch: 22 trial 10534 training loss: 0.046896787360310555\n",
      "epoch: 22 trial 10535 training loss: 0.0596588458865881\n",
      "epoch: 22 trial 10536 training loss: 0.024089474231004715\n",
      "epoch: 22 trial 10537 training loss: 0.042198522947728634\n",
      "epoch: 22 trial 10538 training loss: 0.016089982353150845\n",
      "epoch: 22 trial 10539 training loss: 0.050457059405744076\n",
      "epoch: 22 trial 10540 training loss: 0.028163600713014603\n",
      "epoch: 22 trial 10541 training loss: 0.03868349455296993\n",
      "epoch: 22 trial 10542 training loss: 0.08419224061071873\n",
      "epoch: 22 trial 10543 training loss: 0.024419914465397596\n",
      "epoch: 22 trial 10544 training loss: 0.010891590733081102\n",
      "epoch: 22 trial 10545 training loss: 0.036296277306973934\n",
      "epoch: 22 trial 10546 training loss: 0.0043429204961284995\n",
      "epoch: 22 trial 10547 training loss: 0.039414224214851856\n",
      "epoch: 22 trial 10548 training loss: 0.0415764469653368\n",
      "epoch: 22 trial 10549 training loss: 0.013757562730461359\n",
      "epoch: 22 trial 10550 training loss: 0.0019422646146267653\n",
      "epoch: 22 trial 10551 training loss: 0.0035819723270833492\n",
      "epoch: 22 trial 10552 training loss: 0.015431791543960571\n",
      "epoch: 22 trial 10553 training loss: 0.03752583730965853\n",
      "epoch: 22 trial 10554 training loss: 0.004730363609269261\n",
      "epoch: 22 trial 10555 training loss: 0.004941700724884868\n",
      "epoch: 22 trial 10556 training loss: 0.006273486069403589\n",
      "epoch: 22 trial 10557 training loss: 0.007612496963702142\n",
      "epoch: 22 trial 10558 training loss: 0.011152749648317695\n",
      "epoch: 22 trial 10559 training loss: 0.024609865620732307\n",
      "epoch: 22 trial 10560 training loss: 0.10653337463736534\n",
      "epoch: 22 trial 10561 training loss: 0.01873961230739951\n",
      "epoch: 22 trial 10562 training loss: 0.019971909001469612\n",
      "epoch: 22 trial 10563 training loss: 0.02694424521178007\n",
      "epoch: 22 trial 10564 training loss: 0.039025054313242435\n",
      "epoch: 22 trial 10565 training loss: 0.03593895863741636\n",
      "epoch: 22 trial 10566 training loss: 0.034056046046316624\n",
      "epoch: 22 trial 10567 training loss: 0.04313864465802908\n",
      "epoch: 22 trial 10568 training loss: 0.07423310913145542\n",
      "epoch: 22 trial 10569 training loss: 0.10446021519601345\n",
      "epoch: 22 trial 10570 training loss: 0.023728287778794765\n",
      "epoch: 22 trial 10571 training loss: 0.020937767811119556\n",
      "epoch: 22 trial 10572 training loss: 0.08802412077784538\n",
      "epoch: 22 trial 10573 training loss: 0.016478312201797962\n",
      "epoch: 22 trial 10574 training loss: 0.05156206898391247\n",
      "epoch: 22 trial 10575 training loss: 0.09509510733187199\n",
      "epoch: 22 trial 10576 training loss: 0.09379302710294724\n",
      "epoch: 22 trial 10577 training loss: 0.0424438351765275\n",
      "epoch: 22 trial 10578 training loss: 0.11182801425457001\n",
      "epoch: 22 trial 10579 training loss: 0.019500543363392353\n",
      "epoch: 22 trial 10580 training loss: 0.0439359350129962\n",
      "epoch: 22 trial 10581 training loss: 0.03189625032246113\n",
      "epoch: 22 trial 10582 training loss: 0.1023181639611721\n",
      "epoch: 22 trial 10583 training loss: 0.008341572945937514\n",
      "epoch: 22 trial 10584 training loss: 0.008034731727093458\n",
      "epoch: 22 trial 10585 training loss: 0.021078300196677446\n",
      "epoch: 22 trial 10586 training loss: 0.025694663636386395\n",
      "epoch: 22 trial 10587 training loss: 0.060671052895486355\n",
      "epoch: 22 trial 10588 training loss: 0.0588108841329813\n",
      "epoch: 22 trial 10589 training loss: 0.0175191150046885\n",
      "epoch: 22 trial 10590 training loss: 0.056188562884926796\n",
      "epoch: 22 trial 10591 training loss: 0.013669363223016262\n",
      "epoch: 22 trial 10592 training loss: 0.016629197169095278\n",
      "epoch: 22 trial 10593 training loss: 0.020285659935325384\n",
      "epoch: 22 trial 10594 training loss: 0.014070425648242235\n",
      "epoch: 22 trial 10595 training loss: 0.019680297933518887\n",
      "epoch: 22 trial 10596 training loss: 0.04337886534631252\n",
      "epoch: 22 trial 10597 training loss: 0.025294176768511534\n",
      "epoch: 22 trial 10598 training loss: 0.022483781911432743\n",
      "epoch: 22 trial 10599 training loss: 0.020884106867015362\n",
      "epoch: 22 trial 10600 training loss: 0.016899496782571077\n",
      "epoch: 22 trial 10601 training loss: 0.005144745227880776\n",
      "epoch: 22 trial 10602 training loss: 0.010187909239903092\n",
      "epoch: 22 trial 10603 training loss: 0.015561430249363184\n",
      "epoch: 22 trial 10604 training loss: 0.015138676390051842\n",
      "epoch: 22 trial 10605 training loss: 0.0061296914936974645\n",
      "epoch: 22 trial 10606 training loss: 0.012531930347904563\n",
      "epoch: 22 trial 10607 training loss: 0.0844743512570858\n",
      "epoch: 22 trial 10608 training loss: 0.009909466374665499\n",
      "epoch: 22 trial 10609 training loss: 0.03618928510695696\n",
      "epoch: 22 trial 10610 training loss: 0.014137431746348739\n",
      "epoch: 22 trial 10611 training loss: 0.04779881052672863\n",
      "epoch: 22 trial 10612 training loss: 0.04448233637958765\n",
      "epoch: 22 trial 10613 training loss: 0.041022016666829586\n",
      "epoch: 22 trial 10614 training loss: 0.026414023712277412\n",
      "epoch: 22 trial 10615 training loss: 0.020102697890251875\n",
      "epoch: 22 trial 10616 training loss: 0.09875551983714104\n",
      "epoch: 22 trial 10617 training loss: 0.005596589762717485\n",
      "epoch: 22 trial 10618 training loss: 0.017843202222138643\n",
      "epoch: 22 trial 10619 training loss: 0.12063334137201309\n",
      "epoch: 22 trial 10620 training loss: 0.02532228920608759\n",
      "epoch: 22 trial 10621 training loss: 0.020893407054245472\n",
      "epoch: 22 trial 10622 training loss: 0.01874289382249117\n",
      "epoch: 22 trial 10623 training loss: 0.012373162200674415\n",
      "epoch: 22 trial 10624 training loss: 0.026888233609497547\n",
      "epoch: 22 trial 10625 training loss: 0.012950363103300333\n",
      "epoch: 22 trial 10626 training loss: 0.054691338911652565\n",
      "epoch: 22 trial 10627 training loss: 0.15921170637011528\n",
      "epoch: 22 trial 10628 training loss: 0.06992040202021599\n",
      "epoch: 22 trial 10629 training loss: 0.030011079274117947\n",
      "epoch: 22 trial 10630 training loss: 0.05606039613485336\n",
      "epoch: 22 trial 10631 training loss: 0.09423995763063431\n",
      "epoch: 22 trial 10632 training loss: 0.026844358537346125\n",
      "epoch: 22 trial 10633 training loss: 0.06486170552670956\n",
      "epoch: 22 trial 10634 training loss: 0.007279307581484318\n",
      "epoch: 22 trial 10635 training loss: 0.03736125584691763\n",
      "epoch: 22 trial 10636 training loss: 0.011913178255781531\n",
      "epoch: 22 trial 10637 training loss: 0.028721489012241364\n",
      "epoch: 22 trial 10638 training loss: 0.11108081787824631\n",
      "epoch: 22 trial 10639 training loss: 0.04794508218765259\n",
      "epoch: 22 trial 10640 training loss: 0.019547345582395792\n",
      "epoch: 22 trial 10641 training loss: 0.019736737478524446\n",
      "epoch: 22 trial 10642 training loss: 0.07082788087427616\n",
      "epoch: 22 trial 10643 training loss: 0.014113185228779912\n",
      "epoch: 22 trial 10644 training loss: 0.05732353962957859\n",
      "epoch: 22 trial 10645 training loss: 0.03719969838857651\n",
      "epoch: 22 trial 10646 training loss: 0.04004717990756035\n",
      "epoch: 22 trial 10647 training loss: 0.011662997072562575\n",
      "epoch: 22 trial 10648 training loss: 0.05670964531600475\n",
      "epoch: 23 trial 10649 training loss: 0.013453264255076647\n",
      "epoch: 23 trial 10650 training loss: 0.012422363739460707\n",
      "epoch: 23 trial 10651 training loss: 0.010598021559417248\n",
      "epoch: 23 trial 10652 training loss: 0.06324240192770958\n",
      "epoch: 23 trial 10653 training loss: 0.0041038060444407165\n",
      "epoch: 23 trial 10654 training loss: 0.005326084094122052\n",
      "epoch: 23 trial 10655 training loss: 0.025041199289262295\n",
      "epoch: 23 trial 10656 training loss: 0.030222522094845772\n",
      "epoch: 23 trial 10657 training loss: 0.006082734325900674\n",
      "epoch: 23 trial 10658 training loss: 0.01497108954936266\n",
      "epoch: 23 trial 10659 training loss: 0.052011460065841675\n",
      "epoch: 23 trial 10660 training loss: 0.0334869846701622\n",
      "epoch: 23 trial 10661 training loss: 0.014055782463401556\n",
      "epoch: 23 trial 10662 training loss: 0.01597895286977291\n",
      "epoch: 23 trial 10663 training loss: 0.021877354476600885\n",
      "epoch: 23 trial 10664 training loss: 0.02284125378355384\n",
      "epoch: 23 trial 10665 training loss: 0.11352265626192093\n",
      "epoch: 23 trial 10666 training loss: 0.04776197299361229\n",
      "epoch: 23 trial 10667 training loss: 0.016316052060574293\n",
      "epoch: 23 trial 10668 training loss: 0.01920413365587592\n",
      "epoch: 23 trial 10669 training loss: 0.016528420615941286\n",
      "epoch: 23 trial 10670 training loss: 0.08791285753250122\n",
      "epoch: 23 trial 10671 training loss: 0.012681218096986413\n",
      "epoch: 23 trial 10672 training loss: 0.0036166708450764418\n",
      "epoch: 23 trial 10673 training loss: 0.14476900547742844\n",
      "epoch: 23 trial 10674 training loss: 0.028663700446486473\n",
      "epoch: 23 trial 10675 training loss: 0.011881363112479448\n",
      "epoch: 23 trial 10676 training loss: 0.10060933046042919\n",
      "epoch: 23 trial 10677 training loss: 0.016003369819372892\n",
      "epoch: 23 trial 10678 training loss: 0.0161592666991055\n",
      "epoch: 23 trial 10679 training loss: 0.024092371575534344\n",
      "epoch: 23 trial 10680 training loss: 0.013400610536336899\n",
      "epoch: 23 trial 10681 training loss: 0.029193990863859653\n",
      "epoch: 23 trial 10682 training loss: 0.047813584096729755\n",
      "epoch: 23 trial 10683 training loss: 0.01446609478443861\n",
      "epoch: 23 trial 10684 training loss: 0.06059383228421211\n",
      "epoch: 23 trial 10685 training loss: 0.13603148609399796\n",
      "epoch: 23 trial 10686 training loss: 0.2860292121767998\n",
      "epoch: 23 trial 10687 training loss: 0.040319438092410564\n",
      "epoch: 23 trial 10688 training loss: 0.0895452406257391\n",
      "epoch: 23 trial 10689 training loss: 0.0977560542523861\n",
      "epoch: 23 trial 10690 training loss: 0.011333144269883633\n",
      "epoch: 23 trial 10691 training loss: 0.07346131093800068\n",
      "epoch: 23 trial 10692 training loss: 0.03657238371670246\n",
      "epoch: 23 trial 10693 training loss: 0.02250692341476679\n",
      "epoch: 23 trial 10694 training loss: 0.056964531540870667\n",
      "epoch: 23 trial 10695 training loss: 0.009798768209293485\n",
      "epoch: 23 trial 10696 training loss: 0.0275493785738945\n",
      "epoch: 23 trial 10697 training loss: 0.01497875526547432\n",
      "epoch: 23 trial 10698 training loss: 0.09844212979078293\n",
      "epoch: 23 trial 10699 training loss: 0.034679665230214596\n",
      "epoch: 23 trial 10700 training loss: 0.11168367974460125\n",
      "epoch: 23 trial 10701 training loss: 0.03701398801058531\n",
      "epoch: 23 trial 10702 training loss: 0.03999969828873873\n",
      "epoch: 23 trial 10703 training loss: 0.03443587478250265\n",
      "epoch: 23 trial 10704 training loss: 0.05955677106976509\n",
      "epoch: 23 trial 10705 training loss: 0.02766294777393341\n",
      "epoch: 23 trial 10706 training loss: 0.018269064836204052\n",
      "epoch: 23 trial 10707 training loss: 0.02880054572597146\n",
      "epoch: 23 trial 10708 training loss: 0.08674637228250504\n",
      "epoch: 23 trial 10709 training loss: 0.024899078998714685\n",
      "epoch: 23 trial 10710 training loss: 0.12199067696928978\n",
      "epoch: 23 trial 10711 training loss: 0.015432794578373432\n",
      "epoch: 23 trial 10712 training loss: 0.003191836760379374\n",
      "epoch: 23 trial 10713 training loss: 0.04662294965237379\n",
      "epoch: 23 trial 10714 training loss: 0.05495388060808182\n",
      "epoch: 23 trial 10715 training loss: 0.02301616780459881\n",
      "epoch: 23 trial 10716 training loss: 0.023086898028850555\n",
      "epoch: 23 trial 10717 training loss: 0.053106918931007385\n",
      "epoch: 23 trial 10718 training loss: 0.03760250099003315\n",
      "epoch: 23 trial 10719 training loss: 0.04419899731874466\n",
      "epoch: 23 trial 10720 training loss: 0.03530576266348362\n",
      "epoch: 23 trial 10721 training loss: 0.015330720925703645\n",
      "epoch: 23 trial 10722 training loss: 0.06927349790930748\n",
      "epoch: 23 trial 10723 training loss: 0.013522813096642494\n",
      "epoch: 23 trial 10724 training loss: 0.024303587153553963\n",
      "epoch: 23 trial 10725 training loss: 0.028823649045079947\n",
      "epoch: 23 trial 10726 training loss: 0.07709407061338425\n",
      "epoch: 23 trial 10727 training loss: 0.019096712116152048\n",
      "epoch: 23 trial 10728 training loss: 0.05169472377747297\n",
      "epoch: 23 trial 10729 training loss: 0.024662774056196213\n",
      "epoch: 23 trial 10730 training loss: 0.04606988467276096\n",
      "epoch: 23 trial 10731 training loss: 0.048133513890206814\n",
      "epoch: 23 trial 10732 training loss: 0.0241883615963161\n",
      "epoch: 23 trial 10733 training loss: 0.0637436369433999\n",
      "epoch: 23 trial 10734 training loss: 0.01610938459634781\n",
      "epoch: 23 trial 10735 training loss: 0.03038587886840105\n",
      "epoch: 23 trial 10736 training loss: 0.053061891347169876\n",
      "epoch: 23 trial 10737 training loss: 0.053856272250413895\n",
      "epoch: 23 trial 10738 training loss: 0.005444802809506655\n",
      "epoch: 23 trial 10739 training loss: 0.043539248406887054\n",
      "epoch: 23 trial 10740 training loss: 0.02473282953724265\n",
      "epoch: 23 trial 10741 training loss: 0.022135153878480196\n",
      "epoch: 23 trial 10742 training loss: 0.06512853689491749\n",
      "epoch: 23 trial 10743 training loss: 0.04528652876615524\n",
      "epoch: 23 trial 10744 training loss: 0.05718952603638172\n",
      "epoch: 23 trial 10745 training loss: 0.01048565423116088\n",
      "epoch: 23 trial 10746 training loss: 0.08198653906583786\n",
      "epoch: 23 trial 10747 training loss: 0.021446199622005224\n",
      "epoch: 23 trial 10748 training loss: 0.05355279054492712\n",
      "epoch: 23 trial 10749 training loss: 0.028911635279655457\n",
      "epoch: 23 trial 10750 training loss: 0.09026867523789406\n",
      "epoch: 23 trial 10751 training loss: 0.04560867138206959\n",
      "epoch: 23 trial 10752 training loss: 0.028924057260155678\n",
      "epoch: 23 trial 10753 training loss: 0.23490029573440552\n",
      "epoch: 23 trial 10754 training loss: 0.07074914500117302\n",
      "epoch: 23 trial 10755 training loss: 0.03104706108570099\n",
      "epoch: 23 trial 10756 training loss: 0.16042136400938034\n",
      "epoch: 23 trial 10757 training loss: 0.07877266593277454\n",
      "epoch: 23 trial 10758 training loss: 0.026101932395249605\n",
      "epoch: 23 trial 10759 training loss: 0.022634520661085844\n",
      "epoch: 23 trial 10760 training loss: 0.021738161332905293\n",
      "epoch: 23 trial 10761 training loss: 0.014939924236387014\n",
      "epoch: 23 trial 10762 training loss: 0.009854678995907307\n",
      "epoch: 23 trial 10763 training loss: 0.05603841505944729\n",
      "epoch: 23 trial 10764 training loss: 0.019339379854500294\n",
      "epoch: 23 trial 10765 training loss: 0.024848762899637222\n",
      "epoch: 23 trial 10766 training loss: 0.03525523375719786\n",
      "epoch: 23 trial 10767 training loss: 0.031646355986595154\n",
      "epoch: 23 trial 10768 training loss: 0.031181706115603447\n",
      "epoch: 23 trial 10769 training loss: 0.021328061819076538\n",
      "epoch: 23 trial 10770 training loss: 0.035493431612849236\n",
      "epoch: 23 trial 10771 training loss: 0.08205229416489601\n",
      "epoch: 23 trial 10772 training loss: 0.03412807546555996\n",
      "epoch: 23 trial 10773 training loss: 0.009416177636012435\n",
      "epoch: 23 trial 10774 training loss: 0.02112811803817749\n",
      "epoch: 23 trial 10775 training loss: 0.024337071925401688\n",
      "epoch: 23 trial 10776 training loss: 0.025288627482950687\n",
      "epoch: 23 trial 10777 training loss: 0.14467867463827133\n",
      "epoch: 23 trial 10778 training loss: 0.024456992745399475\n",
      "epoch: 23 trial 10779 training loss: 0.03667695913463831\n",
      "epoch: 23 trial 10780 training loss: 0.030797310173511505\n",
      "epoch: 23 trial 10781 training loss: 0.03265647869557142\n",
      "epoch: 23 trial 10782 training loss: 0.045383249409496784\n",
      "epoch: 23 trial 10783 training loss: 0.011132493382319808\n",
      "epoch: 23 trial 10784 training loss: 0.04244327452033758\n",
      "epoch: 23 trial 10785 training loss: 0.014066193718463182\n",
      "epoch: 23 trial 10786 training loss: 0.11745768412947655\n",
      "epoch: 23 trial 10787 training loss: 0.01954862754791975\n",
      "epoch: 23 trial 10788 training loss: 0.07157077826559544\n",
      "epoch: 23 trial 10789 training loss: 0.03762043733149767\n",
      "epoch: 23 trial 10790 training loss: 0.03378236573189497\n",
      "epoch: 23 trial 10791 training loss: 0.029508430510759354\n",
      "epoch: 23 trial 10792 training loss: 0.12806754931807518\n",
      "epoch: 23 trial 10793 training loss: 0.00864524906501174\n",
      "epoch: 23 trial 10794 training loss: 0.01507525285705924\n",
      "epoch: 23 trial 10795 training loss: 0.08262194506824017\n",
      "epoch: 23 trial 10796 training loss: 0.024407731369137764\n",
      "epoch: 23 trial 10797 training loss: 0.031371490098536015\n",
      "epoch: 23 trial 10798 training loss: 0.054211956448853016\n",
      "epoch: 23 trial 10799 training loss: 0.023398747202008963\n",
      "epoch: 23 trial 10800 training loss: 0.019299376290291548\n",
      "epoch: 23 trial 10801 training loss: 0.017235067673027515\n",
      "epoch: 23 trial 10802 training loss: 0.034402999095618725\n",
      "epoch: 23 trial 10803 training loss: 0.015017939731478691\n",
      "epoch: 23 trial 10804 training loss: 0.008129209047183394\n",
      "epoch: 23 trial 10805 training loss: 0.026310304179787636\n",
      "epoch: 23 trial 10806 training loss: 0.004193314700387418\n",
      "epoch: 23 trial 10807 training loss: 0.011643830221146345\n",
      "epoch: 23 trial 10808 training loss: 0.018096627667546272\n",
      "epoch: 23 trial 10809 training loss: 0.010994418291375041\n",
      "epoch: 23 trial 10810 training loss: 0.05897133983671665\n",
      "epoch: 23 trial 10811 training loss: 0.004381531849503517\n",
      "epoch: 23 trial 10812 training loss: 0.055465418845415115\n",
      "epoch: 23 trial 10813 training loss: 0.012054197723045945\n",
      "epoch: 23 trial 10814 training loss: 0.01960943778976798\n",
      "epoch: 23 trial 10815 training loss: 0.02257095417007804\n",
      "epoch: 23 trial 10816 training loss: 0.013280448038130999\n",
      "epoch: 23 trial 10817 training loss: 0.03120953682810068\n",
      "epoch: 23 trial 10818 training loss: 0.1256910264492035\n",
      "epoch: 23 trial 10819 training loss: 0.05277491174638271\n",
      "epoch: 23 trial 10820 training loss: 0.010454717557877302\n",
      "epoch: 23 trial 10821 training loss: 0.061800530180335045\n",
      "epoch: 23 trial 10822 training loss: 0.03345039580017328\n",
      "epoch: 23 trial 10823 training loss: 0.023723206482827663\n",
      "epoch: 23 trial 10824 training loss: 0.016913069412112236\n",
      "epoch: 23 trial 10825 training loss: 0.02695927582681179\n",
      "epoch: 23 trial 10826 training loss: 0.03212957177311182\n",
      "epoch: 23 trial 10827 training loss: 0.038380421698093414\n",
      "epoch: 23 trial 10828 training loss: 0.04089399706572294\n",
      "epoch: 23 trial 10829 training loss: 0.07032036036252975\n",
      "epoch: 23 trial 10830 training loss: 0.007907110499218106\n",
      "epoch: 23 trial 10831 training loss: 0.05907273478806019\n",
      "epoch: 23 trial 10832 training loss: 0.020227806642651558\n",
      "epoch: 23 trial 10833 training loss: 0.016892755404114723\n",
      "epoch: 23 trial 10834 training loss: 0.040195723064243793\n",
      "epoch: 23 trial 10835 training loss: 0.049764471128582954\n",
      "epoch: 23 trial 10836 training loss: 0.01593354484066367\n",
      "epoch: 23 trial 10837 training loss: 0.03801264241337776\n",
      "epoch: 23 trial 10838 training loss: 0.07235833257436752\n",
      "epoch: 23 trial 10839 training loss: 0.07029253616929054\n",
      "epoch: 23 trial 10840 training loss: 0.013284556800499558\n",
      "epoch: 23 trial 10841 training loss: 0.019514126237481833\n",
      "epoch: 23 trial 10842 training loss: 0.014592005172744393\n",
      "epoch: 23 trial 10843 training loss: 0.02149649104103446\n",
      "epoch: 23 trial 10844 training loss: 0.03864658810198307\n",
      "epoch: 23 trial 10845 training loss: 0.030302676372230053\n",
      "epoch: 23 trial 10846 training loss: 0.07179876789450645\n",
      "epoch: 23 trial 10847 training loss: 0.11309244856238365\n",
      "epoch: 23 trial 10848 training loss: 0.01876633334904909\n",
      "epoch: 23 trial 10849 training loss: 0.0061640351777896285\n",
      "epoch: 23 trial 10850 training loss: 0.007893905974924564\n",
      "epoch: 23 trial 10851 training loss: 0.011417855741456151\n",
      "epoch: 23 trial 10852 training loss: 0.016174579970538616\n",
      "epoch: 23 trial 10853 training loss: 0.023337566293776035\n",
      "epoch: 23 trial 10854 training loss: 0.09597170352935791\n",
      "epoch: 23 trial 10855 training loss: 0.031732513569295406\n",
      "epoch: 23 trial 10856 training loss: 0.013355213217437267\n",
      "epoch: 23 trial 10857 training loss: 0.09438289143145084\n",
      "epoch: 23 trial 10858 training loss: 0.046038988046348095\n",
      "epoch: 23 trial 10859 training loss: 0.057619484141469\n",
      "epoch: 23 trial 10860 training loss: 0.06203631870448589\n",
      "epoch: 23 trial 10861 training loss: 0.09758059866726398\n",
      "epoch: 23 trial 10862 training loss: 0.014809438027441502\n",
      "epoch: 23 trial 10863 training loss: 0.009278997778892517\n",
      "epoch: 23 trial 10864 training loss: 0.022718746215105057\n",
      "epoch: 23 trial 10865 training loss: 0.022439579479396343\n",
      "epoch: 23 trial 10866 training loss: 0.019488869234919548\n",
      "epoch: 23 trial 10867 training loss: 0.04992100968956947\n",
      "epoch: 23 trial 10868 training loss: 0.07943190261721611\n",
      "epoch: 23 trial 10869 training loss: 0.03161064442247152\n",
      "epoch: 23 trial 10870 training loss: 0.04834902472794056\n",
      "epoch: 23 trial 10871 training loss: 0.03901605773717165\n",
      "epoch: 23 trial 10872 training loss: 0.09661033190786839\n",
      "epoch: 23 trial 10873 training loss: 0.014746446162462234\n",
      "epoch: 23 trial 10874 training loss: 0.012232072418555617\n",
      "epoch: 23 trial 10875 training loss: 0.02190459007397294\n",
      "epoch: 23 trial 10876 training loss: 0.024190419353544712\n",
      "epoch: 23 trial 10877 training loss: 0.045558047480881214\n",
      "epoch: 23 trial 10878 training loss: 0.01477520726621151\n",
      "epoch: 23 trial 10879 training loss: 0.16935299709439278\n",
      "epoch: 23 trial 10880 training loss: 0.015909035690128803\n",
      "epoch: 23 trial 10881 training loss: 0.09401979111135006\n",
      "epoch: 23 trial 10882 training loss: 0.08936097286641598\n",
      "epoch: 23 trial 10883 training loss: 0.01485374802723527\n",
      "epoch: 23 trial 10884 training loss: 0.06717001646757126\n",
      "epoch: 23 trial 10885 training loss: 0.015039729420095682\n",
      "epoch: 23 trial 10886 training loss: 0.04422890953719616\n",
      "epoch: 23 trial 10887 training loss: 0.04794499929994345\n",
      "epoch: 23 trial 10888 training loss: 0.06953023746609688\n",
      "epoch: 23 trial 10889 training loss: 0.01527046738192439\n",
      "epoch: 23 trial 10890 training loss: 0.03708234243094921\n",
      "epoch: 23 trial 10891 training loss: 0.03319777175784111\n",
      "epoch: 23 trial 10892 training loss: 0.030287484638392925\n",
      "epoch: 23 trial 10893 training loss: 0.060639407485723495\n",
      "epoch: 23 trial 10894 training loss: 0.017868739552795887\n",
      "epoch: 23 trial 10895 training loss: 0.0293377498164773\n",
      "epoch: 23 trial 10896 training loss: 0.019658072385936975\n",
      "epoch: 23 trial 10897 training loss: 0.023022510576993227\n",
      "epoch: 23 trial 10898 training loss: 0.038093073293566704\n",
      "epoch: 23 trial 10899 training loss: 0.011411349754780531\n",
      "epoch: 23 trial 10900 training loss: 0.006610844400711358\n",
      "epoch: 23 trial 10901 training loss: 0.032772503793239594\n",
      "epoch: 23 trial 10902 training loss: 0.09131071716547012\n",
      "epoch: 23 trial 10903 training loss: 0.02147663477808237\n",
      "epoch: 23 trial 10904 training loss: 0.09371418319642544\n",
      "epoch: 23 trial 10905 training loss: 0.05241747014224529\n",
      "epoch: 23 trial 10906 training loss: 0.028666160069406033\n",
      "epoch: 23 trial 10907 training loss: 0.14392859861254692\n",
      "epoch: 23 trial 10908 training loss: 0.012347396928817034\n",
      "epoch: 23 trial 10909 training loss: 0.010214282665401697\n",
      "epoch: 23 trial 10910 training loss: 0.010319755412638187\n",
      "epoch: 23 trial 10911 training loss: 0.003964173491112888\n",
      "epoch: 23 trial 10912 training loss: 0.029024079907685518\n",
      "epoch: 23 trial 10913 training loss: 0.03407606575638056\n",
      "epoch: 23 trial 10914 training loss: 0.055377732031047344\n",
      "epoch: 23 trial 10915 training loss: 0.028975562192499638\n",
      "epoch: 23 trial 10916 training loss: 0.046054620295763016\n",
      "epoch: 23 trial 10917 training loss: 0.02036097878590226\n",
      "epoch: 23 trial 10918 training loss: 0.07918841578066349\n",
      "epoch: 23 trial 10919 training loss: 0.01638350309804082\n",
      "epoch: 23 trial 10920 training loss: 0.007465531001798809\n",
      "epoch: 23 trial 10921 training loss: 0.023331040050834417\n",
      "epoch: 23 trial 10922 training loss: 0.004943972686305642\n",
      "epoch: 23 trial 10923 training loss: 0.06815155129879713\n",
      "epoch: 23 trial 10924 training loss: 0.003041697375010699\n",
      "epoch: 23 trial 10925 training loss: 0.023296930361539125\n",
      "epoch: 23 trial 10926 training loss: 0.01810513809323311\n",
      "epoch: 23 trial 10927 training loss: 0.06122618541121483\n",
      "epoch: 23 trial 10928 training loss: 0.04528646171092987\n",
      "epoch: 23 trial 10929 training loss: 0.019076910335570574\n",
      "epoch: 23 trial 10930 training loss: 0.03419029153883457\n",
      "epoch: 23 trial 10931 training loss: 0.06634853966534138\n",
      "epoch: 23 trial 10932 training loss: 0.0284690847620368\n",
      "epoch: 23 trial 10933 training loss: 0.06769401952624321\n",
      "epoch: 23 trial 10934 training loss: 0.022611893713474274\n",
      "epoch: 23 trial 10935 training loss: 0.06645369343459606\n",
      "epoch: 23 trial 10936 training loss: 0.020869264844805002\n",
      "epoch: 23 trial 10937 training loss: 0.007344890618696809\n",
      "epoch: 23 trial 10938 training loss: 0.050023418851196766\n",
      "epoch: 23 trial 10939 training loss: 0.029205859638750553\n",
      "epoch: 23 trial 10940 training loss: 0.017980513628572226\n",
      "epoch: 23 trial 10941 training loss: 0.019718172028660774\n",
      "epoch: 23 trial 10942 training loss: 0.010072034550830722\n",
      "epoch: 23 trial 10943 training loss: 0.021823294926434755\n",
      "epoch: 23 trial 10944 training loss: 0.03458232805132866\n",
      "epoch: 23 trial 10945 training loss: 0.016686501447111368\n",
      "epoch: 23 trial 10946 training loss: 0.03624946065247059\n",
      "epoch: 23 trial 10947 training loss: 0.08174773119390011\n",
      "epoch: 23 trial 10948 training loss: 0.039516088552773\n",
      "epoch: 23 trial 10949 training loss: 0.01377401314675808\n",
      "epoch: 23 trial 10950 training loss: 0.0053191491169855\n",
      "epoch: 23 trial 10951 training loss: 0.007420003646984696\n",
      "epoch: 23 trial 10952 training loss: 0.027205437887459993\n",
      "epoch: 23 trial 10953 training loss: 0.05087517388164997\n",
      "epoch: 23 trial 10954 training loss: 0.043726407922804356\n",
      "epoch: 23 trial 10955 training loss: 0.0036868975730612874\n",
      "epoch: 23 trial 10956 training loss: 0.034575787372887135\n",
      "epoch: 23 trial 10957 training loss: 0.028453443199396133\n",
      "epoch: 23 trial 10958 training loss: 0.018521174555644393\n",
      "epoch: 23 trial 10959 training loss: 0.01538465777412057\n",
      "epoch: 23 trial 10960 training loss: 0.017521978355944157\n",
      "epoch: 23 trial 10961 training loss: 0.04373234324157238\n",
      "epoch: 23 trial 10962 training loss: 0.02337357262149453\n",
      "epoch: 23 trial 10963 training loss: 0.030841490253806114\n",
      "epoch: 23 trial 10964 training loss: 0.07364166341722012\n",
      "epoch: 23 trial 10965 training loss: 0.022990053053945303\n",
      "epoch: 23 trial 10966 training loss: 0.0356513187289238\n",
      "epoch: 23 trial 10967 training loss: 0.06079951208084822\n",
      "epoch: 23 trial 10968 training loss: 0.0367138022556901\n",
      "epoch: 23 trial 10969 training loss: 0.06500694155693054\n",
      "epoch: 23 trial 10970 training loss: 0.01727598672732711\n",
      "epoch: 23 trial 10971 training loss: 0.030184549279510975\n",
      "epoch: 23 trial 10972 training loss: 0.02663576230406761\n",
      "epoch: 23 trial 10973 training loss: 0.023045837879180908\n",
      "epoch: 23 trial 10974 training loss: 0.026391522958874702\n",
      "epoch: 23 trial 10975 training loss: 0.08369138464331627\n",
      "epoch: 23 trial 10976 training loss: 0.014411967247724533\n",
      "epoch: 23 trial 10977 training loss: 0.08427778631448746\n",
      "epoch: 23 trial 10978 training loss: 0.029740876518189907\n",
      "epoch: 23 trial 10979 training loss: 0.007112782564945519\n",
      "epoch: 23 trial 10980 training loss: 0.04917926248162985\n",
      "epoch: 23 trial 10981 training loss: 0.056904193945229053\n",
      "epoch: 23 trial 10982 training loss: 0.01084794825874269\n",
      "epoch: 23 trial 10983 training loss: 0.03513884637504816\n",
      "epoch: 23 trial 10984 training loss: 0.011435339692980051\n",
      "epoch: 23 trial 10985 training loss: 0.03239358775317669\n",
      "epoch: 23 trial 10986 training loss: 0.023474032524973154\n",
      "epoch: 23 trial 10987 training loss: 0.03896447457373142\n",
      "epoch: 23 trial 10988 training loss: 0.27101464569568634\n",
      "epoch: 23 trial 10989 training loss: 0.058356648311018944\n",
      "epoch: 23 trial 10990 training loss: 0.00869718473404646\n",
      "epoch: 23 trial 10991 training loss: 0.01812162809073925\n",
      "epoch: 23 trial 10992 training loss: 0.11682607606053352\n",
      "epoch: 23 trial 10993 training loss: 0.05080603063106537\n",
      "epoch: 23 trial 10994 training loss: 0.07007797434926033\n",
      "epoch: 23 trial 10995 training loss: 0.05440972559154034\n",
      "epoch: 23 trial 10996 training loss: 0.030231842771172523\n",
      "epoch: 23 trial 10997 training loss: 0.008695976343005896\n",
      "epoch: 23 trial 10998 training loss: 0.07009664177894592\n",
      "epoch: 23 trial 10999 training loss: 0.029838004149496555\n",
      "epoch: 23 trial 11000 training loss: 0.08741502091288567\n",
      "epoch: 23 trial 11001 training loss: 0.014067443553358316\n",
      "epoch: 23 trial 11002 training loss: 0.005967310164123774\n",
      "epoch: 23 trial 11003 training loss: 0.025962425861507654\n",
      "epoch: 23 trial 11004 training loss: 0.005346593097783625\n",
      "epoch: 23 trial 11005 training loss: 0.015149210579693317\n",
      "epoch: 23 trial 11006 training loss: 0.09379317425191402\n",
      "epoch: 23 trial 11007 training loss: 0.07282623276114464\n",
      "epoch: 23 trial 11008 training loss: 0.026874341070652008\n",
      "epoch: 23 trial 11009 training loss: 0.10690135136246681\n",
      "epoch: 23 trial 11010 training loss: 0.08465806767344475\n",
      "epoch: 23 trial 11011 training loss: 0.025098306126892567\n",
      "epoch: 23 trial 11012 training loss: 0.06205979734659195\n",
      "epoch: 23 trial 11013 training loss: 0.03550301305949688\n",
      "epoch: 23 trial 11014 training loss: 0.013057224918156862\n",
      "epoch: 23 trial 11015 training loss: 0.004701867350377142\n",
      "epoch: 23 trial 11016 training loss: 0.127593494951725\n",
      "epoch: 23 trial 11017 training loss: 0.06420488096773624\n",
      "epoch: 23 trial 11018 training loss: 0.04494943656027317\n",
      "epoch: 23 trial 11019 training loss: 0.058611782267689705\n",
      "epoch: 23 trial 11020 training loss: 0.02535099908709526\n",
      "epoch: 23 trial 11021 training loss: 0.04192967992275953\n",
      "epoch: 23 trial 11022 training loss: 0.014672665856778622\n",
      "epoch: 23 trial 11023 training loss: 0.05227037984877825\n",
      "epoch: 23 trial 11024 training loss: 0.02811238169670105\n",
      "epoch: 23 trial 11025 training loss: 0.035496870055794716\n",
      "epoch: 23 trial 11026 training loss: 0.09215083718299866\n",
      "epoch: 23 trial 11027 training loss: 0.02063983678817749\n",
      "epoch: 23 trial 11028 training loss: 0.010257998714223504\n",
      "epoch: 23 trial 11029 training loss: 0.03680945560336113\n",
      "epoch: 23 trial 11030 training loss: 0.004769439226947725\n",
      "epoch: 23 trial 11031 training loss: 0.03945171274244785\n",
      "epoch: 23 trial 11032 training loss: 0.04162514675408602\n",
      "epoch: 23 trial 11033 training loss: 0.014352508820593357\n",
      "epoch: 23 trial 11034 training loss: 0.00203692534705624\n",
      "epoch: 23 trial 11035 training loss: 0.0035308701335452497\n",
      "epoch: 23 trial 11036 training loss: 0.015531770419329405\n",
      "epoch: 23 trial 11037 training loss: 0.036420698277652264\n",
      "epoch: 23 trial 11038 training loss: 0.004312008386477828\n",
      "epoch: 23 trial 11039 training loss: 0.00529935397207737\n",
      "epoch: 23 trial 11040 training loss: 0.006675387965515256\n",
      "epoch: 23 trial 11041 training loss: 0.008418421959504485\n",
      "epoch: 23 trial 11042 training loss: 0.010979414684697986\n",
      "epoch: 23 trial 11043 training loss: 0.026015928015112877\n",
      "epoch: 23 trial 11044 training loss: 0.11264651641249657\n",
      "epoch: 23 trial 11045 training loss: 0.018680718261748552\n",
      "epoch: 23 trial 11046 training loss: 0.016618108609691262\n",
      "epoch: 23 trial 11047 training loss: 0.030902660451829433\n",
      "epoch: 23 trial 11048 training loss: 0.04100365936756134\n",
      "epoch: 23 trial 11049 training loss: 0.03684936137869954\n",
      "epoch: 23 trial 11050 training loss: 0.02989121526479721\n",
      "epoch: 23 trial 11051 training loss: 0.04286842793226242\n",
      "epoch: 23 trial 11052 training loss: 0.07036463543772697\n",
      "epoch: 23 trial 11053 training loss: 0.11098180338740349\n",
      "epoch: 23 trial 11054 training loss: 0.020121016073971987\n",
      "epoch: 23 trial 11055 training loss: 0.022333649452775717\n",
      "epoch: 23 trial 11056 training loss: 0.08480258472263813\n",
      "epoch: 23 trial 11057 training loss: 0.019368832930922508\n",
      "epoch: 23 trial 11058 training loss: 0.051655812188982964\n",
      "epoch: 23 trial 11059 training loss: 0.09746458940207958\n",
      "epoch: 23 trial 11060 training loss: 0.09079158306121826\n",
      "epoch: 23 trial 11061 training loss: 0.03972304053604603\n",
      "epoch: 23 trial 11062 training loss: 0.11049586907029152\n",
      "epoch: 23 trial 11063 training loss: 0.015941850608214736\n",
      "epoch: 23 trial 11064 training loss: 0.0379975875839591\n",
      "epoch: 23 trial 11065 training loss: 0.03213408775627613\n",
      "epoch: 23 trial 11066 training loss: 0.1088721826672554\n",
      "epoch: 23 trial 11067 training loss: 0.0095321258995682\n",
      "epoch: 23 trial 11068 training loss: 0.007772848242893815\n",
      "epoch: 23 trial 11069 training loss: 0.019641334656625986\n",
      "epoch: 23 trial 11070 training loss: 0.02874281769618392\n",
      "epoch: 23 trial 11071 training loss: 0.05729351565241814\n",
      "epoch: 23 trial 11072 training loss: 0.05444664042443037\n",
      "epoch: 23 trial 11073 training loss: 0.01696654548868537\n",
      "epoch: 23 trial 11074 training loss: 0.05325986351817846\n",
      "epoch: 23 trial 11075 training loss: 0.014534756541252136\n",
      "epoch: 23 trial 11076 training loss: 0.016493793576955795\n",
      "epoch: 23 trial 11077 training loss: 0.020465198904275894\n",
      "epoch: 23 trial 11078 training loss: 0.013946590479463339\n",
      "epoch: 23 trial 11079 training loss: 0.019109380431473255\n",
      "epoch: 23 trial 11080 training loss: 0.04071189742535353\n",
      "epoch: 23 trial 11081 training loss: 0.026166997849941254\n",
      "epoch: 23 trial 11082 training loss: 0.02112488029524684\n",
      "epoch: 23 trial 11083 training loss: 0.019858392421156168\n",
      "epoch: 23 trial 11084 training loss: 0.017318225000053644\n",
      "epoch: 23 trial 11085 training loss: 0.005240430356934667\n",
      "epoch: 23 trial 11086 training loss: 0.011370161082595587\n",
      "epoch: 23 trial 11087 training loss: 0.01658552885055542\n",
      "epoch: 23 trial 11088 training loss: 0.015086467377841473\n",
      "epoch: 23 trial 11089 training loss: 0.0058608854888007045\n",
      "epoch: 23 trial 11090 training loss: 0.01339389942586422\n",
      "epoch: 23 trial 11091 training loss: 0.08110437728464603\n",
      "epoch: 23 trial 11092 training loss: 0.010118791367858648\n",
      "epoch: 23 trial 11093 training loss: 0.03489406034350395\n",
      "epoch: 23 trial 11094 training loss: 0.013467784970998764\n",
      "epoch: 23 trial 11095 training loss: 0.05112369731068611\n",
      "epoch: 23 trial 11096 training loss: 0.041995229199528694\n",
      "epoch: 23 trial 11097 training loss: 0.04324761824682355\n",
      "epoch: 23 trial 11098 training loss: 0.025454478804022074\n",
      "epoch: 23 trial 11099 training loss: 0.01819488126784563\n",
      "epoch: 23 trial 11100 training loss: 0.09468955174088478\n",
      "epoch: 23 trial 11101 training loss: 0.0058191599091514945\n",
      "epoch: 23 trial 11102 training loss: 0.01776104560121894\n",
      "epoch: 23 trial 11103 training loss: 0.12397512048482895\n",
      "epoch: 23 trial 11104 training loss: 0.023405204992741346\n",
      "epoch: 23 trial 11105 training loss: 0.020123983733356\n",
      "epoch: 23 trial 11106 training loss: 0.019388056825846434\n",
      "epoch: 23 trial 11107 training loss: 0.012202108511701226\n",
      "epoch: 23 trial 11108 training loss: 0.028193244710564613\n",
      "epoch: 23 trial 11109 training loss: 0.012384138768538833\n",
      "epoch: 23 trial 11110 training loss: 0.05482988432049751\n",
      "epoch: 23 trial 11111 training loss: 0.1546327918767929\n",
      "epoch: 23 trial 11112 training loss: 0.06655332073569298\n",
      "epoch: 23 trial 11113 training loss: 0.029502658173441887\n",
      "epoch: 23 trial 11114 training loss: 0.06235881336033344\n",
      "epoch: 23 trial 11115 training loss: 0.09629041515290737\n",
      "epoch: 23 trial 11116 training loss: 0.026107057463377714\n",
      "epoch: 23 trial 11117 training loss: 0.071023378521204\n",
      "epoch: 23 trial 11118 training loss: 0.00828186352737248\n",
      "epoch: 23 trial 11119 training loss: 0.04077999945729971\n",
      "epoch: 23 trial 11120 training loss: 0.0126750145573169\n",
      "epoch: 23 trial 11121 training loss: 0.026534320786595345\n",
      "epoch: 23 trial 11122 training loss: 0.11114073172211647\n",
      "epoch: 23 trial 11123 training loss: 0.0459437957033515\n",
      "epoch: 23 trial 11124 training loss: 0.020131013821810484\n",
      "epoch: 23 trial 11125 training loss: 0.02006178768351674\n",
      "epoch: 23 trial 11126 training loss: 0.06715300679206848\n",
      "epoch: 23 trial 11127 training loss: 0.014794660033658147\n",
      "epoch: 23 trial 11128 training loss: 0.05603584460914135\n",
      "epoch: 23 trial 11129 training loss: 0.03598120901733637\n",
      "epoch: 23 trial 11130 training loss: 0.04195013269782066\n",
      "epoch: 23 trial 11131 training loss: 0.012624298455193639\n",
      "epoch: 23 trial 11132 training loss: 0.05023328494280577\n",
      "epoch: 24 trial 11133 training loss: 0.014827818609774113\n",
      "epoch: 24 trial 11134 training loss: 0.012447073357179761\n",
      "epoch: 24 trial 11135 training loss: 0.00952736649196595\n",
      "epoch: 24 trial 11136 training loss: 0.0594165176153183\n",
      "epoch: 24 trial 11137 training loss: 0.004226597375236452\n",
      "epoch: 24 trial 11138 training loss: 0.00525521847885102\n",
      "epoch: 24 trial 11139 training loss: 0.024665529374033213\n",
      "epoch: 24 trial 11140 training loss: 0.031184238381683826\n",
      "epoch: 24 trial 11141 training loss: 0.005516326054930687\n",
      "epoch: 24 trial 11142 training loss: 0.0146791972219944\n",
      "epoch: 24 trial 11143 training loss: 0.05473523586988449\n",
      "epoch: 24 trial 11144 training loss: 0.03131725825369358\n",
      "epoch: 24 trial 11145 training loss: 0.014880253467708826\n",
      "epoch: 24 trial 11146 training loss: 0.016192364040762186\n",
      "epoch: 24 trial 11147 training loss: 0.021619907580316067\n",
      "epoch: 24 trial 11148 training loss: 0.0221950588747859\n",
      "epoch: 24 trial 11149 training loss: 0.10812577605247498\n",
      "epoch: 24 trial 11150 training loss: 0.047548952512443066\n",
      "epoch: 24 trial 11151 training loss: 0.01693898718804121\n",
      "epoch: 24 trial 11152 training loss: 0.019644450396299362\n",
      "epoch: 24 trial 11153 training loss: 0.017076951917260885\n",
      "epoch: 24 trial 11154 training loss: 0.07111045904457569\n",
      "epoch: 24 trial 11155 training loss: 0.011821763357147574\n",
      "epoch: 24 trial 11156 training loss: 0.0033677270403131843\n",
      "epoch: 24 trial 11157 training loss: 0.14190300554037094\n",
      "epoch: 24 trial 11158 training loss: 0.03025474213063717\n",
      "epoch: 24 trial 11159 training loss: 0.011872101109474897\n",
      "epoch: 24 trial 11160 training loss: 0.09700105153024197\n",
      "epoch: 24 trial 11161 training loss: 0.014486233936622739\n",
      "epoch: 24 trial 11162 training loss: 0.019196489825844765\n",
      "epoch: 24 trial 11163 training loss: 0.026182119734585285\n",
      "epoch: 24 trial 11164 training loss: 0.011659426498226821\n",
      "epoch: 24 trial 11165 training loss: 0.026469947304576635\n",
      "epoch: 24 trial 11166 training loss: 0.047917612828314304\n",
      "epoch: 24 trial 11167 training loss: 0.015202876646071672\n",
      "epoch: 24 trial 11168 training loss: 0.0581034179776907\n",
      "epoch: 24 trial 11169 training loss: 0.1456623412668705\n",
      "epoch: 24 trial 11170 training loss: 0.275667741894722\n",
      "epoch: 24 trial 11171 training loss: 0.045717488043010235\n",
      "epoch: 24 trial 11172 training loss: 0.0816915463656187\n",
      "epoch: 24 trial 11173 training loss: 0.09260017797350883\n",
      "epoch: 24 trial 11174 training loss: 0.011609622742980719\n",
      "epoch: 24 trial 11175 training loss: 0.08689165860414505\n",
      "epoch: 24 trial 11176 training loss: 0.03615552932024002\n",
      "epoch: 24 trial 11177 training loss: 0.028078154660761356\n",
      "epoch: 24 trial 11178 training loss: 0.05763260833919048\n",
      "epoch: 24 trial 11179 training loss: 0.007614273927174509\n",
      "epoch: 24 trial 11180 training loss: 0.015987944090738893\n",
      "epoch: 24 trial 11181 training loss: 0.023507707752287388\n",
      "epoch: 24 trial 11182 training loss: 0.07710813544690609\n",
      "epoch: 24 trial 11183 training loss: 0.03514603618532419\n",
      "epoch: 24 trial 11184 training loss: 0.09133044444024563\n",
      "epoch: 24 trial 11185 training loss: 0.0900325309485197\n",
      "epoch: 24 trial 11186 training loss: 0.12088700011372566\n",
      "epoch: 24 trial 11187 training loss: 0.07310489192605019\n",
      "epoch: 24 trial 11188 training loss: 0.06018194183707237\n",
      "epoch: 24 trial 11189 training loss: 0.029697583056986332\n",
      "epoch: 24 trial 11190 training loss: 0.013986155856400728\n",
      "epoch: 24 trial 11191 training loss: 0.012547600781545043\n",
      "epoch: 24 trial 11192 training loss: 0.0883763674646616\n",
      "epoch: 24 trial 11193 training loss: 0.019760344177484512\n",
      "epoch: 24 trial 11194 training loss: 0.08552947081625462\n",
      "epoch: 24 trial 11195 training loss: 0.04696566518396139\n",
      "epoch: 24 trial 11196 training loss: 0.004511775681748986\n",
      "epoch: 24 trial 11197 training loss: 0.02683909982442856\n",
      "epoch: 24 trial 11198 training loss: 0.029745795764029026\n",
      "epoch: 24 trial 11199 training loss: 0.020339233335107565\n",
      "epoch: 24 trial 11200 training loss: 0.010854138294234872\n",
      "epoch: 24 trial 11201 training loss: 0.04498358815908432\n",
      "epoch: 24 trial 11202 training loss: 0.10041390359401703\n",
      "epoch: 24 trial 11203 training loss: 0.04995615128427744\n",
      "epoch: 24 trial 11204 training loss: 0.019536152482032776\n",
      "epoch: 24 trial 11205 training loss: 0.01097714388743043\n",
      "epoch: 24 trial 11206 training loss: 0.07064958103001118\n",
      "epoch: 24 trial 11207 training loss: 0.016856742557138205\n",
      "epoch: 24 trial 11208 training loss: 0.024615904316306114\n",
      "epoch: 24 trial 11209 training loss: 0.026285219937562943\n",
      "epoch: 24 trial 11210 training loss: 0.09162401594221592\n",
      "epoch: 24 trial 11211 training loss: 0.011783500434830785\n",
      "epoch: 24 trial 11212 training loss: 0.055979310534894466\n",
      "epoch: 24 trial 11213 training loss: 0.035086017567664385\n",
      "epoch: 24 trial 11214 training loss: 0.03871842846274376\n",
      "epoch: 24 trial 11215 training loss: 0.048350307159125805\n",
      "epoch: 24 trial 11216 training loss: 0.02762711001560092\n",
      "epoch: 24 trial 11217 training loss: 0.07602805830538273\n",
      "epoch: 24 trial 11218 training loss: 0.017887773923575878\n",
      "epoch: 24 trial 11219 training loss: 0.026430560275912285\n",
      "epoch: 24 trial 11220 training loss: 0.0699117798358202\n",
      "epoch: 24 trial 11221 training loss: 0.055601563304662704\n",
      "epoch: 24 trial 11222 training loss: 0.005466817645356059\n",
      "epoch: 24 trial 11223 training loss: 0.04299814160913229\n",
      "epoch: 24 trial 11224 training loss: 0.024179290048778057\n",
      "epoch: 24 trial 11225 training loss: 0.022559344302862883\n",
      "epoch: 24 trial 11226 training loss: 0.06965509802103043\n",
      "epoch: 24 trial 11227 training loss: 0.047425384633243084\n",
      "epoch: 24 trial 11228 training loss: 0.05495051294565201\n",
      "epoch: 24 trial 11229 training loss: 0.010917735053226352\n",
      "epoch: 24 trial 11230 training loss: 0.07840397581458092\n",
      "epoch: 24 trial 11231 training loss: 0.026254205033183098\n",
      "epoch: 24 trial 11232 training loss: 0.044664738699793816\n",
      "epoch: 24 trial 11233 training loss: 0.030448850244283676\n",
      "epoch: 24 trial 11234 training loss: 0.09339271485805511\n",
      "epoch: 24 trial 11235 training loss: 0.04677167069166899\n",
      "epoch: 24 trial 11236 training loss: 0.03094652947038412\n",
      "epoch: 24 trial 11237 training loss: 0.23970939964056015\n",
      "epoch: 24 trial 11238 training loss: 0.06371311843395233\n",
      "epoch: 24 trial 11239 training loss: 0.03045692201703787\n",
      "epoch: 24 trial 11240 training loss: 0.14547144807875156\n",
      "epoch: 24 trial 11241 training loss: 0.07309200800955296\n",
      "epoch: 24 trial 11242 training loss: 0.025390796829015017\n",
      "epoch: 24 trial 11243 training loss: 0.02260325662791729\n",
      "epoch: 24 trial 11244 training loss: 0.024260510690510273\n",
      "epoch: 24 trial 11245 training loss: 0.016366134397685528\n",
      "epoch: 24 trial 11246 training loss: 0.010475394781678915\n",
      "epoch: 24 trial 11247 training loss: 0.05503646284341812\n",
      "epoch: 24 trial 11248 training loss: 0.018881184980273247\n",
      "epoch: 24 trial 11249 training loss: 0.02344869077205658\n",
      "epoch: 24 trial 11250 training loss: 0.030876241624355316\n",
      "epoch: 24 trial 11251 training loss: 0.029426801949739456\n",
      "epoch: 24 trial 11252 training loss: 0.026231604162603617\n",
      "epoch: 24 trial 11253 training loss: 0.01842798339203\n",
      "epoch: 24 trial 11254 training loss: 0.030994263477623463\n",
      "epoch: 24 trial 11255 training loss: 0.07330094277858734\n",
      "epoch: 24 trial 11256 training loss: 0.03190201427787542\n",
      "epoch: 24 trial 11257 training loss: 0.00968964770436287\n",
      "epoch: 24 trial 11258 training loss: 0.020143819507211447\n",
      "epoch: 24 trial 11259 training loss: 0.025467789266258478\n",
      "epoch: 24 trial 11260 training loss: 0.026111927814781666\n",
      "epoch: 24 trial 11261 training loss: 0.12922652065753937\n",
      "epoch: 24 trial 11262 training loss: 0.0243230857886374\n",
      "epoch: 24 trial 11263 training loss: 0.03298632521182299\n",
      "epoch: 24 trial 11264 training loss: 0.028155255131423473\n",
      "epoch: 24 trial 11265 training loss: 0.030235186219215393\n",
      "epoch: 24 trial 11266 training loss: 0.04196973517537117\n",
      "epoch: 24 trial 11267 training loss: 0.011587009998038411\n",
      "epoch: 24 trial 11268 training loss: 0.04725650604814291\n",
      "epoch: 24 trial 11269 training loss: 0.014841461088508368\n",
      "epoch: 24 trial 11270 training loss: 0.11205166392028332\n",
      "epoch: 24 trial 11271 training loss: 0.016629944555461407\n",
      "epoch: 24 trial 11272 training loss: 0.05848166439682245\n",
      "epoch: 24 trial 11273 training loss: 0.04038182646036148\n",
      "epoch: 24 trial 11274 training loss: 0.03271926939487457\n",
      "epoch: 24 trial 11275 training loss: 0.027991398237645626\n",
      "epoch: 24 trial 11276 training loss: 0.13179617375135422\n",
      "epoch: 24 trial 11277 training loss: 0.00822451920248568\n",
      "epoch: 24 trial 11278 training loss: 0.01812902232632041\n",
      "epoch: 24 trial 11279 training loss: 0.08799177780747414\n",
      "epoch: 24 trial 11280 training loss: 0.022192022297531366\n",
      "epoch: 24 trial 11281 training loss: 0.03183422237634659\n",
      "epoch: 24 trial 11282 training loss: 0.053719693794846535\n",
      "epoch: 24 trial 11283 training loss: 0.025376814417541027\n",
      "epoch: 24 trial 11284 training loss: 0.019956171046942472\n",
      "epoch: 24 trial 11285 training loss: 0.016582718584686518\n",
      "epoch: 24 trial 11286 training loss: 0.034731619991362095\n",
      "epoch: 24 trial 11287 training loss: 0.01223417604342103\n",
      "epoch: 24 trial 11288 training loss: 0.00978154269978404\n",
      "epoch: 24 trial 11289 training loss: 0.030273488722741604\n",
      "epoch: 24 trial 11290 training loss: 0.004497309913858771\n",
      "epoch: 24 trial 11291 training loss: 0.012131818803027272\n",
      "epoch: 24 trial 11292 training loss: 0.01700588036328554\n",
      "epoch: 24 trial 11293 training loss: 0.011320729041472077\n",
      "epoch: 24 trial 11294 training loss: 0.05158120673149824\n",
      "epoch: 24 trial 11295 training loss: 0.005022374098189175\n",
      "epoch: 24 trial 11296 training loss: 0.053065601736307144\n",
      "epoch: 24 trial 11297 training loss: 0.011857117526233196\n",
      "epoch: 24 trial 11298 training loss: 0.020782219246029854\n",
      "epoch: 24 trial 11299 training loss: 0.02313242293894291\n",
      "epoch: 24 trial 11300 training loss: 0.013365104794502258\n",
      "epoch: 24 trial 11301 training loss: 0.03402977343648672\n",
      "epoch: 24 trial 11302 training loss: 0.11963780224323273\n",
      "epoch: 24 trial 11303 training loss: 0.052721887826919556\n",
      "epoch: 24 trial 11304 training loss: 0.009520520456135273\n",
      "epoch: 24 trial 11305 training loss: 0.06880161911249161\n",
      "epoch: 24 trial 11306 training loss: 0.03264528885483742\n",
      "epoch: 24 trial 11307 training loss: 0.02193465526215732\n",
      "epoch: 24 trial 11308 training loss: 0.017243722453713417\n",
      "epoch: 24 trial 11309 training loss: 0.02361946552991867\n",
      "epoch: 24 trial 11310 training loss: 0.03196683432906866\n",
      "epoch: 24 trial 11311 training loss: 0.03693792782723904\n",
      "epoch: 24 trial 11312 training loss: 0.03131838096305728\n",
      "epoch: 24 trial 11313 training loss: 0.07871191017329693\n",
      "epoch: 24 trial 11314 training loss: 0.007697677938267589\n",
      "epoch: 24 trial 11315 training loss: 0.06223898008465767\n",
      "epoch: 24 trial 11316 training loss: 0.017925193067640066\n",
      "epoch: 24 trial 11317 training loss: 0.0171297425404191\n",
      "epoch: 24 trial 11318 training loss: 0.04744784161448479\n",
      "epoch: 24 trial 11319 training loss: 0.051517054438591\n",
      "epoch: 24 trial 11320 training loss: 0.016318057663738728\n",
      "epoch: 24 trial 11321 training loss: 0.03613669890910387\n",
      "epoch: 24 trial 11322 training loss: 0.07851701229810715\n",
      "epoch: 24 trial 11323 training loss: 0.07513189874589443\n",
      "epoch: 24 trial 11324 training loss: 0.01260402170009911\n",
      "epoch: 24 trial 11325 training loss: 0.022167695220559835\n",
      "epoch: 24 trial 11326 training loss: 0.015530301723629236\n",
      "epoch: 24 trial 11327 training loss: 0.019064235500991344\n",
      "epoch: 24 trial 11328 training loss: 0.04247210267931223\n",
      "epoch: 24 trial 11329 training loss: 0.031312487088143826\n",
      "epoch: 24 trial 11330 training loss: 0.0698554776608944\n",
      "epoch: 24 trial 11331 training loss: 0.11096037738025188\n",
      "epoch: 24 trial 11332 training loss: 0.018899510614573956\n",
      "epoch: 24 trial 11333 training loss: 0.006169331143610179\n",
      "epoch: 24 trial 11334 training loss: 0.007577391341328621\n",
      "epoch: 24 trial 11335 training loss: 0.011084427125751972\n",
      "epoch: 24 trial 11336 training loss: 0.015147995669394732\n",
      "epoch: 24 trial 11337 training loss: 0.024584234692156315\n",
      "epoch: 24 trial 11338 training loss: 0.10034248232841492\n",
      "epoch: 24 trial 11339 training loss: 0.03353999648243189\n",
      "epoch: 24 trial 11340 training loss: 0.012610113248229027\n",
      "epoch: 24 trial 11341 training loss: 0.09688523411750793\n",
      "epoch: 24 trial 11342 training loss: 0.04276515822857618\n",
      "epoch: 24 trial 11343 training loss: 0.05900238826870918\n",
      "epoch: 24 trial 11344 training loss: 0.06869629956781864\n",
      "epoch: 24 trial 11345 training loss: 0.0991462767124176\n",
      "epoch: 24 trial 11346 training loss: 0.014647838659584522\n",
      "epoch: 24 trial 11347 training loss: 0.009723018622025847\n",
      "epoch: 24 trial 11348 training loss: 0.02274189330637455\n",
      "epoch: 24 trial 11349 training loss: 0.024159580934792757\n",
      "epoch: 24 trial 11350 training loss: 0.021255822386592627\n",
      "epoch: 24 trial 11351 training loss: 0.04771589860320091\n",
      "epoch: 24 trial 11352 training loss: 0.07861347869038582\n",
      "epoch: 24 trial 11353 training loss: 0.03001786768436432\n",
      "epoch: 24 trial 11354 training loss: 0.048165096901357174\n",
      "epoch: 24 trial 11355 training loss: 0.04277467727661133\n",
      "epoch: 24 trial 11356 training loss: 0.09604772552847862\n",
      "epoch: 24 trial 11357 training loss: 0.013055951101705432\n",
      "epoch: 24 trial 11358 training loss: 0.013289749156683683\n",
      "epoch: 24 trial 11359 training loss: 0.022013770882040262\n",
      "epoch: 24 trial 11360 training loss: 0.024014607537537813\n",
      "epoch: 24 trial 11361 training loss: 0.04606272280216217\n",
      "epoch: 24 trial 11362 training loss: 0.015253094024956226\n",
      "epoch: 24 trial 11363 training loss: 0.17480158060789108\n",
      "epoch: 24 trial 11364 training loss: 0.017515707295387983\n",
      "epoch: 24 trial 11365 training loss: 0.09281676076352596\n",
      "epoch: 24 trial 11366 training loss: 0.08718560449779034\n",
      "epoch: 24 trial 11367 training loss: 0.013372612651437521\n",
      "epoch: 24 trial 11368 training loss: 0.06491735391318798\n",
      "epoch: 24 trial 11369 training loss: 0.016229405533522367\n",
      "epoch: 24 trial 11370 training loss: 0.04817801155149937\n",
      "epoch: 24 trial 11371 training loss: 0.04688153136521578\n",
      "epoch: 24 trial 11372 training loss: 0.0622370969504118\n",
      "epoch: 24 trial 11373 training loss: 0.014730866067111492\n",
      "epoch: 24 trial 11374 training loss: 0.037917694076895714\n",
      "epoch: 24 trial 11375 training loss: 0.033549741841852665\n",
      "epoch: 24 trial 11376 training loss: 0.033465348184108734\n",
      "epoch: 24 trial 11377 training loss: 0.0631780307739973\n",
      "epoch: 24 trial 11378 training loss: 0.01754090003669262\n",
      "epoch: 24 trial 11379 training loss: 0.023754665162414312\n",
      "epoch: 24 trial 11380 training loss: 0.019328495021909475\n",
      "epoch: 24 trial 11381 training loss: 0.023325929418206215\n",
      "epoch: 24 trial 11382 training loss: 0.03790522925555706\n",
      "epoch: 24 trial 11383 training loss: 0.010730855166912079\n",
      "epoch: 24 trial 11384 training loss: 0.007823829771950841\n",
      "epoch: 24 trial 11385 training loss: 0.03412639629095793\n",
      "epoch: 24 trial 11386 training loss: 0.08494758605957031\n",
      "epoch: 24 trial 11387 training loss: 0.02076523145660758\n",
      "epoch: 24 trial 11388 training loss: 0.0882713794708252\n",
      "epoch: 24 trial 11389 training loss: 0.058862168341875076\n",
      "epoch: 24 trial 11390 training loss: 0.02910406282171607\n",
      "epoch: 24 trial 11391 training loss: 0.13281836733222008\n",
      "epoch: 24 trial 11392 training loss: 0.012661061016842723\n",
      "epoch: 24 trial 11393 training loss: 0.010223945835605264\n",
      "epoch: 24 trial 11394 training loss: 0.009047696832567453\n",
      "epoch: 24 trial 11395 training loss: 0.004409988061524928\n",
      "epoch: 24 trial 11396 training loss: 0.0316774258390069\n",
      "epoch: 24 trial 11397 training loss: 0.03305951692163944\n",
      "epoch: 24 trial 11398 training loss: 0.048583267256617546\n",
      "epoch: 24 trial 11399 training loss: 0.028528869152069092\n",
      "epoch: 24 trial 11400 training loss: 0.04530653636902571\n",
      "epoch: 24 trial 11401 training loss: 0.01889175781980157\n",
      "epoch: 24 trial 11402 training loss: 0.08040595799684525\n",
      "epoch: 24 trial 11403 training loss: 0.018090987112373114\n",
      "epoch: 24 trial 11404 training loss: 0.0074590754229575396\n",
      "epoch: 24 trial 11405 training loss: 0.02093055797740817\n",
      "epoch: 24 trial 11406 training loss: 0.00520347198471427\n",
      "epoch: 24 trial 11407 training loss: 0.07087492011487484\n",
      "epoch: 24 trial 11408 training loss: 0.0029241295997053385\n",
      "epoch: 24 trial 11409 training loss: 0.022417749743908644\n",
      "epoch: 24 trial 11410 training loss: 0.017489840742200613\n",
      "epoch: 24 trial 11411 training loss: 0.05892144329845905\n",
      "epoch: 24 trial 11412 training loss: 0.044176435098052025\n",
      "epoch: 24 trial 11413 training loss: 0.02531138900667429\n",
      "epoch: 24 trial 11414 training loss: 0.029951238073408604\n",
      "epoch: 24 trial 11415 training loss: 0.057554288767278194\n",
      "epoch: 24 trial 11416 training loss: 0.02541497629135847\n",
      "epoch: 24 trial 11417 training loss: 0.07083302177488804\n",
      "epoch: 24 trial 11418 training loss: 0.019293483812361956\n",
      "epoch: 24 trial 11419 training loss: 0.06341150403022766\n",
      "epoch: 24 trial 11420 training loss: 0.02031528064981103\n",
      "epoch: 24 trial 11421 training loss: 0.007220630068331957\n",
      "epoch: 24 trial 11422 training loss: 0.0505507942289114\n",
      "epoch: 24 trial 11423 training loss: 0.027743912767618895\n",
      "epoch: 24 trial 11424 training loss: 0.01629383396357298\n",
      "epoch: 24 trial 11425 training loss: 0.01816246472299099\n",
      "epoch: 24 trial 11426 training loss: 0.00946786068379879\n",
      "epoch: 24 trial 11427 training loss: 0.022616500966250896\n",
      "epoch: 24 trial 11428 training loss: 0.034937978722155094\n",
      "epoch: 24 trial 11429 training loss: 0.014319042209535837\n",
      "epoch: 24 trial 11430 training loss: 0.03224275168031454\n",
      "epoch: 24 trial 11431 training loss: 0.08623560145497322\n",
      "epoch: 24 trial 11432 training loss: 0.04048418067395687\n",
      "epoch: 24 trial 11433 training loss: 0.013443206436932087\n",
      "epoch: 24 trial 11434 training loss: 0.004680697456933558\n",
      "epoch: 24 trial 11435 training loss: 0.007832451490685344\n",
      "epoch: 24 trial 11436 training loss: 0.027283729054033756\n",
      "epoch: 24 trial 11437 training loss: 0.05336271785199642\n",
      "epoch: 24 trial 11438 training loss: 0.04424943123012781\n",
      "epoch: 24 trial 11439 training loss: 0.0038115298375487328\n",
      "epoch: 24 trial 11440 training loss: 0.033681177534163\n",
      "epoch: 24 trial 11441 training loss: 0.02753028552979231\n",
      "epoch: 24 trial 11442 training loss: 0.020199747756123543\n",
      "epoch: 24 trial 11443 training loss: 0.014987192116677761\n",
      "epoch: 24 trial 11444 training loss: 0.01673513581044972\n",
      "epoch: 24 trial 11445 training loss: 0.043048884719610214\n",
      "epoch: 24 trial 11446 training loss: 0.024760274216532707\n",
      "epoch: 24 trial 11447 training loss: 0.03104974515736103\n",
      "epoch: 24 trial 11448 training loss: 0.0718548633158207\n",
      "epoch: 24 trial 11449 training loss: 0.020203933585435152\n",
      "epoch: 24 trial 11450 training loss: 0.03510748967528343\n",
      "epoch: 24 trial 11451 training loss: 0.059045836329460144\n",
      "epoch: 24 trial 11452 training loss: 0.03358983341604471\n",
      "epoch: 24 trial 11453 training loss: 0.0630047470331192\n",
      "epoch: 24 trial 11454 training loss: 0.017377964686602354\n",
      "epoch: 24 trial 11455 training loss: 0.030395429581403732\n",
      "epoch: 24 trial 11456 training loss: 0.02575449552386999\n",
      "epoch: 24 trial 11457 training loss: 0.022613207809627056\n",
      "epoch: 24 trial 11458 training loss: 0.0231095845811069\n",
      "epoch: 24 trial 11459 training loss: 0.09653464518487453\n",
      "epoch: 24 trial 11460 training loss: 0.01536653796210885\n",
      "epoch: 24 trial 11461 training loss: 0.08351551182568073\n",
      "epoch: 24 trial 11462 training loss: 0.03142355103045702\n",
      "epoch: 24 trial 11463 training loss: 0.007259603589773178\n",
      "epoch: 24 trial 11464 training loss: 0.047074819914996624\n",
      "epoch: 24 trial 11465 training loss: 0.058067865669727325\n",
      "epoch: 24 trial 11466 training loss: 0.011446170508861542\n",
      "epoch: 24 trial 11467 training loss: 0.0360407792031765\n",
      "epoch: 24 trial 11468 training loss: 0.010679274098947644\n",
      "epoch: 24 trial 11469 training loss: 0.030921329744160175\n",
      "epoch: 24 trial 11470 training loss: 0.02365712821483612\n",
      "epoch: 24 trial 11471 training loss: 0.03834578860551119\n",
      "epoch: 24 trial 11472 training loss: 0.2768177092075348\n",
      "epoch: 24 trial 11473 training loss: 0.058556558564305305\n",
      "epoch: 24 trial 11474 training loss: 0.008963759522885084\n",
      "epoch: 24 trial 11475 training loss: 0.018025868106633425\n",
      "epoch: 24 trial 11476 training loss: 0.12261959165334702\n",
      "epoch: 24 trial 11477 training loss: 0.04686556477099657\n",
      "epoch: 24 trial 11478 training loss: 0.07581121101975441\n",
      "epoch: 24 trial 11479 training loss: 0.05226353649049997\n",
      "epoch: 24 trial 11480 training loss: 0.028785834554582834\n",
      "epoch: 24 trial 11481 training loss: 0.009211569791659713\n",
      "epoch: 24 trial 11482 training loss: 0.07619616203010082\n",
      "epoch: 24 trial 11483 training loss: 0.030958393588662148\n",
      "epoch: 24 trial 11484 training loss: 0.09157194569706917\n",
      "epoch: 24 trial 11485 training loss: 0.011815632693469524\n",
      "epoch: 24 trial 11486 training loss: 0.005288309999741614\n",
      "epoch: 24 trial 11487 training loss: 0.028295152820646763\n",
      "epoch: 24 trial 11488 training loss: 0.0050565446726977825\n",
      "epoch: 24 trial 11489 training loss: 0.015023569110780954\n",
      "epoch: 24 trial 11490 training loss: 0.09818598814308643\n",
      "epoch: 24 trial 11491 training loss: 0.06663387827575207\n",
      "epoch: 24 trial 11492 training loss: 0.02651742473244667\n",
      "epoch: 24 trial 11493 training loss: 0.10405194573104382\n",
      "epoch: 24 trial 11494 training loss: 0.0897502601146698\n",
      "epoch: 24 trial 11495 training loss: 0.02440733928233385\n",
      "epoch: 24 trial 11496 training loss: 0.0586708839982748\n",
      "epoch: 24 trial 11497 training loss: 0.03353950288146734\n",
      "epoch: 24 trial 11498 training loss: 0.015180903021246195\n",
      "epoch: 24 trial 11499 training loss: 0.005009429296478629\n",
      "epoch: 24 trial 11500 training loss: 0.13623593375086784\n",
      "epoch: 24 trial 11501 training loss: 0.06674894317984581\n",
      "epoch: 24 trial 11502 training loss: 0.04839744791388512\n",
      "epoch: 24 trial 11503 training loss: 0.05802961438894272\n",
      "epoch: 24 trial 11504 training loss: 0.026491778902709484\n",
      "epoch: 24 trial 11505 training loss: 0.03756334912031889\n",
      "epoch: 24 trial 11506 training loss: 0.014965508133172989\n",
      "epoch: 24 trial 11507 training loss: 0.05083676055073738\n",
      "epoch: 24 trial 11508 training loss: 0.026543349493294954\n",
      "epoch: 24 trial 11509 training loss: 0.033983062487095594\n",
      "epoch: 24 trial 11510 training loss: 0.09126313403248787\n",
      "epoch: 24 trial 11511 training loss: 0.02182701602578163\n",
      "epoch: 24 trial 11512 training loss: 0.010322264628484845\n",
      "epoch: 24 trial 11513 training loss: 0.03700454439967871\n",
      "epoch: 24 trial 11514 training loss: 0.0045626722276210785\n",
      "epoch: 24 trial 11515 training loss: 0.03527004085481167\n",
      "epoch: 24 trial 11516 training loss: 0.04153153207153082\n",
      "epoch: 24 trial 11517 training loss: 0.015183412469923496\n",
      "epoch: 24 trial 11518 training loss: 0.0019722673459909856\n",
      "epoch: 24 trial 11519 training loss: 0.003536268719471991\n",
      "epoch: 24 trial 11520 training loss: 0.015470877755433321\n",
      "epoch: 24 trial 11521 training loss: 0.03507251664996147\n",
      "epoch: 24 trial 11522 training loss: 0.004706505686044693\n",
      "epoch: 24 trial 11523 training loss: 0.0049239101354032755\n",
      "epoch: 24 trial 11524 training loss: 0.006724853999912739\n",
      "epoch: 24 trial 11525 training loss: 0.008508106227964163\n",
      "epoch: 24 trial 11526 training loss: 0.010842100949957967\n",
      "epoch: 24 trial 11527 training loss: 0.02399800019338727\n",
      "epoch: 24 trial 11528 training loss: 0.1073480173945427\n",
      "epoch: 24 trial 11529 training loss: 0.021772008389234543\n",
      "epoch: 24 trial 11530 training loss: 0.0170775530859828\n",
      "epoch: 24 trial 11531 training loss: 0.026355938520282507\n",
      "epoch: 24 trial 11532 training loss: 0.0365142528899014\n",
      "epoch: 24 trial 11533 training loss: 0.038282569497823715\n",
      "epoch: 24 trial 11534 training loss: 0.03366217389702797\n",
      "epoch: 24 trial 11535 training loss: 0.04318703152239323\n",
      "epoch: 24 trial 11536 training loss: 0.06711399368941784\n",
      "epoch: 24 trial 11537 training loss: 0.1082233414053917\n",
      "epoch: 24 trial 11538 training loss: 0.022140623070299625\n",
      "epoch: 24 trial 11539 training loss: 0.021868481300771236\n",
      "epoch: 24 trial 11540 training loss: 0.08910544589161873\n",
      "epoch: 24 trial 11541 training loss: 0.017444548662751913\n",
      "epoch: 24 trial 11542 training loss: 0.04995950683951378\n",
      "epoch: 24 trial 11543 training loss: 0.09190920181572437\n",
      "epoch: 24 trial 11544 training loss: 0.08547323569655418\n",
      "epoch: 24 trial 11545 training loss: 0.04202954098582268\n",
      "epoch: 24 trial 11546 training loss: 0.10962065868079662\n",
      "epoch: 24 trial 11547 training loss: 0.01764794159680605\n",
      "epoch: 24 trial 11548 training loss: 0.035738405771553516\n",
      "epoch: 24 trial 11549 training loss: 0.03047951776534319\n",
      "epoch: 24 trial 11550 training loss: 0.10232133232057095\n",
      "epoch: 24 trial 11551 training loss: 0.010151221416890621\n",
      "epoch: 24 trial 11552 training loss: 0.007415811298415065\n",
      "epoch: 24 trial 11553 training loss: 0.019741324707865715\n",
      "epoch: 24 trial 11554 training loss: 0.02770777139812708\n",
      "epoch: 24 trial 11555 training loss: 0.05287259537726641\n",
      "epoch: 24 trial 11556 training loss: 0.05810071900486946\n",
      "epoch: 24 trial 11557 training loss: 0.018543486949056387\n",
      "epoch: 24 trial 11558 training loss: 0.05256791040301323\n",
      "epoch: 24 trial 11559 training loss: 0.014282163698226213\n",
      "epoch: 24 trial 11560 training loss: 0.015976044116541743\n",
      "epoch: 24 trial 11561 training loss: 0.020701749715954065\n",
      "epoch: 24 trial 11562 training loss: 0.015153137035667896\n",
      "epoch: 24 trial 11563 training loss: 0.01904828753322363\n",
      "epoch: 24 trial 11564 training loss: 0.04455457627773285\n",
      "epoch: 24 trial 11565 training loss: 0.024868623819202185\n",
      "epoch: 24 trial 11566 training loss: 0.022393339313566685\n",
      "epoch: 24 trial 11567 training loss: 0.01966617815196514\n",
      "epoch: 24 trial 11568 training loss: 0.018257728312164545\n",
      "epoch: 24 trial 11569 training loss: 0.005074007203802466\n",
      "epoch: 24 trial 11570 training loss: 0.010582855204120278\n",
      "epoch: 24 trial 11571 training loss: 0.015622362028807402\n",
      "epoch: 24 trial 11572 training loss: 0.014879679307341576\n",
      "epoch: 24 trial 11573 training loss: 0.005662744748406112\n",
      "epoch: 24 trial 11574 training loss: 0.014418571256101131\n",
      "epoch: 24 trial 11575 training loss: 0.07923340238630772\n",
      "epoch: 24 trial 11576 training loss: 0.01051894179545343\n",
      "epoch: 24 trial 11577 training loss: 0.03528192825615406\n",
      "epoch: 24 trial 11578 training loss: 0.015011148061603308\n",
      "epoch: 24 trial 11579 training loss: 0.043797535821795464\n",
      "epoch: 24 trial 11580 training loss: 0.04285105597227812\n",
      "epoch: 24 trial 11581 training loss: 0.0392054682597518\n",
      "epoch: 24 trial 11582 training loss: 0.026502003893256187\n",
      "epoch: 24 trial 11583 training loss: 0.02092762291431427\n",
      "epoch: 24 trial 11584 training loss: 0.09904986247420311\n",
      "epoch: 24 trial 11585 training loss: 0.00604434940032661\n",
      "epoch: 24 trial 11586 training loss: 0.019565236289054155\n",
      "epoch: 24 trial 11587 training loss: 0.12311549857258797\n",
      "epoch: 24 trial 11588 training loss: 0.02378363534808159\n",
      "epoch: 24 trial 11589 training loss: 0.019692732486873865\n",
      "epoch: 24 trial 11590 training loss: 0.019413607195019722\n",
      "epoch: 24 trial 11591 training loss: 0.012179724406450987\n",
      "epoch: 24 trial 11592 training loss: 0.02792499214410782\n",
      "epoch: 24 trial 11593 training loss: 0.012821466196328402\n",
      "epoch: 24 trial 11594 training loss: 0.055829063057899475\n",
      "epoch: 24 trial 11595 training loss: 0.15759828314185143\n",
      "epoch: 24 trial 11596 training loss: 0.07151472195982933\n",
      "epoch: 24 trial 11597 training loss: 0.029521907679736614\n",
      "epoch: 24 trial 11598 training loss: 0.05772610008716583\n",
      "epoch: 24 trial 11599 training loss: 0.09218704700469971\n",
      "epoch: 24 trial 11600 training loss: 0.026051035150885582\n",
      "epoch: 24 trial 11601 training loss: 0.06813524477183819\n",
      "epoch: 24 trial 11602 training loss: 0.007027048151940107\n",
      "epoch: 24 trial 11603 training loss: 0.04050019662827253\n",
      "epoch: 24 trial 11604 training loss: 0.012574478751048446\n",
      "epoch: 24 trial 11605 training loss: 0.029024429619312286\n",
      "epoch: 24 trial 11606 training loss: 0.11439402401447296\n",
      "epoch: 24 trial 11607 training loss: 0.04313731845468283\n",
      "epoch: 24 trial 11608 training loss: 0.01876697177067399\n",
      "epoch: 24 trial 11609 training loss: 0.019329828675836325\n",
      "epoch: 24 trial 11610 training loss: 0.06833464838564396\n",
      "epoch: 24 trial 11611 training loss: 0.014164625201374292\n",
      "epoch: 24 trial 11612 training loss: 0.055415431037545204\n",
      "epoch: 24 trial 11613 training loss: 0.03813899401575327\n",
      "epoch: 24 trial 11614 training loss: 0.042311263270676136\n",
      "epoch: 24 trial 11615 training loss: 0.011915075592696667\n",
      "epoch: 24 trial 11616 training loss: 0.05540128983557224\n",
      "epoch: 25 trial 11617 training loss: 0.01458989828824997\n",
      "epoch: 25 trial 11618 training loss: 0.013140160124748945\n",
      "epoch: 25 trial 11619 training loss: 0.01082803844474256\n",
      "epoch: 25 trial 11620 training loss: 0.06154694966971874\n",
      "epoch: 25 trial 11621 training loss: 0.004691207781434059\n",
      "epoch: 25 trial 11622 training loss: 0.005442137364298105\n",
      "epoch: 25 trial 11623 training loss: 0.02619198616594076\n",
      "epoch: 25 trial 11624 training loss: 0.029663674533367157\n",
      "epoch: 25 trial 11625 training loss: 0.006005624774843454\n",
      "epoch: 25 trial 11626 training loss: 0.01514706015586853\n",
      "epoch: 25 trial 11627 training loss: 0.054899754002690315\n",
      "epoch: 25 trial 11628 training loss: 0.03187275491654873\n",
      "epoch: 25 trial 11629 training loss: 0.014155072160065174\n",
      "epoch: 25 trial 11630 training loss: 0.015300699509680271\n",
      "epoch: 25 trial 11631 training loss: 0.02126851212233305\n",
      "epoch: 25 trial 11632 training loss: 0.022215346340090036\n",
      "epoch: 25 trial 11633 training loss: 0.11726035922765732\n",
      "epoch: 25 trial 11634 training loss: 0.04509103111922741\n",
      "epoch: 25 trial 11635 training loss: 0.016169487964361906\n",
      "epoch: 25 trial 11636 training loss: 0.02001404669135809\n",
      "epoch: 25 trial 11637 training loss: 0.016855784226208925\n",
      "epoch: 25 trial 11638 training loss: 0.079362528398633\n",
      "epoch: 25 trial 11639 training loss: 0.01099210069514811\n",
      "epoch: 25 trial 11640 training loss: 0.004136495059356093\n",
      "epoch: 25 trial 11641 training loss: 0.13408014923334122\n",
      "epoch: 25 trial 11642 training loss: 0.030082466080784798\n",
      "epoch: 25 trial 11643 training loss: 0.012249583378434181\n",
      "epoch: 25 trial 11644 training loss: 0.10229373909533024\n",
      "epoch: 25 trial 11645 training loss: 0.0164624594617635\n",
      "epoch: 25 trial 11646 training loss: 0.016211939975619316\n",
      "epoch: 25 trial 11647 training loss: 0.02871782425791025\n",
      "epoch: 25 trial 11648 training loss: 0.012850264785811305\n",
      "epoch: 25 trial 11649 training loss: 0.030445819720625877\n",
      "epoch: 25 trial 11650 training loss: 0.053295593708753586\n",
      "epoch: 25 trial 11651 training loss: 0.015192221384495497\n",
      "epoch: 25 trial 11652 training loss: 0.0641754399985075\n",
      "epoch: 25 trial 11653 training loss: 0.13506684266030788\n",
      "epoch: 25 trial 11654 training loss: 0.2786700427532196\n",
      "epoch: 25 trial 11655 training loss: 0.042801087722182274\n",
      "epoch: 25 trial 11656 training loss: 0.09036996401846409\n",
      "epoch: 25 trial 11657 training loss: 0.09694679081439972\n",
      "epoch: 25 trial 11658 training loss: 0.010700443759560585\n",
      "epoch: 25 trial 11659 training loss: 0.08233143016695976\n",
      "epoch: 25 trial 11660 training loss: 0.03568963333964348\n",
      "epoch: 25 trial 11661 training loss: 0.024394681677222252\n",
      "epoch: 25 trial 11662 training loss: 0.05700859799981117\n",
      "epoch: 25 trial 11663 training loss: 0.010790548520162702\n",
      "epoch: 25 trial 11664 training loss: 0.027807588689029217\n",
      "epoch: 25 trial 11665 training loss: 0.014288120903074741\n",
      "epoch: 25 trial 11666 training loss: 0.10384150594472885\n",
      "epoch: 25 trial 11667 training loss: 0.030862705782055855\n",
      "epoch: 25 trial 11668 training loss: 0.11406039074063301\n",
      "epoch: 25 trial 11669 training loss: 0.03766039665788412\n",
      "epoch: 25 trial 11670 training loss: 0.0394146079197526\n",
      "epoch: 25 trial 11671 training loss: 0.033762551844120026\n",
      "epoch: 25 trial 11672 training loss: 0.0586076769977808\n",
      "epoch: 25 trial 11673 training loss: 0.02600136585533619\n",
      "epoch: 25 trial 11674 training loss: 0.01663259696215391\n",
      "epoch: 25 trial 11675 training loss: 0.03143812716007233\n",
      "epoch: 25 trial 11676 training loss: 0.08725514076650143\n",
      "epoch: 25 trial 11677 training loss: 0.028799396008253098\n",
      "epoch: 25 trial 11678 training loss: 0.12374590337276459\n",
      "epoch: 25 trial 11679 training loss: 0.016742571722716093\n",
      "epoch: 25 trial 11680 training loss: 0.0030474751838482916\n",
      "epoch: 25 trial 11681 training loss: 0.05315243639051914\n",
      "epoch: 25 trial 11682 training loss: 0.05505520850419998\n",
      "epoch: 25 trial 11683 training loss: 0.020355990389361978\n",
      "epoch: 25 trial 11684 training loss: 0.022764612920582294\n",
      "epoch: 25 trial 11685 training loss: 0.054230197332799435\n",
      "epoch: 25 trial 11686 training loss: 0.043013631366193295\n",
      "epoch: 25 trial 11687 training loss: 0.048591362312436104\n",
      "epoch: 25 trial 11688 training loss: 0.03622788842767477\n",
      "epoch: 25 trial 11689 training loss: 0.014496899209916592\n",
      "epoch: 25 trial 11690 training loss: 0.06605361960828304\n",
      "epoch: 25 trial 11691 training loss: 0.013378136325627565\n",
      "epoch: 25 trial 11692 training loss: 0.02422772953286767\n",
      "epoch: 25 trial 11693 training loss: 0.03161841072142124\n",
      "epoch: 25 trial 11694 training loss: 0.08272896334528923\n",
      "epoch: 25 trial 11695 training loss: 0.017535261809825897\n",
      "epoch: 25 trial 11696 training loss: 0.049684327095746994\n",
      "epoch: 25 trial 11697 training loss: 0.026127925608307123\n",
      "epoch: 25 trial 11698 training loss: 0.04349845089018345\n",
      "epoch: 25 trial 11699 training loss: 0.043205526657402515\n",
      "epoch: 25 trial 11700 training loss: 0.02285253582522273\n",
      "epoch: 25 trial 11701 training loss: 0.06790604628622532\n",
      "epoch: 25 trial 11702 training loss: 0.015295862220227718\n",
      "epoch: 25 trial 11703 training loss: 0.03116196021437645\n",
      "epoch: 25 trial 11704 training loss: 0.04893640801310539\n",
      "epoch: 25 trial 11705 training loss: 0.05548299103975296\n",
      "epoch: 25 trial 11706 training loss: 0.005585717270150781\n",
      "epoch: 25 trial 11707 training loss: 0.0400843471288681\n",
      "epoch: 25 trial 11708 training loss: 0.026270220056176186\n",
      "epoch: 25 trial 11709 training loss: 0.022402660455554724\n",
      "epoch: 25 trial 11710 training loss: 0.06340691819787025\n",
      "epoch: 25 trial 11711 training loss: 0.04538718797266483\n",
      "epoch: 25 trial 11712 training loss: 0.05592305399477482\n",
      "epoch: 25 trial 11713 training loss: 0.010902098380029202\n",
      "epoch: 25 trial 11714 training loss: 0.07503459602594376\n",
      "epoch: 25 trial 11715 training loss: 0.022866907063871622\n",
      "epoch: 25 trial 11716 training loss: 0.048898499459028244\n",
      "epoch: 25 trial 11717 training loss: 0.02745757158845663\n",
      "epoch: 25 trial 11718 training loss: 0.09715895354747772\n",
      "epoch: 25 trial 11719 training loss: 0.0487153148278594\n",
      "epoch: 25 trial 11720 training loss: 0.029607081785798073\n",
      "epoch: 25 trial 11721 training loss: 0.21672465279698372\n",
      "epoch: 25 trial 11722 training loss: 0.07462934218347073\n",
      "epoch: 25 trial 11723 training loss: 0.027377921156585217\n",
      "epoch: 25 trial 11724 training loss: 0.15496011450886726\n",
      "epoch: 25 trial 11725 training loss: 0.07916435599327087\n",
      "epoch: 25 trial 11726 training loss: 0.025368814822286367\n",
      "epoch: 25 trial 11727 training loss: 0.019907898269593716\n",
      "epoch: 25 trial 11728 training loss: 0.01919952780008316\n",
      "epoch: 25 trial 11729 training loss: 0.015054292511194944\n",
      "epoch: 25 trial 11730 training loss: 0.010256106033921242\n",
      "epoch: 25 trial 11731 training loss: 0.05542059428989887\n",
      "epoch: 25 trial 11732 training loss: 0.01968953525647521\n",
      "epoch: 25 trial 11733 training loss: 0.023471014574170113\n",
      "epoch: 25 trial 11734 training loss: 0.037233482114970684\n",
      "epoch: 25 trial 11735 training loss: 0.03143093269318342\n",
      "epoch: 25 trial 11736 training loss: 0.027472813613712788\n",
      "epoch: 25 trial 11737 training loss: 0.017561763990670443\n",
      "epoch: 25 trial 11738 training loss: 0.03030651342123747\n",
      "epoch: 25 trial 11739 training loss: 0.07087607588618994\n",
      "epoch: 25 trial 11740 training loss: 0.03339746035635471\n",
      "epoch: 25 trial 11741 training loss: 0.011914443457499146\n",
      "epoch: 25 trial 11742 training loss: 0.020161937456578016\n",
      "epoch: 25 trial 11743 training loss: 0.025144770741462708\n",
      "epoch: 25 trial 11744 training loss: 0.02660068590193987\n",
      "epoch: 25 trial 11745 training loss: 0.13182157650589943\n",
      "epoch: 25 trial 11746 training loss: 0.022617421112954617\n",
      "epoch: 25 trial 11747 training loss: 0.03278549946844578\n",
      "epoch: 25 trial 11748 training loss: 0.026454225182533264\n",
      "epoch: 25 trial 11749 training loss: 0.030784903094172478\n",
      "epoch: 25 trial 11750 training loss: 0.04485201183706522\n",
      "epoch: 25 trial 11751 training loss: 0.010347613599151373\n",
      "epoch: 25 trial 11752 training loss: 0.04685978405177593\n",
      "epoch: 25 trial 11753 training loss: 0.014036955311894417\n",
      "epoch: 25 trial 11754 training loss: 0.11735371500253677\n",
      "epoch: 25 trial 11755 training loss: 0.016783106606453657\n",
      "epoch: 25 trial 11756 training loss: 0.06541674863547087\n",
      "epoch: 25 trial 11757 training loss: 0.034461675211787224\n",
      "epoch: 25 trial 11758 training loss: 0.033543216064572334\n",
      "epoch: 25 trial 11759 training loss: 0.025562692899256945\n",
      "epoch: 25 trial 11760 training loss: 0.13685324043035507\n",
      "epoch: 25 trial 11761 training loss: 0.007262405939400196\n",
      "epoch: 25 trial 11762 training loss: 0.01848781667649746\n",
      "epoch: 25 trial 11763 training loss: 0.09587647765874863\n",
      "epoch: 25 trial 11764 training loss: 0.02081328583881259\n",
      "epoch: 25 trial 11765 training loss: 0.030133694410324097\n",
      "epoch: 25 trial 11766 training loss: 0.050903831608593464\n",
      "epoch: 25 trial 11767 training loss: 0.027125975117087364\n",
      "epoch: 25 trial 11768 training loss: 0.019299456849694252\n",
      "epoch: 25 trial 11769 training loss: 0.015243425965309143\n",
      "epoch: 25 trial 11770 training loss: 0.0338653028011322\n",
      "epoch: 25 trial 11771 training loss: 0.014450386865064502\n",
      "epoch: 25 trial 11772 training loss: 0.009367341175675392\n",
      "epoch: 25 trial 11773 training loss: 0.028378192335367203\n",
      "epoch: 25 trial 11774 training loss: 0.0043797873659059405\n",
      "epoch: 25 trial 11775 training loss: 0.01217288663610816\n",
      "epoch: 25 trial 11776 training loss: 0.01794852688908577\n",
      "epoch: 25 trial 11777 training loss: 0.011135774664580822\n",
      "epoch: 25 trial 11778 training loss: 0.05465609394013882\n",
      "epoch: 25 trial 11779 training loss: 0.0049696286441758275\n",
      "epoch: 25 trial 11780 training loss: 0.058509429916739464\n",
      "epoch: 25 trial 11781 training loss: 0.011834332952275872\n",
      "epoch: 25 trial 11782 training loss: 0.02161808032542467\n",
      "epoch: 25 trial 11783 training loss: 0.02398226084187627\n",
      "epoch: 25 trial 11784 training loss: 0.013672816567122936\n",
      "epoch: 25 trial 11785 training loss: 0.033987682312726974\n",
      "epoch: 25 trial 11786 training loss: 0.11523422598838806\n",
      "epoch: 25 trial 11787 training loss: 0.04940332379192114\n",
      "epoch: 25 trial 11788 training loss: 0.010148274013772607\n",
      "epoch: 25 trial 11789 training loss: 0.06747580505907536\n",
      "epoch: 25 trial 11790 training loss: 0.030737120658159256\n",
      "epoch: 25 trial 11791 training loss: 0.022203662898391485\n",
      "epoch: 25 trial 11792 training loss: 0.01664231065660715\n",
      "epoch: 25 trial 11793 training loss: 0.02591724181547761\n",
      "epoch: 25 trial 11794 training loss: 0.028579686768352985\n",
      "epoch: 25 trial 11795 training loss: 0.035623363219201565\n",
      "epoch: 25 trial 11796 training loss: 0.03122048545628786\n",
      "epoch: 25 trial 11797 training loss: 0.0861947163939476\n",
      "epoch: 25 trial 11798 training loss: 0.007989362580701709\n",
      "epoch: 25 trial 11799 training loss: 0.0663845781236887\n",
      "epoch: 25 trial 11800 training loss: 0.01993151241913438\n",
      "epoch: 25 trial 11801 training loss: 0.017479734495282173\n",
      "epoch: 25 trial 11802 training loss: 0.04788598883897066\n",
      "epoch: 25 trial 11803 training loss: 0.05010863393545151\n",
      "epoch: 25 trial 11804 training loss: 0.01729380525648594\n",
      "epoch: 25 trial 11805 training loss: 0.036897300742566586\n",
      "epoch: 25 trial 11806 training loss: 0.0835721604526043\n",
      "epoch: 25 trial 11807 training loss: 0.07728688605129719\n",
      "epoch: 25 trial 11808 training loss: 0.01181900897063315\n",
      "epoch: 25 trial 11809 training loss: 0.023451107554137707\n",
      "epoch: 25 trial 11810 training loss: 0.013695211382582784\n",
      "epoch: 25 trial 11811 training loss: 0.01814583269879222\n",
      "epoch: 25 trial 11812 training loss: 0.04476793482899666\n",
      "epoch: 25 trial 11813 training loss: 0.03148422297090292\n",
      "epoch: 25 trial 11814 training loss: 0.07132563553750515\n",
      "epoch: 25 trial 11815 training loss: 0.11696330457925797\n",
      "epoch: 25 trial 11816 training loss: 0.01819893019273877\n",
      "epoch: 25 trial 11817 training loss: 0.006210459512658417\n",
      "epoch: 25 trial 11818 training loss: 0.007795078679919243\n",
      "epoch: 25 trial 11819 training loss: 0.011819292092695832\n",
      "epoch: 25 trial 11820 training loss: 0.015718042384833097\n",
      "epoch: 25 trial 11821 training loss: 0.02505466155707836\n",
      "epoch: 25 trial 11822 training loss: 0.09661650098860264\n",
      "epoch: 25 trial 11823 training loss: 0.0336763933300972\n",
      "epoch: 25 trial 11824 training loss: 0.013556142337620258\n",
      "epoch: 25 trial 11825 training loss: 0.09621631912887096\n",
      "epoch: 25 trial 11826 training loss: 0.045873669907450676\n",
      "epoch: 25 trial 11827 training loss: 0.05976545065641403\n",
      "epoch: 25 trial 11828 training loss: 0.06928996928036213\n",
      "epoch: 25 trial 11829 training loss: 0.10236300528049469\n",
      "epoch: 25 trial 11830 training loss: 0.013401765376329422\n",
      "epoch: 25 trial 11831 training loss: 0.01037272671237588\n",
      "epoch: 25 trial 11832 training loss: 0.023642350919544697\n",
      "epoch: 25 trial 11833 training loss: 0.02237493311986327\n",
      "epoch: 25 trial 11834 training loss: 0.02049961406737566\n",
      "epoch: 25 trial 11835 training loss: 0.04400630667805672\n",
      "epoch: 25 trial 11836 training loss: 0.07195044122636318\n",
      "epoch: 25 trial 11837 training loss: 0.032186479307711124\n",
      "epoch: 25 trial 11838 training loss: 0.04601552616804838\n",
      "epoch: 25 trial 11839 training loss: 0.04197833873331547\n",
      "epoch: 25 trial 11840 training loss: 0.0925325807183981\n",
      "epoch: 25 trial 11841 training loss: 0.012676601763814688\n",
      "epoch: 25 trial 11842 training loss: 0.011843058979138732\n",
      "epoch: 25 trial 11843 training loss: 0.021981780882924795\n",
      "epoch: 25 trial 11844 training loss: 0.025121514219790697\n",
      "epoch: 25 trial 11845 training loss: 0.047591183334589005\n",
      "epoch: 25 trial 11846 training loss: 0.015348827932029963\n",
      "epoch: 25 trial 11847 training loss: 0.1764608658850193\n",
      "epoch: 25 trial 11848 training loss: 0.016921864822506905\n",
      "epoch: 25 trial 11849 training loss: 0.09589513763785362\n",
      "epoch: 25 trial 11850 training loss: 0.08705678954720497\n",
      "epoch: 25 trial 11851 training loss: 0.01288337935693562\n",
      "epoch: 25 trial 11852 training loss: 0.062072157859802246\n",
      "epoch: 25 trial 11853 training loss: 0.01393620204180479\n",
      "epoch: 25 trial 11854 training loss: 0.0467223571613431\n",
      "epoch: 25 trial 11855 training loss: 0.04825624264776707\n",
      "epoch: 25 trial 11856 training loss: 0.06387263163924217\n",
      "epoch: 25 trial 11857 training loss: 0.01537373848259449\n",
      "epoch: 25 trial 11858 training loss: 0.037515812553465366\n",
      "epoch: 25 trial 11859 training loss: 0.036766172386705875\n",
      "epoch: 25 trial 11860 training loss: 0.03632015269249678\n",
      "epoch: 25 trial 11861 training loss: 0.06288809888064861\n",
      "epoch: 25 trial 11862 training loss: 0.017449061619117856\n",
      "epoch: 25 trial 11863 training loss: 0.024187919218093157\n",
      "epoch: 25 trial 11864 training loss: 0.01821850799024105\n",
      "epoch: 25 trial 11865 training loss: 0.021989115979522467\n",
      "epoch: 25 trial 11866 training loss: 0.03814710211008787\n",
      "epoch: 25 trial 11867 training loss: 0.01129869557917118\n",
      "epoch: 25 trial 11868 training loss: 0.008247406221926212\n",
      "epoch: 25 trial 11869 training loss: 0.03274490963667631\n",
      "epoch: 25 trial 11870 training loss: 0.08464149385690689\n",
      "epoch: 25 trial 11871 training loss: 0.02098645595833659\n",
      "epoch: 25 trial 11872 training loss: 0.09158652648329735\n",
      "epoch: 25 trial 11873 training loss: 0.057083819061517715\n",
      "epoch: 25 trial 11874 training loss: 0.0276067852973938\n",
      "epoch: 25 trial 11875 training loss: 0.13299722969532013\n",
      "epoch: 25 trial 11876 training loss: 0.013369190040975809\n",
      "epoch: 25 trial 11877 training loss: 0.010472622234374285\n",
      "epoch: 25 trial 11878 training loss: 0.009026608662679791\n",
      "epoch: 25 trial 11879 training loss: 0.004059460829012096\n",
      "epoch: 25 trial 11880 training loss: 0.03254770580679178\n",
      "epoch: 25 trial 11881 training loss: 0.035740495659410954\n",
      "epoch: 25 trial 11882 training loss: 0.05672823451459408\n",
      "epoch: 25 trial 11883 training loss: 0.027363489847630262\n",
      "epoch: 25 trial 11884 training loss: 0.04889086354523897\n",
      "epoch: 25 trial 11885 training loss: 0.018314118031412363\n",
      "epoch: 25 trial 11886 training loss: 0.08551753126084805\n",
      "epoch: 25 trial 11887 training loss: 0.0172292604111135\n",
      "epoch: 25 trial 11888 training loss: 0.007808944443240762\n",
      "epoch: 25 trial 11889 training loss: 0.022324213292449713\n",
      "epoch: 25 trial 11890 training loss: 0.005372763145714998\n",
      "epoch: 25 trial 11891 training loss: 0.07174801453948021\n",
      "epoch: 25 trial 11892 training loss: 0.0030523426830768585\n",
      "epoch: 25 trial 11893 training loss: 0.023479295428842306\n",
      "epoch: 25 trial 11894 training loss: 0.01832849532365799\n",
      "epoch: 25 trial 11895 training loss: 0.06382961478084326\n",
      "epoch: 25 trial 11896 training loss: 0.044439214281737804\n",
      "epoch: 25 trial 11897 training loss: 0.02169915661215782\n",
      "epoch: 25 trial 11898 training loss: 0.03371044714003801\n",
      "epoch: 25 trial 11899 training loss: 0.0618933979421854\n",
      "epoch: 25 trial 11900 training loss: 0.02733604423701763\n",
      "epoch: 25 trial 11901 training loss: 0.06875346787273884\n",
      "epoch: 25 trial 11902 training loss: 0.020752236247062683\n",
      "epoch: 25 trial 11903 training loss: 0.0623925756663084\n",
      "epoch: 25 trial 11904 training loss: 0.021099109202623367\n",
      "epoch: 25 trial 11905 training loss: 0.007828939473256469\n",
      "epoch: 25 trial 11906 training loss: 0.04738174844533205\n",
      "epoch: 25 trial 11907 training loss: 0.029571102000772953\n",
      "epoch: 25 trial 11908 training loss: 0.01692661689594388\n",
      "epoch: 25 trial 11909 training loss: 0.01970719126984477\n",
      "epoch: 25 trial 11910 training loss: 0.010134710930287838\n",
      "epoch: 25 trial 11911 training loss: 0.02254727343097329\n",
      "epoch: 25 trial 11912 training loss: 0.03288015117868781\n",
      "epoch: 25 trial 11913 training loss: 0.014762229286134243\n",
      "epoch: 25 trial 11914 training loss: 0.030416407622396946\n",
      "epoch: 25 trial 11915 training loss: 0.08625582233071327\n",
      "epoch: 25 trial 11916 training loss: 0.037719075568020344\n",
      "epoch: 25 trial 11917 training loss: 0.011878031771630049\n",
      "epoch: 25 trial 11918 training loss: 0.004367594490759075\n",
      "epoch: 25 trial 11919 training loss: 0.00771348737180233\n",
      "epoch: 25 trial 11920 training loss: 0.02853074949234724\n",
      "epoch: 25 trial 11921 training loss: 0.05328024458140135\n",
      "epoch: 25 trial 11922 training loss: 0.04323288891464472\n",
      "epoch: 25 trial 11923 training loss: 0.003656131331808865\n",
      "epoch: 25 trial 11924 training loss: 0.034182109870016575\n",
      "epoch: 25 trial 11925 training loss: 0.027464615181088448\n",
      "epoch: 25 trial 11926 training loss: 0.02309794072061777\n",
      "epoch: 25 trial 11927 training loss: 0.016074747312813997\n",
      "epoch: 25 trial 11928 training loss: 0.01773278694599867\n",
      "epoch: 25 trial 11929 training loss: 0.04172277171164751\n",
      "epoch: 25 trial 11930 training loss: 0.0251624071970582\n",
      "epoch: 25 trial 11931 training loss: 0.03211010619997978\n",
      "epoch: 25 trial 11932 training loss: 0.07402823120355606\n",
      "epoch: 25 trial 11933 training loss: 0.020752272102981806\n",
      "epoch: 25 trial 11934 training loss: 0.03185287211090326\n",
      "epoch: 25 trial 11935 training loss: 0.06139391474425793\n",
      "epoch: 25 trial 11936 training loss: 0.031958931125700474\n",
      "epoch: 25 trial 11937 training loss: 0.0594619270414114\n",
      "epoch: 25 trial 11938 training loss: 0.018607513047754765\n",
      "epoch: 25 trial 11939 training loss: 0.034230444580316544\n",
      "epoch: 25 trial 11940 training loss: 0.026203789748251438\n",
      "epoch: 25 trial 11941 training loss: 0.022131752222776413\n",
      "epoch: 25 trial 11942 training loss: 0.02404109202325344\n",
      "epoch: 25 trial 11943 training loss: 0.08510836027562618\n",
      "epoch: 25 trial 11944 training loss: 0.01467518787831068\n",
      "epoch: 25 trial 11945 training loss: 0.07958624325692654\n",
      "epoch: 25 trial 11946 training loss: 0.03172417264431715\n",
      "epoch: 25 trial 11947 training loss: 0.007459170883521438\n",
      "epoch: 25 trial 11948 training loss: 0.04781285859644413\n",
      "epoch: 25 trial 11949 training loss: 0.058815501630306244\n",
      "epoch: 25 trial 11950 training loss: 0.011718287132680416\n",
      "epoch: 25 trial 11951 training loss: 0.03421421907842159\n",
      "epoch: 25 trial 11952 training loss: 0.011028479086235166\n",
      "epoch: 25 trial 11953 training loss: 0.030636967159807682\n",
      "epoch: 25 trial 11954 training loss: 0.019330669660121202\n",
      "epoch: 25 trial 11955 training loss: 0.037569171749055386\n",
      "epoch: 25 trial 11956 training loss: 0.27236589789390564\n",
      "epoch: 25 trial 11957 training loss: 0.06047783046960831\n",
      "epoch: 25 trial 11958 training loss: 0.00862409034743905\n",
      "epoch: 25 trial 11959 training loss: 0.01625629933550954\n",
      "epoch: 25 trial 11960 training loss: 0.12032420933246613\n",
      "epoch: 25 trial 11961 training loss: 0.04837806522846222\n",
      "epoch: 25 trial 11962 training loss: 0.07887138798832893\n",
      "epoch: 25 trial 11963 training loss: 0.05666727386415005\n",
      "epoch: 25 trial 11964 training loss: 0.030952725559473038\n",
      "epoch: 25 trial 11965 training loss: 0.008985727094113827\n",
      "epoch: 25 trial 11966 training loss: 0.07454400323331356\n",
      "epoch: 25 trial 11967 training loss: 0.03259158879518509\n",
      "epoch: 25 trial 11968 training loss: 0.0915096327662468\n",
      "epoch: 25 trial 11969 training loss: 0.010867333272472024\n",
      "epoch: 25 trial 11970 training loss: 0.0049970586551353335\n",
      "epoch: 25 trial 11971 training loss: 0.026720834895968437\n",
      "epoch: 25 trial 11972 training loss: 0.005314895883202553\n",
      "epoch: 25 trial 11973 training loss: 0.013321322156116366\n",
      "epoch: 25 trial 11974 training loss: 0.10032595321536064\n",
      "epoch: 25 trial 11975 training loss: 0.06912324763834476\n",
      "epoch: 25 trial 11976 training loss: 0.025655625388026237\n",
      "epoch: 25 trial 11977 training loss: 0.11652015522122383\n",
      "epoch: 25 trial 11978 training loss: 0.09196625091135502\n",
      "epoch: 25 trial 11979 training loss: 0.02333515789359808\n",
      "epoch: 25 trial 11980 training loss: 0.06046571768820286\n",
      "epoch: 25 trial 11981 training loss: 0.03636262472718954\n",
      "epoch: 25 trial 11982 training loss: 0.013084592297673225\n",
      "epoch: 25 trial 11983 training loss: 0.005001547746360302\n",
      "epoch: 25 trial 11984 training loss: 0.12099184282124043\n",
      "epoch: 25 trial 11985 training loss: 0.06730389408767223\n",
      "epoch: 25 trial 11986 training loss: 0.04738964419811964\n",
      "epoch: 25 trial 11987 training loss: 0.06021680682897568\n",
      "epoch: 25 trial 11988 training loss: 0.023124448023736477\n",
      "epoch: 25 trial 11989 training loss: 0.03983116988092661\n",
      "epoch: 25 trial 11990 training loss: 0.014862855430692434\n",
      "epoch: 25 trial 11991 training loss: 0.04855081532150507\n",
      "epoch: 25 trial 11992 training loss: 0.02754541113972664\n",
      "epoch: 25 trial 11993 training loss: 0.036228365264832973\n",
      "epoch: 25 trial 11994 training loss: 0.09041707031428814\n",
      "epoch: 25 trial 11995 training loss: 0.02469544531777501\n",
      "epoch: 25 trial 11996 training loss: 0.010656543541699648\n",
      "epoch: 25 trial 11997 training loss: 0.035693343728780746\n",
      "epoch: 25 trial 11998 training loss: 0.0044584558345377445\n",
      "epoch: 25 trial 11999 training loss: 0.03526695631444454\n",
      "epoch: 25 trial 12000 training loss: 0.04074306972324848\n",
      "epoch: 25 trial 12001 training loss: 0.015562154352664948\n",
      "epoch: 25 trial 12002 training loss: 0.002081071666907519\n",
      "epoch: 25 trial 12003 training loss: 0.0035240977304056287\n",
      "epoch: 25 trial 12004 training loss: 0.015550688374787569\n",
      "epoch: 25 trial 12005 training loss: 0.03730750177055597\n",
      "epoch: 25 trial 12006 training loss: 0.004373371368274093\n",
      "epoch: 25 trial 12007 training loss: 0.0048793378518894315\n",
      "epoch: 25 trial 12008 training loss: 0.007059136172756553\n",
      "epoch: 25 trial 12009 training loss: 0.007738870102912188\n",
      "epoch: 25 trial 12010 training loss: 0.011737190652638674\n",
      "epoch: 25 trial 12011 training loss: 0.025975716300308704\n",
      "epoch: 25 trial 12012 training loss: 0.11059489287436008\n",
      "epoch: 25 trial 12013 training loss: 0.02202613092958927\n",
      "epoch: 25 trial 12014 training loss: 0.017266621813178062\n",
      "epoch: 25 trial 12015 training loss: 0.029834983870387077\n",
      "epoch: 25 trial 12016 training loss: 0.03872583992779255\n",
      "epoch: 25 trial 12017 training loss: 0.03738128114491701\n",
      "epoch: 25 trial 12018 training loss: 0.032782034017145634\n",
      "epoch: 25 trial 12019 training loss: 0.047851383686065674\n",
      "epoch: 25 trial 12020 training loss: 0.0708354152739048\n",
      "epoch: 25 trial 12021 training loss: 0.10673080943524837\n",
      "epoch: 25 trial 12022 training loss: 0.02089521335437894\n",
      "epoch: 25 trial 12023 training loss: 0.02191441785544157\n",
      "epoch: 25 trial 12024 training loss: 0.08563204109668732\n",
      "epoch: 25 trial 12025 training loss: 0.0199467153288424\n",
      "epoch: 25 trial 12026 training loss: 0.045993966050446033\n",
      "epoch: 25 trial 12027 training loss: 0.08942268230021\n",
      "epoch: 25 trial 12028 training loss: 0.08327554725110531\n",
      "epoch: 25 trial 12029 training loss: 0.04051797837018967\n",
      "epoch: 25 trial 12030 training loss: 0.11206107586622238\n",
      "epoch: 25 trial 12031 training loss: 0.019207105040550232\n",
      "epoch: 25 trial 12032 training loss: 0.0367969274520874\n",
      "epoch: 25 trial 12033 training loss: 0.030456310138106346\n",
      "epoch: 25 trial 12034 training loss: 0.10403655841946602\n",
      "epoch: 25 trial 12035 training loss: 0.009783517103642225\n",
      "epoch: 25 trial 12036 training loss: 0.008059912011958659\n",
      "epoch: 25 trial 12037 training loss: 0.020248483400791883\n",
      "epoch: 25 trial 12038 training loss: 0.02763314824551344\n",
      "epoch: 25 trial 12039 training loss: 0.05871487781405449\n",
      "epoch: 25 trial 12040 training loss: 0.056569816544651985\n",
      "epoch: 25 trial 12041 training loss: 0.017203134251758456\n",
      "epoch: 25 trial 12042 training loss: 0.05664819851517677\n",
      "epoch: 25 trial 12043 training loss: 0.014579400420188904\n",
      "epoch: 25 trial 12044 training loss: 0.01719519542530179\n",
      "epoch: 25 trial 12045 training loss: 0.021573311183601618\n",
      "epoch: 25 trial 12046 training loss: 0.01538845943287015\n",
      "epoch: 25 trial 12047 training loss: 0.019243290182203054\n",
      "epoch: 25 trial 12048 training loss: 0.04450427554547787\n",
      "epoch: 25 trial 12049 training loss: 0.02597565809264779\n",
      "epoch: 25 trial 12050 training loss: 0.023009732831269503\n",
      "epoch: 25 trial 12051 training loss: 0.018303430173546076\n",
      "epoch: 25 trial 12052 training loss: 0.016175215132534504\n",
      "epoch: 25 trial 12053 training loss: 0.005017620627768338\n",
      "epoch: 25 trial 12054 training loss: 0.010088386945426464\n",
      "epoch: 25 trial 12055 training loss: 0.016051618847995996\n",
      "epoch: 25 trial 12056 training loss: 0.01434589084237814\n",
      "epoch: 25 trial 12057 training loss: 0.005614825873635709\n",
      "epoch: 25 trial 12058 training loss: 0.013460319954901934\n",
      "epoch: 25 trial 12059 training loss: 0.08716993220150471\n",
      "epoch: 25 trial 12060 training loss: 0.00992840901017189\n",
      "epoch: 25 trial 12061 training loss: 0.039916264824569225\n",
      "epoch: 25 trial 12062 training loss: 0.013993347994983196\n",
      "epoch: 25 trial 12063 training loss: 0.041914667934179306\n",
      "epoch: 25 trial 12064 training loss: 0.040611800737679005\n",
      "epoch: 25 trial 12065 training loss: 0.04263370856642723\n",
      "epoch: 25 trial 12066 training loss: 0.02549361065030098\n",
      "epoch: 25 trial 12067 training loss: 0.022124866023659706\n",
      "epoch: 25 trial 12068 training loss: 0.10545029863715172\n",
      "epoch: 25 trial 12069 training loss: 0.00578428665176034\n",
      "epoch: 25 trial 12070 training loss: 0.017941752914339304\n",
      "epoch: 25 trial 12071 training loss: 0.12892307341098785\n",
      "epoch: 25 trial 12072 training loss: 0.026232853531837463\n",
      "epoch: 25 trial 12073 training loss: 0.020760328508913517\n",
      "epoch: 25 trial 12074 training loss: 0.017915749922394753\n",
      "epoch: 25 trial 12075 training loss: 0.012638067826628685\n",
      "epoch: 25 trial 12076 training loss: 0.028598828241229057\n",
      "epoch: 25 trial 12077 training loss: 0.0132280548568815\n",
      "epoch: 25 trial 12078 training loss: 0.055697670206427574\n",
      "epoch: 25 trial 12079 training loss: 0.14935444667935371\n",
      "epoch: 25 trial 12080 training loss: 0.06682364270091057\n",
      "epoch: 25 trial 12081 training loss: 0.02839812356978655\n",
      "epoch: 25 trial 12082 training loss: 0.0615582000464201\n",
      "epoch: 25 trial 12083 training loss: 0.09893637895584106\n",
      "epoch: 25 trial 12084 training loss: 0.022141003981232643\n",
      "epoch: 25 trial 12085 training loss: 0.07053731940686703\n",
      "epoch: 25 trial 12086 training loss: 0.008550351951271296\n",
      "epoch: 25 trial 12087 training loss: 0.041029300540685654\n",
      "epoch: 25 trial 12088 training loss: 0.012392085511237383\n",
      "epoch: 25 trial 12089 training loss: 0.028385700657963753\n",
      "epoch: 25 trial 12090 training loss: 0.11250831931829453\n",
      "epoch: 25 trial 12091 training loss: 0.043771364726126194\n",
      "epoch: 25 trial 12092 training loss: 0.018861989956349134\n",
      "epoch: 25 trial 12093 training loss: 0.020574762020260096\n",
      "epoch: 25 trial 12094 training loss: 0.07124882657080889\n",
      "epoch: 25 trial 12095 training loss: 0.018550234381109476\n",
      "epoch: 25 trial 12096 training loss: 0.05439379625022411\n",
      "epoch: 25 trial 12097 training loss: 0.0395149914547801\n",
      "epoch: 25 trial 12098 training loss: 0.04570774920284748\n",
      "epoch: 25 trial 12099 training loss: 0.013401450589299202\n",
      "epoch: 25 trial 12100 training loss: 0.05293373018503189\n",
      "0.05293373018503189\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2 # Number of LSTM recurrent layers\n",
    "input_size = 17 # Number of features for input\n",
    "hidden_size = 16 # Number of features in hidden state\n",
    "seq_length = 17 # Length of sequences in a batch\n",
    "num_classes = 1 # Predicted output value\n",
    "\n",
    "batch_size = 32 # Batch size: number of samples that will be propagated through the network at once\n",
    "\n",
    "# you may change the learning rate and numbers of epochs run\n",
    "learning_rate = 0.0001 # Learning rate: determines the step size at each iteration while moving toward a minimum of a loss function\n",
    "lstm_epochs = 8 # Number of epochs: number of times the entire dataset will be passed through the network\n",
    "\n",
    "criterion = nn.MSELoss() # Loss function: Mean Squared Error \n",
    "\n",
    "lstm_model = LSTM(num_layers, input_size, hidden_size, seq_length, num_classes) # Initialize LSTM model based on the parameters\n",
    "\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate) # Initialize optimizer: Adam optimizer\n",
    "\n",
    "lstm_train_loss, lstm_val_loss = train(lstm_model, tvt_dataloaders, criterion, lstm_epochs, optimizer) # Run training\n",
    "\n",
    "torch.save(lstm_model.state_dict(), 'last_large_lstm_model_state_dict.pth') # Change the name of the file to save the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dO5T3PAyC60"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [epoch+1 for epoch in range(lstm_epochs*len(tvt_dataloaders))]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax1.title.set_text('Train Loss')\n",
    "ax1.plot(x, lstm_train_loss, color='green',label='Train Loss')\n",
    "\n",
    "ax2.title.set_text('Val Loss')\n",
    "ax2.plot(x, lstm_val_loss, color='green',label='Loss')\n",
    "\n",
    "ax3 = ax2.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax3.set_ylabel('Accuracy', color=color)\n",
    "# ax3.plot(x, lstm_val_acc, color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "color = 'tab:green'\n",
    "ax2.set_ylabel('Loss', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgOaQ9oKyVOj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (LSTM): 0.01\n"
     ]
    }
   ],
   "source": [
    "lstm_loss = val(lstm_model, test_dataloader, criterion)\n",
    "\n",
    "\n",
    "print(f\"Test Loss (LSTM): {lstm_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eK35oQvleo3S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "0\n",
      "Input shape: torch.Size([64, 1440, 17])\n",
      "Output: [(tensor([0.6187]), tensor(0.8027)), (tensor([0.5739]), tensor(0.5358)), (tensor([0.6503]), tensor(0.6188)), (tensor([0.5756]), tensor(0.5244)), (tensor([0.4817]), tensor(0.5093)), (tensor([0.5955]), tensor(0.4823)), (tensor([0.8830]), tensor(0.9118)), (tensor([0.7039]), tensor(0.9382)), (tensor([0.6264]), tensor(0.6701)), (tensor([0.5853]), tensor(0.5427)), (tensor([0.6595]), tensor(0.9020)), (tensor([0.5128]), tensor(0.5654)), (tensor([0.6144]), tensor(0.7665)), (tensor([0.6064]), tensor(0.5904)), (tensor([0.6103]), tensor(0.6083)), (tensor([0.8536]), tensor(0.8731)), (tensor([0.6729]), tensor(0.6176)), (tensor([0.5931]), tensor(0.5416)), (tensor([0.8898]), tensor(0.9046)), (tensor([0.5135]), tensor(0.5606)), (tensor([0.8275]), tensor(0.8699)), (tensor([0.7237]), tensor(0.9532)), (tensor([0.6136]), tensor(0.7948)), (tensor([0.4904]), tensor(0.4972)), (tensor([0.8810]), tensor(0.9265)), (tensor([0.6218]), tensor(0.6500)), (tensor([0.7045]), tensor(0.9799)), (tensor([0.6309]), tensor(0.6184)), (tensor([0.8822]), tensor(0.9338)), (tensor([0.8780]), tensor(0.9291)), (tensor([0.8106]), tensor(0.8700)), (tensor([0.6926]), tensor(0.9561)), (tensor([0.5664]), tensor(0.5123)), (tensor([0.6551]), tensor(0.8315)), (tensor([0.8513]), tensor(0.8899)), (tensor([0.4970]), tensor(0.5515)), (tensor([0.4949]), tensor(0.5417)), (tensor([0.6673]), tensor(0.9281)), (tensor([0.8898]), tensor(0.9026)), (tensor([0.7129]), tensor(0.8961)), (tensor([0.6012]), tensor(0.5506)), (tensor([0.6569]), tensor(0.6061)), (tensor([0.7101]), tensor(0.9116)), (tensor([0.5550]), tensor(0.5268)), (tensor([0.4911]), tensor(0.5976)), (tensor([0.4837]), tensor(0.5654)), (tensor([0.8074]), tensor(0.8895)), (tensor([0.5777]), tensor(0.5112)), (tensor([0.6607]), tensor(0.6165)), (tensor([0.8865]), tensor(0.9050)), (tensor([0.7067]), tensor(0.9327)), (tensor([0.6537]), tensor(0.6068)), (tensor([0.6551]), tensor(0.6315)), (tensor([0.6095]), tensor(0.7693)), (tensor([0.6643]), tensor(0.8992)), (tensor([0.5512]), tensor(0.5201)), (tensor([0.7270]), tensor(0.9350)), (tensor([0.6410]), tensor(0.6612)), (tensor([0.6934]), tensor(0.6204)), (tensor([0.6492]), tensor(0.5672)), (tensor([0.5013]), tensor(0.5770)), (tensor([0.6567]), tensor(0.8488)), (tensor([0.8047]), tensor(0.8766)), (tensor([0.8504]), tensor(0.8800))]\n",
      "1\n",
      "Input shape: torch.Size([64, 1440, 17])\n",
      "Output: [(tensor([0.6632]), tensor(0.5980)), (tensor([0.4233]), tensor(0.4646)), (tensor([0.6253]), tensor(0.7554)), (tensor([0.8163]), tensor(0.8786)), (tensor([0.6610]), tensor(0.8774)), (tensor([0.8751]), tensor(0.9330)), (tensor([0.4672]), tensor(0.4832)), (tensor([0.5684]), tensor(0.5604)), (tensor([0.8788]), tensor(0.9152)), (tensor([0.6564]), tensor(0.5769)), (tensor([0.7107]), tensor(0.9397)), (tensor([0.5959]), tensor(0.4913)), (tensor([0.6378]), tensor(0.6109)), (tensor([0.8114]), tensor(0.8937)), (tensor([0.8086]), tensor(0.8674)), (tensor([0.7500]), tensor(0.8696)), (tensor([0.8846]), tensor(0.9265)), (tensor([0.5752]), tensor(0.4908)), (tensor([0.5769]), tensor(0.5419)), (tensor([0.5886]), tensor(0.5109)), (tensor([0.6503]), tensor(0.5610)), (tensor([0.6524]), tensor(0.8786)), (tensor([0.8778]), tensor(0.8885)), (tensor([0.6965]), tensor(0.9960)), (tensor([0.6876]), tensor(0.9775)), (tensor([0.5981]), tensor(0.5598)), (tensor([0.4862]), tensor(0.5066)), (tensor([0.6568]), tensor(0.9128)), (tensor([0.6331]), tensor(0.7705)), (tensor([0.4934]), tensor(0.5970)), (tensor([0.6636]), tensor(0.6151)), (tensor([0.8450]), tensor(0.8632)), (tensor([0.6710]), tensor(0.8987)), (tensor([0.6550]), tensor(0.9014)), (tensor([0.6237]), tensor(0.8182)), (tensor([0.6613]), tensor(0.5614)), (tensor([0.6912]), tensor(0.9555)), (tensor([0.5771]), tensor(0.5159)), (tensor([0.8857]), tensor(0.9050)), (tensor([0.5046]), tensor(0.5721)), (tensor([0.6145]), tensor(0.8080)), (tensor([0.8082]), tensor(0.8748)), (tensor([0.8777]), tensor(0.9209)), (tensor([0.8888]), tensor(0.9026)), (tensor([0.6486]), tensor(0.8487)), (tensor([0.6645]), tensor(0.5694)), (tensor([0.6998]), tensor(0.9411)), (tensor([0.7074]), tensor(0.9048)), (tensor([0.5489]), tensor(0.5179)), (tensor([0.4287]), tensor(0.4680)), (tensor([0.4150]), tensor(0.4585)), (tensor([0.6409]), tensor(0.8294)), (tensor([0.5863]), tensor(0.7707)), (tensor([0.6366]), tensor(0.6133)), (tensor([0.5776]), tensor(0.5143)), (tensor([0.8773]), tensor(0.9237)), (tensor([0.7286]), tensor(0.9227)), (tensor([0.8831]), tensor(0.9241)), (tensor([0.7957]), tensor(0.8653)), (tensor([0.4385]), tensor(0.4722)), (tensor([0.6116]), tensor(0.6331)), (tensor([0.6096]), tensor(0.4757)), (tensor([0.7038]), tensor(0.9400)), (tensor([0.5635]), tensor(0.5517))]\n",
      "2\n",
      "Input shape: torch.Size([30, 1440, 17])\n",
      "Output: [(tensor([0.7064]), tensor(0.9388)), (tensor([0.5130]), tensor(0.5645)), (tensor([0.6586]), tensor(0.5664)), (tensor([0.4687]), tensor(0.5809)), (tensor([0.5860]), tensor(0.4953)), (tensor([0.4124]), tensor(0.4755)), (tensor([0.6957]), tensor(1.)), (tensor([0.8649]), tensor(0.8966)), (tensor([0.7180]), tensor(0.9082)), (tensor([0.4729]), tensor(0.4835)), (tensor([0.4782]), tensor(0.5707)), (tensor([0.8354]), tensor(0.8355)), (tensor([0.6632]), tensor(0.9034)), (tensor([0.6327]), tensor(0.7567)), (tensor([0.6833]), tensor(0.6169)), (tensor([0.8694]), tensor(0.8986)), (tensor([0.5588]), tensor(0.5197)), (tensor([0.8345]), tensor(0.8582)), (tensor([0.5832]), tensor(0.5068)), (tensor([0.5007]), tensor(0.5680)), (tensor([0.7023]), tensor(0.9483)), (tensor([0.7395]), tensor(0.8866)), (tensor([0.6129]), tensor(0.4792)), (tensor([0.6141]), tensor(0.7933)), (tensor([0.8303]), tensor(0.8599)), (tensor([0.6003]), tensor(0.6326)), (tensor([0.5891]), tensor(0.4612)), (tensor([0.8744]), tensor(0.9235)), (tensor([0.6618]), tensor(0.6188)), (tensor([0.4131]), tensor(0.4708))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(17, 16, num_layers=4, batch_first=True)\n",
       "  (dense): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.eval()\n",
    "print(len(test_dataset))\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i, (inputs, labels) in enumerate(test_dataloader):\n",
    "    print(i)\n",
    "    print(f\"Input shape: {inputs.shape}\")  # Check input shape\n",
    "    # print(f\"Sample input: {inputs[0]}\")     # Inspect the first input of the batch\n",
    "\n",
    "    output = lstm_model(inputs)\n",
    "    print(f\"Output: {list(zip(output, labels))}\")  # Print output\n",
    "\n",
    "lstm_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Change From Current Stock Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the stock price for all stocks, pick the 10 best performing stocks and the 10 worst performing stocks\n",
    "# Use the LSTM model to predict the stock price for the next 5 days\n",
    "for i in range(len(tvt_dataloaders)):\n",
    "  train_loader, val_loader, test_loader = tvt_dataloaders[i]\n",
    "  predict(lstm_model, test_loader)\n",
    "\n",
    "# Calculate the percentage change in stock price for each stock\n",
    "# Take the current stock price and the stock price 5 days from now\n",
    "curr_stock_price = stock_data['Close']\n",
    "# Sort the stocks based on the percentage change in stock price\n",
    "# Pick the top 10 and bottom 10 stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGIAyRLohx0m"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
